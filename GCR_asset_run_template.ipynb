{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "194fe113-d67a-476d-ae96-7f72605a4af1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Jupyter Notebook for GCR**\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a9faf9-02ec-4c90-9af4-dfe0f02f29cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Table of Content**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbe8002-b1cf-432b-8a26-4e8472c83df9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    ">### 0. Introduction\n",
    ">### 1. 환경 구성\n",
    ">### 2. Train Workflow\n",
    ">### 3. Inference Workflow\n",
    ">### 4. Batch Running\n",
    ">### 5. 문의 및 기능 개발 요청\n",
    ">### 6. References   \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adcc16c-5678-4a69-9a45-9f46f6162e44",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **0. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d751b6-26d5-4ec6-bd94-f44dc1599898",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 본 sample notebook은 GCR의 구조와 각 asset의 역할과 산출물, 그 사용법을 처음 접하는 분들이 알기 쉽게 이해할 수 있도록 제작되었습니다.   \n",
    "\n",
    "[ **1. 환경 구성** ]에서는 ALO 등 환경 설치 방법을 설명하고,   \n",
    "[ **2. Train workflow** ]와 [ **3. Inference workflow** ]는 각각 train workflow와 inference workflow의 사용 방법과 산출물을 설명하며,   \n",
    "[ **4. Batch running** ]에서는 sample notebook이 아닌 실제 과제 운용 시에 GCR contents를 수행하는 방법을 설명하고   \n",
    "[ **5. 문의 및 기능 개발 요청** ]에서는 사용 중 문의 사항이나 기능에 대한 수정/개발 요청 방법을,   \n",
    "[ **6. References** ]에서는 추가로 참고하실 collab 문서 등에 대한 links를 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d9f8bd",
   "metadata": {},
   "source": [
    "***NOTE!!!***   \n",
    "<br />\n",
    "본 sample notebook의 제작 시점 (2024년 2월) 현재, ALO version이 기존 2.1에서 2.2로 변경되었으나 ALO 2.2의 jupyter notebook 지원은 완료되지 않은 상황입니다.   \n",
    "이에 따라, 본 sample notebook은 [ **2. Train workflow** ]와 [ **3. Inference workflow** ]는 ALO 2.1에 맞춰 제작되었고, [ **4. Batch running** ]은 ALO 2.2에 맞춰 제작된 점 양해 부탁 드립니다.   \n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a97420-7def-4cf2-ae32-51ee8fbdf4fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Notebook의 Workflow는 다음과 같이 구성됩니다.\n",
    "> Workflow NAME (ex. Train Workflow)\n",
    "\n",
    ">> Workflow 구성 설명\n",
    ">>> **A** asset : A asset 설명   \n",
    ">>> **B** asset : B asset 설명   \n",
    ">>> ...\n",
    "\n",
    ">> Workflow Setup \n",
    ">>> Workflow Setup 코드\n",
    "\n",
    ">> [0] **A** asset\n",
    ">>> parameter 설명 및 실행 코드 \n",
    "\n",
    ">> [1] **B** asset\n",
    ">>> parameter 설명 및 실행 코드\n",
    "\n",
    ">> ..\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f392e9ef-2821-4b89-9499-9d7932f2d0b3",
   "metadata": {},
   "source": [
    "#### 각 Asset은 동작확인을 위해 다음과 같이 구성됩니다.\n",
    "> ASSET NAME\n",
    "\n",
    ">> 주요 Parameter 설명\n",
    ">>> param1: param1 설명   \n",
    ">>> ***param2***: param2 설명 [*option1 / option2 / option3*]   \n",
    ">>> param3: param3 설명 [*option1 / option2*]   \n",
    ">>> ..\n",
    "\n",
    ">> Parameter 설정부\n",
    ">>> #################   \n",
    ">>> parameter 설정 코드   \n",
    ">>> #################   \n",
    "\n",
    ">> Asset 실행부\n",
    ">>> #################   \n",
    ">>> Asset 실행 코드   \n",
    ">>> #################   \n",
    "\n",
    "Asset 별로 experimental_plan.yaml에 주어진 parameter에 대한 설명이 주어집니다.   \n",
    "설명에 따라 parameter 변경 시 experimental_plan.yaml을 직접 수정하거나, Parameter 설정부에서 코드 실행을 통해 바꿀 수 있습니다.   \n",
    "위에서 param2와 같이 ***Bold Italic***으로 표기된 변수는 필수 설정 변수로, 최초 실행 또는 데이터가 바뀔 시 꼭 다시 설정해주어야 하는 변수를 의미합니다.   \n",
    "설정된 parameter로 asset을 실행할 수 있습니다. 또한 Parameter를 변경해가며 Asset 실행 결과 변화를 관찰할 수 있습니다.\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d624da5c-a13c-4d2d-946a-697c7aced3f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **1. 환경 구성**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3858a5e5-db47-49aa-bcf6-08fcb4561ed2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### GCR을 사용하기 위해서는 아래와 같은 방법으로 데이터를 준비해야 합니다.\n",
    "> 1. Train, Inference 두 개의 데이터셋을 준비합니다. 본 notebook에서는 GCR 설치 시 함께 제공되는 default sample data를 이용합니다.  \n",
    "> 2. GCR은 supervised learning을 제공하는 AI content이므로 train set에는 label에 해당하는 column이 존재해야 합니다. 또한, label에 해당하는 column은 결측치가 있어서는 안됩니다.\n",
    "> 3. Train set과 inference set은 label column을 제외하면 column명 list가 일치해야 합니다.\n",
    "> 4. Graph 구성을 위해 사용자가 지정해줘야 할 center node column ('center_node_column')도 결측치가 있어서는 안됩니다.   \n",
    "\n",
    "***GCR은 Graph-powered ML을 제공하므로, label 및 center node columns 외의 column들에 대해서는 결측치에 대한 전처리가 불필요하며, 모든 columns에 대해 범주형 데이터에 대한 전처리도 필요하지 않습니다***   \n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e07bc62-da8e-4304-80b5-0790dbbdb0b2",
   "metadata": {},
   "source": [
    "#### ALO 설치 및 configuration 설정 방법은 다음과 같습니다.\n",
    "<br />\n",
    "\n",
    "***NOTE!!!***   \n",
    "<br />\n",
    "여기에 설명하는 ALO 설치 방법은 sample notebook을 수행하기 위한 ALO 2.1 설치 방법입니다.   \n",
    "만일 console에서 GCR 기반의 AI solution을 개발하시는 경우라면, [ **4. Batch running** ]에 설명된 ALO 2.2 설치를 진행해야 합니다.   \n",
    "향후 ALO 2.2의 jupyter notebook 지원 기능 개발이 완료되면, sample notebook 수행 시에도 ALO 2.2 설치로 통일될 예정입니다.      \n",
    "<br />\n",
    "\n",
    "1. 최상위 디렉토리에서 install.sh를 실행합니다\n",
    "> source install.sh\n",
    "2. install.sh를 실행하면 alo 디렉토리가 설치됩니다.\n",
    "3. 가상환경을 설치 및 실행합니다.\n",
    "> conda create -n gcr python=3.10   \n",
    "> conda init bash   \n",
    "> source ~/.bashrc   \n",
    "> conda activate gcr    \n",
    "3. alo/config 디렉토리로 이동하여 experimental_plan.yaml 파일을 오픈합니다.\n",
    "4. external_path의 load_train_data_path에 아래와 같이 사용할 데이터의 경로(디렉토리)를 입력합니다.\n",
    "\n",
    ">```\n",
    ">external_path:\n",
    ">    - load_train_data_path: /nas001/gcr_test_data/sample/\n",
    ">    - load_inference_data_path:\n",
    ">    - save_train_artifacts_path:\n",
    ">    - save_inference_artifacts_path:\n",
    ">```\n",
    "\n",
    "***NOTE!!!***   \n",
    "<br />\n",
    "현재 sample notebook을 default sample data로 정상적으로 수행하기 위해서는, 아래와 같이 external_path를 설정해줘야 합니다.   \n",
    "이는, GCR 2.0.0은 ALO 2.2에 맞추어 개발되었으나, sample notebook을 수행하기 위한 ALO 2.2 지원이 개발 중인 관계로 본 sample notebook은 ALO 2.1을 사용하고 있는데, ALO 2.2와 ALO 2.1은 요구하는 sample data 위치가 상이하기 때문입니다.   \n",
    "<br />\n",
    "\n",
    ">```\n",
    ">external_path:\n",
    ">    - load_train_data_path: ../../sample_data/train\n",
    ">    - load_inference_data_path: ../../sample_data/test\n",
    ">```\n",
    "\n",
    "5. 필수 변경 parameter를 변경합니다. 나머지 parameter는 컨텐츠 yaml에 제공된 default 값을 사용해도 괜찮습니다.\n",
    "6. 아래 **ALO Setup**을 실행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b6b5c-8668-46b6-b5d8-00888c5580fa",
   "metadata": {},
   "source": [
    "### ALO Setup\n",
    "라이브러리 설치 및 컨텐츠 다운로드를 위해 아래 코드를 실행 해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "133147af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이하 cell들을 수행하다가 error가 발생하여 이곳부터 재 수행해야 할 경우, 아래 code를 열어 초기 위치로 돌아가 주세요.\n",
    "#os.chdir(os.path.abspath(os.path.join('/home/jovyan/240216_aicontents_gcr_2.0.0/gcr')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "137cd365-9ed9-4941-aa2e-67fca06e1a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[2024-02-16 05:26:46,144][PROCESS][INFO]: Successfully loaded << experimental_plan.yaml >> from: \n",
      " /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/config/experimental_plan.yaml\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:26:46,178][PROCESS][INFO]: None of << compare yaml >> version is matched. \n",
      " However, The version of << experimental_plan.yaml >> is recognized as same as compare yaml version << 2.0 >> \u001b[0m\n",
      "\u001b[92m[2024-02-16 05:26:46,181][PROCESS][INFO]: Success versioning up experimental_plan.yaml : 2.0 --> 2.1 (version ref. : compare yaml version)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['train_pipeline', 'inference_pipeline']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "os.chdir(os.path.abspath(os.path.join('./alo')))\n",
    "\n",
    "from src.alo import ALO\n",
    "from src.alo import AssetStructure\n",
    "alo = ALO(); alo.set_proc_logger(); alo.preset()\n",
    "pipelines = list(alo.asset_source.keys())\n",
    "\n",
    "from src.external import external_load_data, external_save_artifacts\n",
    "def run(step, pipeline, asset_structure):\n",
    "    # 반복되는 작업을 함수로 변환\n",
    "    asset_config = alo.asset_source[pipeline]\n",
    "    return alo.process_asset_step(asset_config[step], step, pipeline, asset_structure)\n",
    "\n",
    "# pipeline list 를 가지고 옴\n",
    "pipelines = list(alo.asset_source.keys())\n",
    "pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7337fdb-836f-4519-869b-76c6089743d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **2. Train Workflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171f4e9-d318-47ec-8374-f49fb6b30356",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### GCR의 Train Workflow 구성은 다음과 같습니다.\n",
    "> **[0]** Input asset : *사용자가 지정한 경로로부터 데이터를 Import*   \n",
    "> **[1]** Readiness asset : *Train 실시 전 입력 데이터의 오류를 검사하여 다음 step을 진행할 지 결정*   \n",
    "> **[2]** Train1 asset : *Train set 데이터를 토대로 그래프를 구성하고 필요한 임베딩 추출*   \n",
    "> **[3]** Preprocess asset : *(필요시) 결측치 처리 및 라벨 인코딩*   \n",
    "> **[4]** Train2 asset : *ML 모델 학습*   \n",
    "> **[5]** Output asset : *모델 학습과정에서 생성된 산출물을 올바른 경로에 저장*   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2425131a-066e-40e4-99d9-d90e3c858fac",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train Workflow Setup\n",
    "아래 코드를 실행하여 Train Workflow에 필요한 라이브러리를 먼저 설치 해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b7d0f4d-05da-40d0-9b5d-75c496b5794a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m[2024-02-16 05:26:50,789][PROCESS][WARNING]: You did not write any << s3_private_key_file >> in the config yaml file. When you wanna get data from s3 storage, \n",
      "                                 you have to write the s3_private_key_file path or set << AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY >> in your os environment. \n",
      "\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:26:50,794][PROCESS][INFO]: Successfuly removed << /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/input/train/ >> before loading external data.\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:26:50,797][PROCESS][INFO]: << ../../sample_data/train >> may be relative path. The reference folder of relative path is << config/ >>. \n",
      " If this is not appropriate relative path, Loading external data process would raise error.\u001b[0m\n",
      "\u001b[92m[2024-02-16 05:26:50,838][PROCESS][INFO]: ==================== Successfully done loading external data: \n",
      " ../../sample_data/train --> /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/input/train/\u001b[0m\n",
      "\u001b[92m[2024-02-16 05:26:50,841][PROCESS][INFO]: Successfuly finish loading << ../../sample_data/train >> into << /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/input/ >>\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:26:50,845][PROCESS][INFO]: Start setting-up << input >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:26:50,849][PROCESS][INFO]: Start renewing asset : /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/assets/input\u001b[0m\n",
      "\u001b[92m[2024-02-16 05:26:51,287][PROCESS][INFO]: /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/assets/input successfully pulled.\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:26:51,291][PROCESS][INFO]: Start setting-up << readiness >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:26:51,295][PROCESS][INFO]: Start renewing asset : /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/assets/readiness\u001b[0m\n",
      "\u001b[92m[2024-02-16 05:26:51,916][PROCESS][INFO]: /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/assets/readiness successfully pulled.\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:26:51,919][PROCESS][INFO]: Start setting-up << train1 >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:26:51,923][PROCESS][INFO]: Start renewing asset : /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/assets/train1\u001b[0m\n",
      "\u001b[92m[2024-02-16 05:26:52,312][PROCESS][INFO]: /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/assets/train1 successfully pulled.\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:26:52,315][PROCESS][INFO]: Start setting-up << preprocess >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:26:52,319][PROCESS][INFO]: Start renewing asset : /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/assets/preprocess\u001b[0m\n",
      "\u001b[92m[2024-02-16 05:26:53,014][PROCESS][INFO]: /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/assets/preprocess successfully pulled.\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:26:53,018][PROCESS][INFO]: Start setting-up << train2 >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:26:53,022][PROCESS][INFO]: Start renewing asset : /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/assets/train2\u001b[0m\n",
      "\u001b[92m[2024-02-16 05:26:53,488][PROCESS][INFO]: /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/assets/train2 successfully pulled.\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:26:53,491][PROCESS][INFO]: Start setting-up << output >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:26:53,495][PROCESS][INFO]: Start renewing asset : /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/assets/output\u001b[0m\n",
      "\u001b[92m[2024-02-16 05:26:53,894][PROCESS][INFO]: /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/assets/output successfully pulled.\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:26:53,901][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:26:53,904][PROCESS][INFO]: >>> Ignored installing << torch==2.0.0 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:26:53,907][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:26:53,911][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:26:53,914][PROCESS][INFO]: ======================================== Start dependency installation : << input >> \u001b[0m\n",
      "\u001b[94m[2024-02-16 05:26:53,917][PROCESS][INFO]: Start checking existence & installing package - pandas==1.5.3 | Progress: ( 1 / 15 total packages ) \u001b[0m\n",
      "\u001b[94m[2024-02-16 05:26:53,965][PROCESS][INFO]: >>> Start installing package - pandas==1.5.3\u001b[0m\n",
      "Collecting pandas==1.5.3\n",
      "  Using cached pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from pandas==1.5.3) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from pandas==1.5.3) (2021.3)\n",
      "Collecting numpy>=1.21.0 (from pandas==1.5.3)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Installing collected packages: numpy, pandas\n",
      "Successfully installed numpy-1.26.4 pandas-1.5.3\n",
      "\u001b[94m[2024-02-16 05:27:18,277][PROCESS][INFO]: ======================================== Start dependency installation : << train1 >> \u001b[0m\n",
      "\u001b[94m[2024-02-16 05:27:18,280][PROCESS][INFO]: Start checking existence & installing package - torch==2.0.0 | Progress: ( 2 / 15 total packages ) \u001b[0m\n",
      "\u001b[94m[2024-02-16 05:27:18,309][PROCESS][INFO]: >>> Start installing package - torch==2.0.0\u001b[0m\n",
      "Collecting torch==2.0.0\n",
      "  Using cached torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "Collecting filelock (from torch==2.0.0)\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typing-extensions (from torch==2.0.0)\n",
      "  Using cached typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch==2.0.0)\n",
      "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.0.0)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch==2.0.0)\n",
      "  Using cached Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0)\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0)\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0)\n",
      "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0)\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0)\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0)\n",
      "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0)\n",
      "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0)\n",
      "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0)\n",
      "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0)\n",
      "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0)\n",
      "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Collecting triton==2.0.0 (from torch==2.0.0)\n",
      "  Using cached triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "Requirement already satisfied: setuptools in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (69.0.3)\n",
      "Requirement already satisfied: wheel in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.42.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cmake (from triton==2.0.0->torch==2.0.0)\n",
      "  Using cached cmake-3.28.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting lit (from triton==2.0.0->torch==2.0.0)\n",
      "  Using cached lit-17.0.6-py3-none-any.whl\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.0.0)\n",
      "  Using cached MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch==2.0.0)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Using cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Using cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Using cached cmake-3.28.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.3 MB)\n",
      "Installing collected packages: mpmath, lit, cmake, typing-extensions, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, networkx, MarkupSafe, filelock, nvidia-cusolver-cu11, nvidia-cudnn-cu11, jinja2, triton, torch\n",
      "Successfully installed MarkupSafe-2.1.5 cmake-3.28.3 filelock-3.13.1 jinja2-3.1.3 lit-17.0.6 mpmath-1.3.0 networkx-3.2.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 sympy-1.12 torch-2.0.0 triton-2.0.0 typing-extensions-4.9.0\n",
      "\u001b[94m[2024-02-16 05:29:41,309][PROCESS][INFO]: Start checking existence & installing package - torchbiggraph@git+https://github.com/seongwooxp/PyTorch-BigGraph | Progress: ( 3 / 15 total packages ) \u001b[0m\n",
      "\u001b[94m[2024-02-16 05:29:41,351][PROCESS][INFO]: >>> Start installing package - torchbiggraph@git+https://github.com/seongwooxp/PyTorch-BigGraph\u001b[0m\n",
      "Collecting torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph\n",
      "  Cloning https://github.com/seongwooxp/PyTorch-BigGraph to /tmp/pip-install-wfsjmuqj/torchbiggraph_cd2464b475644c7b9d7f75edbb19fa34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/seongwooxp/PyTorch-BigGraph /tmp/pip-install-wfsjmuqj/torchbiggraph_cd2464b475644c7b9d7f75edbb19fa34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Resolved https://github.com/seongwooxp/PyTorch-BigGraph to commit 24c9aa0c6725c08e1c526fd9395cd7dfb4777e26\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting attrs>=18.2 (from torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting h5py>=2.8 (from torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph)\n",
      "  Using cached h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (1.26.4)\n",
      "Requirement already satisfied: setuptools in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (69.0.3)\n",
      "Requirement already satisfied: torch>=1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (2.0.0)\n",
      "Collecting tqdm (from torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: filelock in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from torch>=1->torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from torch>=1->torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from torch>=1->torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (1.12)\n",
      "Requirement already satisfied: networkx in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from torch>=1->torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from torch>=1->torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from torch>=1->torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from torch>=1->torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from torch>=1->torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from torch>=1->torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from torch>=1->torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from torch>=1->torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from torch>=1->torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from torch>=1->torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from torch>=1->torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from torch>=1->torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from torch>=1->torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from torch>=1->torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (2.0.0)\n",
      "Requirement already satisfied: wheel in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1->torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (0.42.0)\n",
      "Requirement already satisfied: cmake in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from triton==2.0.0->torch>=1->torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (3.28.3)\n",
      "Requirement already satisfied: lit in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from triton==2.0.0->torch>=1->torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (17.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from jinja2->torch>=1->torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from sympy->torch>=1->torchbiggraph@ git+https://github.com/seongwooxp/PyTorch-BigGraph) (1.3.0)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Building wheels for collected packages: torchbiggraph\n",
      "  Building wheel for torchbiggraph (setup.py): started\n",
      "  Building wheel for torchbiggraph (setup.py): finished with status 'done'\n",
      "  Created wheel for torchbiggraph: filename=torchbiggraph-1.0.1.dev0-cp310-cp310-linux_x86_64.whl size=214198 sha256=d9155efffb23743bff50d0c61cd98b0b6a926d9d9d8e968515ae2741d0cebef0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-pa9vm0p6/wheels/41/24/81/08e4b6d696226324131f45e3f4b489ec0c554156a623c3eaf0\n",
      "Successfully built torchbiggraph\n",
      "Installing collected packages: tqdm, h5py, attrs, torchbiggraph\n",
      "Successfully installed attrs-23.2.0 h5py-3.10.0 torchbiggraph-1.0.1.dev0 tqdm-4.66.2\n",
      "\u001b[94m[2024-02-16 05:30:27,807][PROCESS][INFO]: ======================================== Start dependency installation : << preprocess >> \u001b[0m\n",
      "\u001b[94m[2024-02-16 05:30:27,810][PROCESS][INFO]: Start checking existence & installing package - category_encoders | Progress: ( 4 / 15 total packages ) \u001b[0m\n",
      "\u001b[94m[2024-02-16 05:30:27,852][PROCESS][INFO]: >>> Start installing package - category_encoders\u001b[0m\n",
      "Collecting category_encoders\n",
      "  Using cached category_encoders-2.6.3-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from category_encoders) (1.26.4)\n",
      "Collecting scikit-learn>=0.20.0 (from category_encoders)\n",
      "  Using cached scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy>=1.0.0 (from category_encoders)\n",
      "  Using cached scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting statsmodels>=0.9.0 (from category_encoders)\n",
      "  Using cached statsmodels-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from category_encoders) (1.5.3)\n",
      "Collecting patsy>=0.5.1 (from category_encoders)\n",
      "  Using cached patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from pandas>=1.0.5->category_encoders) (2021.3)\n",
      "Requirement already satisfied: six in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting joblib>=1.2.0 (from scikit-learn>=0.20.0->category_encoders)\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn>=0.20.0->category_encoders)\n",
      "  Using cached threadpoolctl-3.3.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from statsmodels>=0.9.0->category_encoders) (23.2)\n",
      "Using cached category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
      "Using cached patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "Using cached scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "Using cached scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "Using cached statsmodels-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Using cached threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, patsy, joblib, scikit-learn, statsmodels, category_encoders\n",
      "Successfully installed category_encoders-2.6.3 joblib-1.3.2 patsy-0.5.6 scikit-learn-1.4.0 scipy-1.12.0 statsmodels-0.14.1 threadpoolctl-3.3.0\n",
      "\u001b[94m[2024-02-16 05:31:03,939][PROCESS][INFO]: ======================================== Start dependency installation : << train2 >> \u001b[0m\n",
      "\u001b[94m[2024-02-16 05:31:03,944][PROCESS][INFO]: Start checking existence & installing package - numpy==1.25.2 | Progress: ( 5 / 15 total packages ) \u001b[0m\n",
      "\u001b[94m[2024-02-16 05:31:03,997][PROCESS][INFO]: >>> Start installing package - numpy==1.25.2\u001b[0m\n",
      "Collecting numpy==1.25.2\n",
      "  Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-1.25.2\n",
      "\u001b[94m[2024-02-16 05:31:15,304][PROCESS][INFO]: Start checking existence & installing package - scikit-learn | Progress: ( 6 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2024-02-16 05:31:15,552][PROCESS][INFO]: [OK] << scikit-learn >> already exists\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:31:15,556][PROCESS][INFO]: Start checking existence & installing package - matplotlib | Progress: ( 7 / 15 total packages ) \u001b[0m\n",
      "\u001b[94m[2024-02-16 05:31:15,605][PROCESS][INFO]: >>> Start installing package - matplotlib\u001b[0m\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.49.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (159 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib) (23.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-10.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Using cached matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "Using cached contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (310 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.49.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "Using cached kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Using cached pillow-10.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "Using cached pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.0 cycler-0.12.1 fonttools-4.49.0 kiwisolver-1.4.5 matplotlib-3.8.3 pillow-10.2.0 pyparsing-3.1.1\n",
      "\u001b[94m[2024-02-16 05:31:29,454][PROCESS][INFO]: Start checking existence & installing package - seaborn | Progress: ( 8 / 15 total packages ) \u001b[0m\n",
      "\u001b[94m[2024-02-16 05:31:29,502][PROCESS][INFO]: >>> Start installing package - seaborn\u001b[0m\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from seaborn) (1.25.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from seaborn) (1.5.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from seaborn) (3.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "\u001b[94m[2024-02-16 05:31:32,443][PROCESS][INFO]: Start checking existence & installing package - shap | Progress: ( 9 / 15 total packages ) \u001b[0m\n",
      "\u001b[94m[2024-02-16 05:31:32,489][PROCESS][INFO]: >>> Start installing package - shap\u001b[0m\n",
      "Collecting shap\n",
      "  Using cached shap-0.44.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
      "Requirement already satisfied: numpy in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from shap) (1.25.2)\n",
      "Requirement already satisfied: scipy in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from shap) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from shap) (1.4.0)\n",
      "Requirement already satisfied: pandas in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from shap) (1.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from shap) (4.66.2)\n",
      "Requirement already satisfied: packaging>20.9 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from shap) (23.2)\n",
      "Collecting slicer==0.0.7 (from shap)\n",
      "  Using cached slicer-0.0.7-py3-none-any.whl (14 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numba (from shap)\n",
      "  Using cached numba-0.59.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting cloudpickle (from shap)\n",
      "  Using cached cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting llvmlite<0.43,>=0.42.0dev0 (from numba->shap)\n",
      "  Using cached llvmlite-0.42.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from pandas->shap) (2021.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from scikit-learn->shap) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from scikit-learn->shap) (3.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
      "Using cached shap-0.44.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (535 kB)\n",
      "Using cached cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Using cached numba-0.59.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "Using cached llvmlite-0.42.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.8 MB)\n",
      "Installing collected packages: slicer, llvmlite, cloudpickle, numba, shap\n",
      "Successfully installed cloudpickle-3.0.0 llvmlite-0.42.0 numba-0.59.0 shap-0.44.1 slicer-0.0.7\n",
      "\u001b[94m[2024-02-16 05:31:47,111][PROCESS][INFO]: Start checking existence & installing package - lightgbm | Progress: ( 10 / 15 total packages ) \u001b[0m\n",
      "\u001b[94m[2024-02-16 05:31:47,168][PROCESS][INFO]: >>> Start installing package - lightgbm\u001b[0m\n",
      "Collecting lightgbm\n",
      "  Using cached lightgbm-4.3.0-py3-none-manylinux_2_27_x86_64.whl\n",
      "Requirement already satisfied: numpy in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from lightgbm) (1.25.2)\n",
      "Requirement already satisfied: scipy in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from lightgbm) (1.12.0)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.3.0\n",
      "\u001b[94m[2024-02-16 05:31:49,570][PROCESS][INFO]: Start checking existence & installing package - catboost | Progress: ( 11 / 15 total packages ) \u001b[0m\n",
      "\u001b[94m[2024-02-16 05:31:49,620][PROCESS][INFO]: >>> Start installing package - catboost\u001b[0m\n",
      "Collecting catboost\n",
      "  Using cached catboost-1.2.2-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting graphviz (from catboost)\n",
      "  Using cached graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied: matplotlib in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from catboost) (3.8.3)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from catboost) (1.25.2)\n",
      "Requirement already satisfied: pandas>=0.24 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from catboost) (1.5.3)\n",
      "Requirement already satisfied: scipy in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from catboost) (1.12.0)\n",
      "Collecting plotly (from catboost)\n",
      "  Using cached plotly-5.19.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: six in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2021.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib->catboost) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib->catboost) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib->catboost) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib->catboost) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib->catboost) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib->catboost) (3.1.1)\n",
      "Collecting tenacity>=6.2.0 (from plotly->catboost)\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Using cached catboost-1.2.2-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
      "Using cached plotly-5.19.0-py3-none-any.whl (15.7 MB)\n",
      "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tenacity, graphviz, plotly, catboost\n",
      "Successfully installed catboost-1.2.2 graphviz-0.20.1 plotly-5.19.0 tenacity-8.2.3\n",
      "\u001b[94m[2024-02-16 05:33:29,001][PROCESS][INFO]: Start checking existence & installing package - ngboost | Progress: ( 12 / 15 total packages ) \u001b[0m\n",
      "\u001b[94m[2024-02-16 05:33:29,049][PROCESS][INFO]: >>> Start installing package - ngboost\u001b[0m\n",
      "Collecting ngboost\n",
      "  Using cached ngboost-0.5.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting lifelines>=0.25 (from ngboost)\n",
      "  Using cached lifelines-0.28.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from ngboost) (1.25.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from ngboost) (1.4.0)\n",
      "Requirement already satisfied: scipy>=1.7.2 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from ngboost) (1.12.0)\n",
      "Requirement already satisfied: tqdm>=4.3 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from ngboost) (4.66.2)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from lifelines>=0.25->ngboost) (1.5.3)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from lifelines>=0.25->ngboost) (3.8.3)\n",
      "Collecting autograd>=1.5 (from lifelines>=0.25->ngboost)\n",
      "  Using cached autograd-1.6.2-py3-none-any.whl.metadata (706 bytes)\n",
      "Collecting autograd-gamma>=0.3 (from lifelines>=0.25->ngboost)\n",
      "  Using cached autograd_gamma-0.5.0-py3-none-any.whl\n",
      "Collecting formulaic>=0.2.2 (from lifelines>=0.25->ngboost)\n",
      "  Using cached formulaic-1.0.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from scikit-learn>=1.0.2->ngboost) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from scikit-learn>=1.0.2->ngboost) (3.3.0)\n",
      "Collecting future>=0.15.2 (from autograd>=1.5->lifelines>=0.25->ngboost)\n",
      "  Using cached future-0.18.3-py3-none-any.whl\n",
      "Collecting interface-meta>=1.2.0 (from formulaic>=0.2.2->lifelines>=0.25->ngboost)\n",
      "  Using cached interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (4.9.0)\n",
      "Collecting wrapt>=1.0 (from formulaic>=0.2.2->lifelines>=0.25->ngboost)\n",
      "  Using cached wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from pandas>=1.2.0->lifelines>=0.25->ngboost) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines>=0.25->ngboost) (1.16.0)\n",
      "Using cached ngboost-0.5.0-py3-none-any.whl (33 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached lifelines-0.28.0-py3-none-any.whl (349 kB)\n",
      "Using cached autograd-1.6.2-py3-none-any.whl (49 kB)\n",
      "Using cached formulaic-1.0.1-py3-none-any.whl (94 kB)\n",
      "Using cached wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "Installing collected packages: wrapt, interface-meta, future, autograd, formulaic, autograd-gamma, lifelines, ngboost\n",
      "Successfully installed autograd-1.6.2 autograd-gamma-0.5.0 formulaic-1.0.1 future-0.18.3 interface-meta-1.3.0 lifelines-0.28.0 ngboost-0.5.0 wrapt-1.16.0\n",
      "\u001b[94m[2024-02-16 05:33:36,700][PROCESS][INFO]: Start checking existence & installing package - numba==0.58.0 | Progress: ( 13 / 15 total packages ) \u001b[0m\n",
      "\u001b[94m[2024-02-16 05:33:36,760][PROCESS][INFO]: >>> Start installing package - numba==0.58.0\u001b[0m\n",
      "Collecting numba==0.58.0\n",
      "  Using cached numba-0.58.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting llvmlite<0.42,>=0.41.0dev0 (from numba==0.58.0)\n",
      "  Using cached llvmlite-0.41.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: numpy<1.26,>=1.21 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from numba==0.58.0) (1.25.2)\n",
      "Using cached numba-0.58.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
      "Using cached llvmlite-0.41.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.6 MB)\n",
      "Installing collected packages: llvmlite, numba\n",
      "  Attempting uninstall: llvmlite\n",
      "    Found existing installation: llvmlite 0.42.0\n",
      "    Uninstalling llvmlite-0.42.0:\n",
      "      Successfully uninstalled llvmlite-0.42.0\n",
      "  Attempting uninstall: numba\n",
      "    Found existing installation: numba 0.59.0\n",
      "    Uninstalling numba-0.59.0:\n",
      "      Successfully uninstalled numba-0.59.0\n",
      "Successfully installed llvmlite-0.41.1 numba-0.58.0\n",
      "\u001b[94m[2024-02-16 05:33:51,053][PROCESS][INFO]: Start checking existence & installing package - missingno | Progress: ( 14 / 15 total packages ) \u001b[0m\n",
      "\u001b[94m[2024-02-16 05:33:51,110][PROCESS][INFO]: >>> Start installing package - missingno\u001b[0m\n",
      "Collecting missingno\n",
      "  Using cached missingno-0.5.2-py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: numpy in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from missingno) (1.25.2)\n",
      "Requirement already satisfied: matplotlib in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from missingno) (3.8.3)\n",
      "Requirement already satisfied: scipy in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from missingno) (1.12.0)\n",
      "Requirement already satisfied: seaborn in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from missingno) (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib->missingno) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib->missingno) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib->missingno) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib->missingno) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib->missingno) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib->missingno) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib->missingno) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from matplotlib->missingno) (2.8.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from seaborn->missingno) (1.5.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from pandas>=1.2->seaborn->missingno) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/jovyan/conda/envs/gcr/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->missingno) (1.16.0)\n",
      "Installing collected packages: missingno\n",
      "Successfully installed missingno-0.5.2\n",
      "\u001b[94m[2024-02-16 05:33:53,636][PROCESS][INFO]: ======================================== Start dependency installation : << force-reinstall >> \u001b[0m\n",
      "\u001b[94m[2024-02-16 05:33:53,640][PROCESS][INFO]: Start checking existence & installing package - numpy==1.25.2 --force-reinstall | Progress: ( 15 / 15 total packages ) \u001b[0m\n",
      "\u001b[94m[2024-02-16 05:33:53,644][PROCESS][INFO]: >>> Start installing package - numpy==1.25.2 --force-reinstall\u001b[0m\n",
      "Collecting numpy==1.25.2\n",
      "  Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.2\n",
      "    Uninstalling numpy-1.25.2:\n",
      "      Successfully uninstalled numpy-1.25.2\n",
      "Successfully installed numpy-1.25.2\n",
      "\u001b[94m[2024-02-16 05:34:04,863][PROCESS][INFO]: ======================================== Finish dependency installation \n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "alo.external_load_data(pipelines[0]) # external load data for train_pipeline\n",
    "# 사용하는 pipeline의 package를 설치\n",
    "# train = 0, infernence = 1을 선택해야 하고 둘다 설치 해야함\n",
    "pipeline = pipelines[0]\n",
    "alo.install_steps(pipeline, alo.control[\"get_asset_source\"])\n",
    " # 초기 data structure 구성\n",
    "alo.set_asset_structure()\n",
    "init_asset_structure = copy.deepcopy(alo.asset_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c18b98-847e-4c58-848c-bec26204375f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [0] Input asset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9f36d6-84e0-440e-80d3-b462254b006a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 주요 Parameter\n",
    "- ***input_path*** : Extenal Path에서 받은 데이터는 *alo/input/train*에 저장됩니다. 이 중에서 사용할 데이터가 저장된 디렉터리 명을 작성해주시면 됩니다.\n",
    "- x_columns : 데이터의 모든 컬럼을 활용하지 않는 경우엔 직접 선택해서 사용할 수 있습니다.\n",
    "- use_all_x : 데이터의 모든 컬럼을 사용하는 경우 True로 설정하고 사용합니다. 이 경우 x_columns는 빈칸이어야 합니다. [*True / False*]\n",
    "- ***y_column*** : Classification, Regression을 위해서는 Label이 있어야 합니다. Label에 해당하는 컬럼을 작성합니다.\n",
    "- groupkey_columns : 특정 컬럼 명을 기준으로 데이터를 그룹으로 나누어 모델링을 하고 싶을 경우에 사용합니다.\n",
    "- drop_columns : use_all_x가 True일 때 삭제하고 싶은 컬럼을 입력합니다.\n",
    "- time_column : 데이터에 시간 컬럼이 있을 경우 입력합니다.\n",
    "- concat_dataframes : 같은 형태 csv 파일 여러 개를 input data로 불러올 시, concat 여부를 선택합니다. [*True / False*]\n",
    "- encoding : pd.read_csv() 시에 사용할 encoding 방법을 설정합니다.\n",
    "\n",
    "#### Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90e2c80e-0a14-4e19-b50f-2aa621aec2da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_path': 'train',\n",
       " 'x_columns': None,\n",
       " 'use_all_x': True,\n",
       " 'y_column': 'target',\n",
       " 'groupkey_columns': None,\n",
       " 'drop_columns': None,\n",
       " 'time_column': None,\n",
       " 'concat_dataframes': None,\n",
       " 'encoding': None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - readiness(1) - train1(2) - preprocess(3) - train2(4) - output(5))\n",
    "step = 0\n",
    "alo.asset_structure= copy.deepcopy(init_asset_structure)\n",
    "alo.asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 input asset argument를 원하는 값으로 수정합니다. \n",
    "#asset_structure.args['x_columns'] = ['']\n",
    "alo.asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017bbb5c-b15e-4b8b-98c9-9b355da55263",
   "metadata": {},
   "source": [
    "#### Input asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ac5a67f-02e5-49f5-9d44-34f24b08baaf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-16 05:35:10,075][USER][INFO][train_pipeline][input]: >> Load path : ['/home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/input/train/train/']\n",
      "[2024-02-16 05:35:10,196][USER][INFO][train_pipeline][input]: >> The file for batch data has been loaded. (File name: /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/input/train/train/samples.csv)\n",
      "[2024-02-16 05:35:10,200][USER][INFO][train_pipeline][input]: You set the << use_all_x >> as << True >> in the yaml file. So skip checking dataframe columns existence.\n",
      "[2024-02-16 05:35:10,216][USER][INFO][train_pipeline][input]: ==================== Success loading dataframe ====================\n",
      "[2024-02-16 05:35:10,219][USER][INFO][train_pipeline][input]: >> Start processing ignore columns & drop columns: ['/home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/input/train/train/samples.csv']\n",
      "[2024-02-16 05:35:10,232][USER][INFO][train_pipeline][input]: >> You set the << use_all_x >> parameter as << True >> in your config yaml. (So, these x_columns are used: ['workclass', 'education-num', 'occupation', 'age', 'marital-status', 'fnlwgt', 'capital-gain', 'hours-per-week', 'ID', 'relationship', 'education', 'sex', 'capital-loss', 'race', 'native-country'] )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2024-02-16 05:35:10,069][ASSET][INFO][train_pipeline][input]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2024-02-16 05:35:10\n",
      "- current step      : input\n",
      "- asset branch.     : tabular_2.0\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : 2.1\n",
      "- load envs. keys   : dict_keys(['project_home', 'solution_metadata_version', 'artifacts', 'alo_version', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'pipeline', 'step', 'num_step', 'asset_branch', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path'])\n",
      "- load args. keys   : dict_keys(['input_path', 'x_columns', 'use_all_x', 'y_column', 'groupkey_columns', 'drop_columns', 'time_column', 'concat_dataframes', 'encoding'])\n",
      "- load config. keys : dict_keys(['meta'])\n",
      "- load data keys    : dict_keys([])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:35:10,233][ASSET][INFO][train_pipeline][input]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2024-02-16 05:35:10\n",
      "- current step      : input\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:35:10,236][PROCESS][INFO]: ==================== Finish pipeline: train_pipeline / step: input\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>target</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "      <td>CX0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "      <td>CX1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "      <td>CX2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "      <td>CX3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>1</td>\n",
       "      <td>CX4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37</td>\n",
       "      <td>Private</td>\n",
       "      <td>284582</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "      <td>CX5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>49</td>\n",
       "      <td>Private</td>\n",
       "      <td>160187</td>\n",
       "      <td>9th</td>\n",
       "      <td>5</td>\n",
       "      <td>Married-spouse-absent</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>Jamaica</td>\n",
       "      <td>1</td>\n",
       "      <td>CX6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42</td>\n",
       "      <td>Private</td>\n",
       "      <td>159449</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>5178</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>CX9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>37</td>\n",
       "      <td>Private</td>\n",
       "      <td>280464</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>CX10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>141297</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>India</td>\n",
       "      <td>0</td>\n",
       "      <td>CX11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt     education  education-num  \\\n",
       "0   39         State-gov   77516     Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311     Bachelors             13   \n",
       "2   38           Private  215646       HS-grad              9   \n",
       "3   53           Private  234721          11th              7   \n",
       "4   28           Private  338409     Bachelors             13   \n",
       "5   37           Private  284582       Masters             14   \n",
       "6   49           Private  160187           9th              5   \n",
       "7   42           Private  159449     Bachelors             13   \n",
       "8   37           Private  280464  Some-college             10   \n",
       "9   30         State-gov  141297     Bachelors             13   \n",
       "\n",
       "          marital-status         occupation   relationship  \\\n",
       "0          Never-married       Adm-clerical  Not-in-family   \n",
       "1     Married-civ-spouse    Exec-managerial        Husband   \n",
       "2               Divorced  Handlers-cleaners  Not-in-family   \n",
       "3     Married-civ-spouse  Handlers-cleaners        Husband   \n",
       "4     Married-civ-spouse     Prof-specialty           Wife   \n",
       "5     Married-civ-spouse    Exec-managerial           Wife   \n",
       "6  Married-spouse-absent      Other-service  Not-in-family   \n",
       "7     Married-civ-spouse    Exec-managerial        Husband   \n",
       "8     Married-civ-spouse    Exec-managerial        Husband   \n",
       "9     Married-civ-spouse     Prof-specialty        Husband   \n",
       "\n",
       "                 race     sex  capital-gain  capital-loss  hours-per-week  \\\n",
       "0               White    Male          2174             0              40   \n",
       "1               White    Male             0             0              13   \n",
       "2               White    Male             0             0              40   \n",
       "3               Black    Male             0             0              40   \n",
       "4               Black  Female             0             0              40   \n",
       "5               White  Female             0             0              40   \n",
       "6               Black  Female             0             0              16   \n",
       "7               White    Male          5178             0              40   \n",
       "8               Black    Male             0             0              80   \n",
       "9  Asian-Pac-Islander    Male             0             0              40   \n",
       "\n",
       "  native-country  target    ID  \n",
       "0  United-States       1   CX0  \n",
       "1  United-States       1   CX1  \n",
       "2  United-States       1   CX2  \n",
       "3  United-States       1   CX3  \n",
       "4           Cuba       1   CX4  \n",
       "5  United-States       1   CX5  \n",
       "6        Jamaica       1   CX6  \n",
       "7  United-States       0   CX9  \n",
       "8  United-States       0  CX10  \n",
       "9          India       0  CX11  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_asset_structure = run(step, pipeline, alo.asset_structure)\n",
    "\n",
    "# input asset의 결과 dataframe은 input_asset_structure.data['dataframe']으로 확인할 수 있습니다.\n",
    "input_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca34284",
   "metadata": {},
   "source": [
    "### [1] Readiness asset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee15dc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 주요 Parameter   \n",
    "<br />\n",
    "GCR 2.0.0부터는 data readiness check을 수행하기 위한 readiness asset이 추가되었습니다.   \n",
    "<br />\n",
    "Readiness asset은 AI content 마다 수행하는 기능과 parameter들이 상이한데, 이는 각 AI content의 동작에 맞도록 input data의 오류를 검사하기 때문입니다.   \n",
    "Input data에서 오류가 발견된 경우, readiness asset은 error를 발생시켜 전체 pipeline을 멈추게 됩니다. 이 때, 발생된 error는 log file에서 확인하실 수 있습니다.   \n",
    "<br />\n",
    "GCR의 readiness asset은 label에 해당하는 column에 결측치가 있는지, graph의 center node에 해당하는 column에 결측치가 있는지를 확인합니다. \n",
    "<br />  \n",
    "이를 위해 필요한 parameter들이 아래와 같습니다.   \n",
    "<br />\n",
    "현재 readiness asset이 alpha version인 관계로, 다른 asset과 parameter 중복이 존재하는데, 이 문제는 readiness asset 정식 version이 release되는 시점 (24년 3월)에 정리될 예정이오니 양해 부탁 드립니다.   \n",
    "<br />\n",
    "\n",
    "- ***x_columns*** : 데이터의 모든 컬럼을 활용하지 않는 경우엔 직접 선택해서 사용할 수 있습니다.\n",
    "- ***y_column*** : Classification, Regression을 위해서는 Label이 있어야 합니다. Label에 해당하는 컬럼을 작성합니다.\n",
    "- ***groupkey_columns*** : 특정 컬럼 명을 기준으로 데이터를 그룹으로 나누어 모델링을 하고 싶을 경우에 사용합니다.\n",
    "- ***center_node_column*** : 특정 컬럼 명을 기준으로 데이터를 그룹으로 나누어 모델링을 하고 싶을 경우에 사용합니다.\n",
    "\n",
    "\n",
    "#### Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6956adbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_columns': None,\n",
       " 'y_column': 'target',\n",
       " 'groupkey_columns': None,\n",
       " 'center_node_column': 'ID'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - readiness(1) - train1(2) - preprocess(3) - train2(4) - output(5))\n",
    "step = 1\n",
    "alo.asset_structure= copy.deepcopy(input_asset_structure)\n",
    "alo.asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 input asset argument를 원하는 값으로 수정합니다. \n",
    "#asset_structure.args['x_columns'] = ['']\n",
    "alo.asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de325081",
   "metadata": {},
   "source": [
    "#### Readiness asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e97ef00",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2024-02-16 05:59:41,411][ASSET][INFO][train_pipeline][readiness]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2024-02-16 05:59:41\n",
      "- current step      : readiness\n",
      "- asset branch.     : gcr-0.9.0\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : 2.1\n",
      "- load envs. keys   : dict_keys(['project_home', 'solution_metadata_version', 'artifacts', 'alo_version', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'pipeline', 'step', 'num_step', 'asset_branch', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['x_columns', 'y_column', 'groupkey_columns', 'center_node_column'])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "In __init__: readiness_check initialized\n",
      "In check_data: Data checking begins at 2024-02-16 05:59:41.415595\n",
      "In check_data: Data checking ends at 2024-02-16 05:59:41.487781. Memory usage = 0.0\n",
      "In __del__: readiness_check deleted\n",
      "\u001b[94m[2024-02-16 05:59:41,487][ASSET][INFO][train_pipeline][readiness]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2024-02-16 05:59:41\n",
      "- current step      : readiness\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'center_node_column'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2024-02-16 05:59:41,490][PROCESS][INFO]: ==================== Finish pipeline: train_pipeline / step: readiness\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>target</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "      <td>CX0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "      <td>CX1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "      <td>CX2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "      <td>CX3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>1</td>\n",
       "      <td>CX4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37</td>\n",
       "      <td>Private</td>\n",
       "      <td>284582</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "      <td>CX5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>49</td>\n",
       "      <td>Private</td>\n",
       "      <td>160187</td>\n",
       "      <td>9th</td>\n",
       "      <td>5</td>\n",
       "      <td>Married-spouse-absent</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>Jamaica</td>\n",
       "      <td>1</td>\n",
       "      <td>CX6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42</td>\n",
       "      <td>Private</td>\n",
       "      <td>159449</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>5178</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>CX9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>37</td>\n",
       "      <td>Private</td>\n",
       "      <td>280464</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>CX10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>141297</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>India</td>\n",
       "      <td>0</td>\n",
       "      <td>CX11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt     education  education-num  \\\n",
       "0   39         State-gov   77516     Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311     Bachelors             13   \n",
       "2   38           Private  215646       HS-grad              9   \n",
       "3   53           Private  234721          11th              7   \n",
       "4   28           Private  338409     Bachelors             13   \n",
       "5   37           Private  284582       Masters             14   \n",
       "6   49           Private  160187           9th              5   \n",
       "7   42           Private  159449     Bachelors             13   \n",
       "8   37           Private  280464  Some-college             10   \n",
       "9   30         State-gov  141297     Bachelors             13   \n",
       "\n",
       "          marital-status         occupation   relationship  \\\n",
       "0          Never-married       Adm-clerical  Not-in-family   \n",
       "1     Married-civ-spouse    Exec-managerial        Husband   \n",
       "2               Divorced  Handlers-cleaners  Not-in-family   \n",
       "3     Married-civ-spouse  Handlers-cleaners        Husband   \n",
       "4     Married-civ-spouse     Prof-specialty           Wife   \n",
       "5     Married-civ-spouse    Exec-managerial           Wife   \n",
       "6  Married-spouse-absent      Other-service  Not-in-family   \n",
       "7     Married-civ-spouse    Exec-managerial        Husband   \n",
       "8     Married-civ-spouse    Exec-managerial        Husband   \n",
       "9     Married-civ-spouse     Prof-specialty        Husband   \n",
       "\n",
       "                 race     sex  capital-gain  capital-loss  hours-per-week  \\\n",
       "0               White    Male          2174             0              40   \n",
       "1               White    Male             0             0              13   \n",
       "2               White    Male             0             0              40   \n",
       "3               Black    Male             0             0              40   \n",
       "4               Black  Female             0             0              40   \n",
       "5               White  Female             0             0              40   \n",
       "6               Black  Female             0             0              16   \n",
       "7               White    Male          5178             0              40   \n",
       "8               Black    Male             0             0              80   \n",
       "9  Asian-Pac-Islander    Male             0             0              40   \n",
       "\n",
       "  native-country  target    ID  \n",
       "0  United-States       1   CX0  \n",
       "1  United-States       1   CX1  \n",
       "2  United-States       1   CX2  \n",
       "3  United-States       1   CX3  \n",
       "4           Cuba       1   CX4  \n",
       "5  United-States       1   CX5  \n",
       "6        Jamaica       1   CX6  \n",
       "7  United-States       0   CX9  \n",
       "8  United-States       0  CX10  \n",
       "9          India       0  CX11  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readiness_asset_structure = run(step, pipeline, alo.asset_structure)\n",
    "\n",
    "# Readiness asset의 결과 dataframe은 readiness_asset_structure.data['dataframe']으로 확인할 수 있습니다.\n",
    "readiness_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e402b7cc-2391-45dd-8a0d-5be4eb8b9b8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [2] Train1 asset (기존 graph asset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b53121",
   "metadata": {},
   "source": [
    "GCR의 가장 핵심이 되는 asset입니다. Raw Data를 임베딩으로 모두 변환하여 별도의 전처리 없이 더 높은 성능의 ML모델을 획득하기 위한 Tool이 됩니다.\n",
    "#### 주요 Parameter\n",
    "- center_node_column : 방사형 그래프는 중심 컬럼을 기준으로 그래프를 구성합니다. 데이터의 대표가 되는 컬럼을 선택하면 됩니다.(ex. ID, Name 등)\n",
    "- embedding_column : 임베딩의 대상이 되는 컬럼을 선택합니다. 라벨이 center_node_column과 대응하지 않는 경우가 있을 수 있습니다. 이때는 label이 대응하는 컬럼을 embedding column으로 설정합니다.\n",
    "- dimension : 임베딩 출력 벡터의 차원수를 설정합니다.\n",
    "- num_epochs: Embedding 학습을 위한 epoch을 설정합니다.\n",
    "- num_partitions : partition 수를 설정합니다. 증가시켜 메모리 사용량을 줄일 수 있습니다.\n",
    "- use_gpu : 운영 시스템이 GPU를 지원할 경우, true로 설정하여 GPU를 사용할 수 있습니다.\n",
    "\n",
    "#### Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05226c4c-981b-46e6-a187-72e3d47433f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'center_node_column': 'ID',\n",
       " 'embedding_column': 'ID',\n",
       " 'dimension': 128,\n",
       " 'num_epochs': 1,\n",
       " 'num_partitions': None,\n",
       " 'use_gpu': None}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - readiness(1) - train1(2) - preprocess(3) - train2(4) - output(5))\n",
    "step = 2 \n",
    "alo.asset_structure= copy.deepcopy(readiness_asset_structure)\n",
    "alo.asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 graph asset argument를 원하는 값으로 수정합니다. \n",
    "#asset_structure.args['dimension'] = 128\n",
    "alo.asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2084ad",
   "metadata": {},
   "source": [
    "#### Train1 asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be198a60-4793-4639-a709-3cb74263d1a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[2024-02-16 06:06:35,857][ASSET][INFO][train_pipeline][train1]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/\u001b[0m\n",
      "\u001b[94m[2024-02-16 06:06:35,860][ASSET][INFO][train_pipeline][train1]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2024-02-16 06:06:35\n",
      "- current step      : train1\n",
      "- asset branch.     : develop\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : 2.1\n",
      "- load envs. keys   : dict_keys(['project_home', 'solution_metadata_version', 'artifacts', 'alo_version', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'pipeline', 'step', 'num_step', 'asset_branch', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['center_node_column', 'embedding_column', 'dimension', 'num_epochs', 'num_partitions', 'use_gpu'])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'center_node_column'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "preprocessing blank space...\n",
      "In __init__: pbg ready\n",
      "[2024-02-16 06:06:36.917073] Using the 14 relation types given in the config\n",
      "[2024-02-16 06:06:36.919977] Searching for the entities in the edge files...\n",
      "[2024-02-16 06:06:37.366143] Entity type fnlwgt:\n",
      "[2024-02-16 06:06:37.367034] - Found 18502 entities\n",
      "[2024-02-16 06:06:37.367900] - Removing the ones with fewer than 1 occurrences...\n",
      "[2024-02-16 06:06:37.370431] - Left with 18502 entities\n",
      "[2024-02-16 06:06:37.371266] - Shuffling them...\n",
      "[2024-02-16 06:06:37.381173] Entity type ID:\n",
      "[2024-02-16 06:06:37.381938] - Found 26072 entities\n",
      "[2024-02-16 06:06:37.382675] - Removing the ones with fewer than 1 occurrences...\n",
      "[2024-02-16 06:06:37.386193] - Left with 26072 entities\n",
      "[2024-02-16 06:06:37.386986] - Shuffling them...\n",
      "[2024-02-16 06:06:37.399915] Entity type capital-gain:\n",
      "[2024-02-16 06:06:37.400701] - Found 118 entities\n",
      "[2024-02-16 06:06:37.401320] - Removing the ones with fewer than 1 occurrences...\n",
      "[2024-02-16 06:06:37.401951] - Left with 118 entities\n",
      "[2024-02-16 06:06:37.402568] - Shuffling them...\n",
      "[2024-02-16 06:06:37.403258] Entity type hours-per-week:\n",
      "[2024-02-16 06:06:37.403864] - Found 92 entities\n",
      "[2024-02-16 06:06:37.404471] - Removing the ones with fewer than 1 occurrences...\n",
      "[2024-02-16 06:06:37.405142] - Left with 92 entities\n",
      "[2024-02-16 06:06:37.405750] - Shuffling them...\n",
      "[2024-02-16 06:06:37.406431] Entity type capital-loss:\n",
      "[2024-02-16 06:06:37.407024] - Found 89 entities\n",
      "[2024-02-16 06:06:37.407639] - Removing the ones with fewer than 1 occurrences...\n",
      "[2024-02-16 06:06:37.408292] - Left with 89 entities\n",
      "[2024-02-16 06:06:37.408921] - Shuffling them...\n",
      "[2024-02-16 06:06:37.409688] Entity type age:\n",
      "[2024-02-16 06:06:37.410349] - Found 72 entities\n",
      "[2024-02-16 06:06:37.411120] - Removing the ones with fewer than 1 occurrences...\n",
      "[2024-02-16 06:06:37.411741] - Left with 72 entities\n",
      "[2024-02-16 06:06:37.412501] - Shuffling them...\n",
      "[2024-02-16 06:06:37.418917] Entity type native-country:\n",
      "[2024-02-16 06:06:37.419410] - Found 42 entities\n",
      "[2024-02-16 06:06:37.419856] - Removing the ones with fewer than 1 occurrences...\n",
      "[2024-02-16 06:06:37.420351] - Left with 42 entities\n",
      "[2024-02-16 06:06:37.420801] - Shuffling them...\n",
      "[2024-02-16 06:06:37.421310] Entity type education-num:\n",
      "[2024-02-16 06:06:37.421756] - Found 16 entities\n",
      "[2024-02-16 06:06:37.422190] - Removing the ones with fewer than 1 occurrences...\n",
      "[2024-02-16 06:06:37.423395] - Left with 16 entities\n",
      "[2024-02-16 06:06:37.423824] - Shuffling them...\n",
      "[2024-02-16 06:06:37.424295] Entity type education:\n",
      "[2024-02-16 06:06:37.424749] - Found 16 entities\n",
      "[2024-02-16 06:06:37.425165] - Removing the ones with fewer than 1 occurrences...\n",
      "[2024-02-16 06:06:37.425585] - Left with 16 entities\n",
      "[2024-02-16 06:06:37.425977] - Shuffling them...\n",
      "[2024-02-16 06:06:37.426396] Entity type occupation:\n",
      "[2024-02-16 06:06:37.426795] - Found 15 entities\n",
      "[2024-02-16 06:06:37.427180] - Removing the ones with fewer than 1 occurrences...\n",
      "[2024-02-16 06:06:37.427602] - Left with 15 entities\n",
      "[2024-02-16 06:06:37.428004] - Shuffling them...\n",
      "[2024-02-16 06:06:37.428431] Entity type workclass:\n",
      "[2024-02-16 06:06:37.428854] - Found 9 entities\n",
      "[2024-02-16 06:06:37.429260] - Removing the ones with fewer than 1 occurrences...\n",
      "[2024-02-16 06:06:37.429670] - Left with 9 entities\n",
      "[2024-02-16 06:06:37.430075] - Shuffling them...\n",
      "[2024-02-16 06:06:37.430492] Entity type marital-status:\n",
      "[2024-02-16 06:06:37.430884] - Found 7 entities\n",
      "[2024-02-16 06:06:37.431286] - Removing the ones with fewer than 1 occurrences...\n",
      "[2024-02-16 06:06:37.431696] - Left with 7 entities\n",
      "[2024-02-16 06:06:37.432077] - Shuffling them...\n",
      "[2024-02-16 06:06:37.432512] Entity type relationship:\n",
      "[2024-02-16 06:06:37.432932] - Found 6 entities\n",
      "[2024-02-16 06:06:37.433365] - Removing the ones with fewer than 1 occurrences...\n",
      "[2024-02-16 06:06:37.433775] - Left with 6 entities\n",
      "[2024-02-16 06:06:37.434183] - Shuffling them...\n",
      "[2024-02-16 06:06:37.434601] Entity type race:\n",
      "[2024-02-16 06:06:37.435001] - Found 5 entities\n",
      "[2024-02-16 06:06:37.435421] - Removing the ones with fewer than 1 occurrences...\n",
      "[2024-02-16 06:06:37.435838] - Left with 5 entities\n",
      "[2024-02-16 06:06:37.436242] - Shuffling them...\n",
      "[2024-02-16 06:06:37.436676] Entity type sex:\n",
      "[2024-02-16 06:06:37.437093] - Found 2 entities\n",
      "[2024-02-16 06:06:37.437514] - Removing the ones with fewer than 1 occurrences...\n",
      "[2024-02-16 06:06:37.437920] - Left with 2 entities\n",
      "[2024-02-16 06:06:37.438354] - Shuffling them...\n",
      "[2024-02-16 06:06:37.439173] Preparing counts and dictionaries for entities and relation types:\n",
      "[2024-02-16 06:06:37.443388] - Writing count of entity type fnlwgt and partition 0\n",
      "[2024-02-16 06:06:37.461790] - Writing count of entity type ID and partition 0\n",
      "[2024-02-16 06:06:37.475959] - Writing count of entity type capital-gain and partition 0\n",
      "[2024-02-16 06:06:37.478745] - Writing count of entity type hours-per-week and partition 0\n",
      "[2024-02-16 06:06:37.481242] - Writing count of entity type capital-loss and partition 0\n",
      "[2024-02-16 06:06:37.484892] - Writing count of entity type age and partition 0\n",
      "[2024-02-16 06:06:37.487591] - Writing count of entity type native-country and partition 0\n",
      "[2024-02-16 06:06:37.490112] - Writing count of entity type education-num and partition 0\n",
      "[2024-02-16 06:06:37.492289] - Writing count of entity type education and partition 0\n",
      "[2024-02-16 06:06:37.494629] - Writing count of entity type occupation and partition 0\n",
      "[2024-02-16 06:06:37.496974] - Writing count of entity type workclass and partition 0\n",
      "[2024-02-16 06:06:37.499492] - Writing count of entity type marital-status and partition 0\n",
      "[2024-02-16 06:06:37.502065] - Writing count of entity type relationship and partition 0\n",
      "[2024-02-16 06:06:37.504840] - Writing count of entity type race and partition 0\n",
      "[2024-02-16 06:06:37.507691] - Writing count of entity type sex and partition 0\n",
      "[2024-02-16 06:06:37.511595] Preparing edge path /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/partitions/edges_partitioned_rel_9, out of the edges found in /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/tsvs/rel_9.tsv\n",
      "[2024-02-16 06:06:37.512720] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2024-02-16 06:06:38.312823] - Processed 26072 edges in total\n",
      "[2024-02-16 06:06:38.314621] Preparing edge path /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/partitions/edges_partitioned_rel_6, out of the edges found in /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/tsvs/rel_6.tsv\n",
      "[2024-02-16 06:06:38.315984] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2024-02-16 06:06:38.458327] - Processed 26072 edges in total\n",
      "[2024-02-16 06:06:38.459520] Preparing edge path /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/partitions/edges_partitioned_rel_3, out of the edges found in /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/tsvs/rel_3.tsv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-16 06:06:38.460650] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2024-02-16 06:06:38.698898] - Processed 26072 edges in total\n",
      "[2024-02-16 06:06:38.700022] Preparing edge path /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/partitions/edges_partitioned_rel_14, out of the edges found in /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/tsvs/rel_14.tsv\n",
      "[2024-02-16 06:06:38.701246] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2024-02-16 06:06:38.833017] - Processed 26072 edges in total\n",
      "[2024-02-16 06:06:38.834403] Preparing edge path /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/partitions/edges_partitioned_rel_12, out of the edges found in /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/tsvs/rel_12.tsv\n",
      "[2024-02-16 06:06:38.835569] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2024-02-16 06:06:38.968935] - Processed 26072 edges in total\n",
      "[2024-02-16 06:06:38.969984] Preparing edge path /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/partitions/edges_partitioned_rel_5, out of the edges found in /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/tsvs/rel_5.tsv\n",
      "[2024-02-16 06:06:38.971207] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2024-02-16 06:06:39.103976] - Processed 26072 edges in total\n",
      "[2024-02-16 06:06:39.105114] Preparing edge path /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/partitions/edges_partitioned_rel_8, out of the edges found in /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/tsvs/rel_8.tsv\n",
      "[2024-02-16 06:06:39.106308] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2024-02-16 06:06:39.240541] - Processed 26072 edges in total\n",
      "[2024-02-16 06:06:39.241788] Preparing edge path /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/partitions/edges_partitioned_rel_4, out of the edges found in /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/tsvs/rel_4.tsv\n",
      "[2024-02-16 06:06:39.242912] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2024-02-16 06:06:39.486345] - Processed 26072 edges in total\n",
      "[2024-02-16 06:06:39.487769] Preparing edge path /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/partitions/edges_partitioned_rel_13, out of the edges found in /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/tsvs/rel_13.tsv\n",
      "[2024-02-16 06:06:39.488923] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2024-02-16 06:06:39.628132] - Processed 26072 edges in total\n",
      "[2024-02-16 06:06:39.629333] Preparing edge path /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/partitions/edges_partitioned_rel_7, out of the edges found in /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/tsvs/rel_7.tsv\n",
      "[2024-02-16 06:06:39.630652] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2024-02-16 06:06:39.769505] - Processed 26072 edges in total\n",
      "[2024-02-16 06:06:39.770579] Preparing edge path /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/partitions/edges_partitioned_rel_11, out of the edges found in /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/tsvs/rel_11.tsv\n",
      "[2024-02-16 06:06:39.772060] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2024-02-16 06:06:40.017112] - Processed 26072 edges in total\n",
      "[2024-02-16 06:06:40.018822] Preparing edge path /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/partitions/edges_partitioned_rel_10, out of the edges found in /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/tsvs/rel_10.tsv\n",
      "[2024-02-16 06:06:40.020071] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2024-02-16 06:06:40.158920] - Processed 26072 edges in total\n",
      "[2024-02-16 06:06:40.159953] Preparing edge path /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/partitions/edges_partitioned_rel_1, out of the edges found in /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/tsvs/rel_1.tsv\n",
      "[2024-02-16 06:06:40.161217] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2024-02-16 06:06:40.297041] - Processed 26072 edges in total\n",
      "[2024-02-16 06:06:40.298161] Preparing edge path /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/partitions/edges_partitioned_rel_2, out of the edges found in /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/tsvs/rel_2.tsv\n",
      "[2024-02-16 06:06:40.299305] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2024-02-16 06:06:40.430788] - Processed 26072 edges in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torchbiggraph/util.py:222: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = tensor.storage_type()._new_shared(size.numel())\n",
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torch/storage.py:959: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if self.device.type not in ['cpu', 'cuda']:\n",
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torch/storage.py:962: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  module = torch if self.device.type == 'cpu' else torch.cuda\n",
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torch/storage.py:985: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  untyped_storage = torch.UntypedStorage._new_shared(size * cls()._element_size())\n",
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torch/storage.py:986: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return cls(wrap_storage=untyped_storage)\n",
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torchbiggraph/train_cpu.py:304: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  ).storage()\n",
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torchbiggraph/train_cpu.py:821: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  self.embedding_storage_freelist[entity].add(embs.storage())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Embedding Complete]\n",
      "In __del__: pbg deleted\n",
      "\u001b[94m[2024-02-16 06:07:01,537][ASSET][INFO][train_pipeline][train1]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2024-02-16 06:07:01\n",
      "- current step      : train1\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'center_node_column'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2024-02-16 06:07:01,540][PROCESS][INFO]: ==================== Finish pipeline: train_pipeline / step: train1\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_000</th>\n",
       "      <th>EMB_001</th>\n",
       "      <th>EMB_002</th>\n",
       "      <th>EMB_003</th>\n",
       "      <th>EMB_004</th>\n",
       "      <th>EMB_005</th>\n",
       "      <th>EMB_006</th>\n",
       "      <th>EMB_007</th>\n",
       "      <th>EMB_008</th>\n",
       "      <th>EMB_009</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_119</th>\n",
       "      <th>EMB_120</th>\n",
       "      <th>EMB_121</th>\n",
       "      <th>EMB_122</th>\n",
       "      <th>EMB_123</th>\n",
       "      <th>EMB_124</th>\n",
       "      <th>EMB_125</th>\n",
       "      <th>EMB_126</th>\n",
       "      <th>EMB_127</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.031787</td>\n",
       "      <td>0.005793</td>\n",
       "      <td>-0.017358</td>\n",
       "      <td>0.001166</td>\n",
       "      <td>-0.011912</td>\n",
       "      <td>-0.014894</td>\n",
       "      <td>0.023695</td>\n",
       "      <td>-0.019463</td>\n",
       "      <td>-0.009830</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014636</td>\n",
       "      <td>0.005233</td>\n",
       "      <td>-0.001315</td>\n",
       "      <td>-0.011508</td>\n",
       "      <td>0.020522</td>\n",
       "      <td>0.006062</td>\n",
       "      <td>-0.023483</td>\n",
       "      <td>-0.003045</td>\n",
       "      <td>0.002571</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.012843</td>\n",
       "      <td>-0.003486</td>\n",
       "      <td>-0.014708</td>\n",
       "      <td>0.018442</td>\n",
       "      <td>-0.051791</td>\n",
       "      <td>-0.005231</td>\n",
       "      <td>-0.015779</td>\n",
       "      <td>-0.124171</td>\n",
       "      <td>0.021208</td>\n",
       "      <td>0.050181</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029428</td>\n",
       "      <td>0.029274</td>\n",
       "      <td>0.008022</td>\n",
       "      <td>-0.004988</td>\n",
       "      <td>-0.028250</td>\n",
       "      <td>0.003807</td>\n",
       "      <td>-0.072039</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.063415</td>\n",
       "      <td>-0.041987</td>\n",
       "      <td>-0.051871</td>\n",
       "      <td>-0.003035</td>\n",
       "      <td>0.003508</td>\n",
       "      <td>-0.021524</td>\n",
       "      <td>0.021369</td>\n",
       "      <td>-0.010615</td>\n",
       "      <td>0.013744</td>\n",
       "      <td>0.018987</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021161</td>\n",
       "      <td>-0.001802</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>-0.032344</td>\n",
       "      <td>0.031109</td>\n",
       "      <td>0.008710</td>\n",
       "      <td>-0.012333</td>\n",
       "      <td>0.041268</td>\n",
       "      <td>-0.001675</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.008741</td>\n",
       "      <td>-0.024126</td>\n",
       "      <td>-0.016491</td>\n",
       "      <td>0.003069</td>\n",
       "      <td>-0.003847</td>\n",
       "      <td>-0.000397</td>\n",
       "      <td>-0.012842</td>\n",
       "      <td>-0.018499</td>\n",
       "      <td>0.019406</td>\n",
       "      <td>0.015205</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008201</td>\n",
       "      <td>0.003023</td>\n",
       "      <td>0.011535</td>\n",
       "      <td>-0.008696</td>\n",
       "      <td>-0.006763</td>\n",
       "      <td>-0.002119</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>0.016253</td>\n",
       "      <td>-0.003845</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.045790</td>\n",
       "      <td>-0.004882</td>\n",
       "      <td>-0.029868</td>\n",
       "      <td>-0.000906</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.019189</td>\n",
       "      <td>0.034408</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>-0.013011</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011713</td>\n",
       "      <td>-0.005105</td>\n",
       "      <td>-0.003003</td>\n",
       "      <td>-0.018553</td>\n",
       "      <td>0.033998</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>-0.012937</td>\n",
       "      <td>0.004715</td>\n",
       "      <td>0.006163</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.030210</td>\n",
       "      <td>-0.003547</td>\n",
       "      <td>-0.029647</td>\n",
       "      <td>0.004381</td>\n",
       "      <td>-0.015021</td>\n",
       "      <td>-0.017025</td>\n",
       "      <td>0.026504</td>\n",
       "      <td>-0.033041</td>\n",
       "      <td>-0.000372</td>\n",
       "      <td>0.016892</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020287</td>\n",
       "      <td>0.005612</td>\n",
       "      <td>-0.001324</td>\n",
       "      <td>-0.017195</td>\n",
       "      <td>0.018050</td>\n",
       "      <td>0.009064</td>\n",
       "      <td>-0.033053</td>\n",
       "      <td>0.008564</td>\n",
       "      <td>0.007816</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.017661</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>-0.005678</td>\n",
       "      <td>-0.011856</td>\n",
       "      <td>0.034336</td>\n",
       "      <td>-0.003125</td>\n",
       "      <td>0.019840</td>\n",
       "      <td>0.077678</td>\n",
       "      <td>-0.011791</td>\n",
       "      <td>-0.027356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014332</td>\n",
       "      <td>-0.015012</td>\n",
       "      <td>-0.008219</td>\n",
       "      <td>-0.002600</td>\n",
       "      <td>0.023903</td>\n",
       "      <td>-0.003288</td>\n",
       "      <td>0.046034</td>\n",
       "      <td>-0.011117</td>\n",
       "      <td>0.010053</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.006494</td>\n",
       "      <td>-0.011239</td>\n",
       "      <td>-0.027089</td>\n",
       "      <td>0.014070</td>\n",
       "      <td>-0.046269</td>\n",
       "      <td>-0.009452</td>\n",
       "      <td>-0.010248</td>\n",
       "      <td>-0.117692</td>\n",
       "      <td>0.021509</td>\n",
       "      <td>0.051211</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032783</td>\n",
       "      <td>0.025134</td>\n",
       "      <td>0.010483</td>\n",
       "      <td>-0.009364</td>\n",
       "      <td>-0.013481</td>\n",
       "      <td>0.008869</td>\n",
       "      <td>-0.068844</td>\n",
       "      <td>0.020472</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.027000</td>\n",
       "      <td>-0.026708</td>\n",
       "      <td>-0.030103</td>\n",
       "      <td>0.005016</td>\n",
       "      <td>-0.009542</td>\n",
       "      <td>-0.012196</td>\n",
       "      <td>0.007530</td>\n",
       "      <td>-0.033858</td>\n",
       "      <td>0.006622</td>\n",
       "      <td>0.019057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016460</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.011368</td>\n",
       "      <td>-0.021313</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.004780</td>\n",
       "      <td>-0.021526</td>\n",
       "      <td>0.025197</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.006076</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>-0.016305</td>\n",
       "      <td>0.013066</td>\n",
       "      <td>-0.035905</td>\n",
       "      <td>-0.005674</td>\n",
       "      <td>-0.004641</td>\n",
       "      <td>-0.082531</td>\n",
       "      <td>0.003839</td>\n",
       "      <td>0.027448</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024313</td>\n",
       "      <td>0.012237</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>-0.001625</td>\n",
       "      <td>-0.003968</td>\n",
       "      <td>0.010324</td>\n",
       "      <td>-0.048183</td>\n",
       "      <td>-0.002195</td>\n",
       "      <td>0.003067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    EMB_000   EMB_001   EMB_002   EMB_003   EMB_004   EMB_005   EMB_006  \\\n",
       "0 -0.031787  0.005793 -0.017358  0.001166 -0.011912 -0.014894  0.023695   \n",
       "1  0.012843 -0.003486 -0.014708  0.018442 -0.051791 -0.005231 -0.015779   \n",
       "2 -0.063415 -0.041987 -0.051871 -0.003035  0.003508 -0.021524  0.021369   \n",
       "3 -0.008741 -0.024126 -0.016491  0.003069 -0.003847 -0.000397 -0.012842   \n",
       "4 -0.045790 -0.004882 -0.029868 -0.000906  0.000048 -0.019189  0.034408   \n",
       "5 -0.030210 -0.003547 -0.029647  0.004381 -0.015021 -0.017025  0.026504   \n",
       "6 -0.017661  0.003340 -0.005678 -0.011856  0.034336 -0.003125  0.019840   \n",
       "7 -0.006494 -0.011239 -0.027089  0.014070 -0.046269 -0.009452 -0.010248   \n",
       "8 -0.027000 -0.026708 -0.030103  0.005016 -0.009542 -0.012196  0.007530   \n",
       "9 -0.006076  0.002377 -0.016305  0.013066 -0.035905 -0.005674 -0.004641   \n",
       "\n",
       "    EMB_007   EMB_008   EMB_009  ...   EMB_119   EMB_120   EMB_121   EMB_122  \\\n",
       "0 -0.019463 -0.009830  0.007817  ... -0.014636  0.005233 -0.001315 -0.011508   \n",
       "1 -0.124171  0.021208  0.050181  ... -0.029428  0.029274  0.008022 -0.004988   \n",
       "2 -0.010615  0.013744  0.018987  ... -0.021161 -0.001802  0.019800 -0.032344   \n",
       "3 -0.018499  0.019406  0.015205  ... -0.008201  0.003023  0.011535 -0.008696   \n",
       "4  0.003233 -0.013011  0.001010  ... -0.011713 -0.005105 -0.003003 -0.018553   \n",
       "5 -0.033041 -0.000372  0.016892  ... -0.020287  0.005612 -0.001324 -0.017195   \n",
       "6  0.077678 -0.011791 -0.027356  ...  0.014332 -0.015012 -0.008219 -0.002600   \n",
       "7 -0.117692  0.021509  0.051211  ... -0.032783  0.025134  0.010483 -0.009364   \n",
       "8 -0.033858  0.006622  0.019057  ... -0.016460  0.002959  0.011368 -0.021313   \n",
       "9 -0.082531  0.003839  0.027448  ... -0.024313  0.012237  0.000493 -0.001625   \n",
       "\n",
       "    EMB_123   EMB_124   EMB_125   EMB_126   EMB_127  target  \n",
       "0  0.020522  0.006062 -0.023483 -0.003045  0.002571       1  \n",
       "1 -0.028250  0.003807 -0.072039  0.013300  0.001254       1  \n",
       "2  0.031109  0.008710 -0.012333  0.041268 -0.001675       1  \n",
       "3 -0.006763 -0.002119  0.002639  0.016253 -0.003845       1  \n",
       "4  0.033998  0.011407 -0.012937  0.004715  0.006163       1  \n",
       "5  0.018050  0.009064 -0.033053  0.008564  0.007816       1  \n",
       "6  0.023903 -0.003288  0.046034 -0.011117  0.010053       1  \n",
       "7 -0.013481  0.008869 -0.068844  0.020472  0.000189       0  \n",
       "8  0.002418  0.004780 -0.021526  0.025197  0.001196       0  \n",
       "9 -0.003968  0.010324 -0.048183 -0.002195  0.003067       0  \n",
       "\n",
       "[10 rows x 129 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asset 실행\n",
    "train1_asset_structure=run(step, pipeline, alo.asset_structure)\n",
    "\n",
    "# graph asset의 결과 dataframe은 train1_asset_structure.data['dataframe']으로 확인할 수 있습니다. \n",
    "train1_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9047dd80-a5c6-4393-aeb3-327e02a73ddd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [3] Preprocess asset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7698dd0-9153-46c0-97d8-6971ab8f3b2c",
   "metadata": {},
   "source": [
    "GCR은 데이터 전처리가 불필요하기 때문에 Preprocess asset의 역할은 크지 않습니다. 다만 사용자가 임베딩 외에 raw data를 학습에 사용하는 경우 (즉, extra_columns_for_ml 설정시) 결측치를 처리하기 위한 용도입니다.\n",
    "#### 주요 Parameter\n",
    "- handling_missing : 결측치 처리 방식을 지정합니다. 'interpolation' 또는 'fill_number' 중에 선택할 수 있으며 GCR에서는 'interpolation'을 권장합니다.\n",
    "- ***handling_encoding_y_column*** : input asset의 y_column과 동일하게 설정합니다. (필수)\n",
    "- ***handling_encoding_y*** : y_column의 인코딩 방식을 설정합니다. GCR에서는 'label'로 설정합니다.\n",
    "- handling_scaling_x : X 컬럼의 scaling 방식을 선택합니다.\n",
    "- load_train_preprocess : False로 설정합니다. (inference workflow 전용)\n",
    "\n",
    "#### Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91e75b62-dd0c-441a-a8db-eb6d15a4345b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'handling_missing': 'interpolation',\n",
       " 'handling_encoding_y_column': 'target',\n",
       " 'handling_encoding_y': 'label',\n",
       " 'handling_scaling_x': 'none',\n",
       " 'load_train_preprocess': False}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - readiness(1) - train1(2) - preprocess(3) - train2(4) - output(5))\n",
    "step = 3 \n",
    "alo.asset_structure = copy.deepcopy(train1_asset_structure)\n",
    "alo.asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 preprocess asset argument를 원하는 값으로 수정합니다. \n",
    "# asset_structure.args['handling_missing'] = dropna\n",
    "alo.asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d78a8-0a32-4f54-aac7-bff907e2ae94",
   "metadata": {},
   "source": [
    "#### Preprocess asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a116e5ee-0eaa-4cad-9e8b-b7452f7068be",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[2024-02-16 06:15:15,129][ASSET][INFO][train_pipeline][preprocess]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/preprocess/\u001b[0m\n",
      "\u001b[94m[2024-02-16 06:15:15,132][ASSET][INFO][train_pipeline][preprocess]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2024-02-16 06:15:15\n",
      "- current step      : preprocess\n",
      "- asset branch.     : release-1.2\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : 2.1\n",
      "- load envs. keys   : dict_keys(['project_home', 'solution_metadata_version', 'artifacts', 'alo_version', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'pipeline', 'step', 'num_step', 'asset_branch', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['handling_missing', 'handling_encoding_y_column', 'handling_encoding_y', 'handling_scaling_x', 'load_train_preprocess'])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'center_node_column'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "target column : label Encoder saved : /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/preprocess/\n",
      "['EMB_000_nan', 'EMB_001_nan', 'EMB_002_nan', 'EMB_003_nan', 'EMB_004_nan', 'EMB_005_nan', 'EMB_006_nan', 'EMB_007_nan', 'EMB_008_nan', 'EMB_009_nan', 'EMB_010_nan', 'EMB_011_nan', 'EMB_012_nan', 'EMB_013_nan', 'EMB_014_nan', 'EMB_015_nan', 'EMB_016_nan', 'EMB_017_nan', 'EMB_018_nan', 'EMB_019_nan', 'EMB_020_nan', 'EMB_021_nan', 'EMB_022_nan', 'EMB_023_nan', 'EMB_024_nan', 'EMB_025_nan', 'EMB_026_nan', 'EMB_027_nan', 'EMB_028_nan', 'EMB_029_nan', 'EMB_030_nan', 'EMB_031_nan', 'EMB_032_nan', 'EMB_033_nan', 'EMB_034_nan', 'EMB_035_nan', 'EMB_036_nan', 'EMB_037_nan', 'EMB_038_nan', 'EMB_039_nan', 'EMB_040_nan', 'EMB_041_nan', 'EMB_042_nan', 'EMB_043_nan', 'EMB_044_nan', 'EMB_045_nan', 'EMB_046_nan', 'EMB_047_nan', 'EMB_048_nan', 'EMB_049_nan', 'EMB_050_nan', 'EMB_051_nan', 'EMB_052_nan', 'EMB_053_nan', 'EMB_054_nan', 'EMB_055_nan', 'EMB_056_nan', 'EMB_057_nan', 'EMB_058_nan', 'EMB_059_nan', 'EMB_060_nan', 'EMB_061_nan', 'EMB_062_nan', 'EMB_063_nan', 'EMB_064_nan', 'EMB_065_nan', 'EMB_066_nan', 'EMB_067_nan', 'EMB_068_nan', 'EMB_069_nan', 'EMB_070_nan', 'EMB_071_nan', 'EMB_072_nan', 'EMB_073_nan', 'EMB_074_nan', 'EMB_075_nan', 'EMB_076_nan', 'EMB_077_nan', 'EMB_078_nan', 'EMB_079_nan', 'EMB_080_nan', 'EMB_081_nan', 'EMB_082_nan', 'EMB_083_nan', 'EMB_084_nan', 'EMB_085_nan', 'EMB_086_nan', 'EMB_087_nan', 'EMB_088_nan', 'EMB_089_nan', 'EMB_090_nan', 'EMB_091_nan', 'EMB_092_nan', 'EMB_093_nan', 'EMB_094_nan', 'EMB_095_nan', 'EMB_096_nan', 'EMB_097_nan', 'EMB_098_nan', 'EMB_099_nan', 'EMB_100_nan', 'EMB_101_nan', 'EMB_102_nan', 'EMB_103_nan', 'EMB_104_nan', 'EMB_105_nan', 'EMB_106_nan', 'EMB_107_nan', 'EMB_108_nan', 'EMB_109_nan', 'EMB_110_nan', 'EMB_111_nan', 'EMB_112_nan', 'EMB_113_nan', 'EMB_114_nan', 'EMB_115_nan', 'EMB_116_nan', 'EMB_117_nan', 'EMB_118_nan', 'EMB_119_nan', 'EMB_120_nan', 'EMB_121_nan', 'EMB_122_nan', 'EMB_123_nan', 'EMB_124_nan', 'EMB_125_nan', 'EMB_126_nan', 'EMB_127_nan'] target_encoded_nan\n",
      "\u001b[94m[2024-02-16 06:15:15,274][ASSET][INFO][train_pipeline][preprocess]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2024-02-16 06:15:15\n",
      "- current step      : preprocess\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'center_node_column', 'columns_map', 'preprocess'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2024-02-16 06:15:15,277][PROCESS][INFO]: ==================== Finish pipeline: train_pipeline / step: preprocess\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_000</th>\n",
       "      <th>EMB_001</th>\n",
       "      <th>EMB_002</th>\n",
       "      <th>EMB_003</th>\n",
       "      <th>EMB_004</th>\n",
       "      <th>EMB_005</th>\n",
       "      <th>EMB_006</th>\n",
       "      <th>EMB_007</th>\n",
       "      <th>EMB_008</th>\n",
       "      <th>EMB_009</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_119_nan</th>\n",
       "      <th>EMB_120_nan</th>\n",
       "      <th>EMB_121_nan</th>\n",
       "      <th>EMB_122_nan</th>\n",
       "      <th>EMB_123_nan</th>\n",
       "      <th>EMB_124_nan</th>\n",
       "      <th>EMB_125_nan</th>\n",
       "      <th>EMB_126_nan</th>\n",
       "      <th>EMB_127_nan</th>\n",
       "      <th>target_encoded_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.031787</td>\n",
       "      <td>0.005793</td>\n",
       "      <td>-0.017358</td>\n",
       "      <td>0.001166</td>\n",
       "      <td>-0.011912</td>\n",
       "      <td>-0.014894</td>\n",
       "      <td>0.023695</td>\n",
       "      <td>-0.019463</td>\n",
       "      <td>-0.009830</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014636</td>\n",
       "      <td>0.005233</td>\n",
       "      <td>-0.001315</td>\n",
       "      <td>-0.011508</td>\n",
       "      <td>0.020522</td>\n",
       "      <td>0.006062</td>\n",
       "      <td>-0.023483</td>\n",
       "      <td>-0.003045</td>\n",
       "      <td>0.002571</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.012843</td>\n",
       "      <td>-0.003486</td>\n",
       "      <td>-0.014708</td>\n",
       "      <td>0.018442</td>\n",
       "      <td>-0.051791</td>\n",
       "      <td>-0.005231</td>\n",
       "      <td>-0.015779</td>\n",
       "      <td>-0.124171</td>\n",
       "      <td>0.021208</td>\n",
       "      <td>0.050181</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029428</td>\n",
       "      <td>0.029274</td>\n",
       "      <td>0.008022</td>\n",
       "      <td>-0.004988</td>\n",
       "      <td>-0.028250</td>\n",
       "      <td>0.003807</td>\n",
       "      <td>-0.072039</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.063415</td>\n",
       "      <td>-0.041987</td>\n",
       "      <td>-0.051871</td>\n",
       "      <td>-0.003035</td>\n",
       "      <td>0.003508</td>\n",
       "      <td>-0.021524</td>\n",
       "      <td>0.021369</td>\n",
       "      <td>-0.010615</td>\n",
       "      <td>0.013744</td>\n",
       "      <td>0.018987</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021161</td>\n",
       "      <td>-0.001802</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>-0.032344</td>\n",
       "      <td>0.031109</td>\n",
       "      <td>0.008710</td>\n",
       "      <td>-0.012333</td>\n",
       "      <td>0.041268</td>\n",
       "      <td>-0.001675</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.008741</td>\n",
       "      <td>-0.024126</td>\n",
       "      <td>-0.016491</td>\n",
       "      <td>0.003069</td>\n",
       "      <td>-0.003847</td>\n",
       "      <td>-0.000397</td>\n",
       "      <td>-0.012842</td>\n",
       "      <td>-0.018499</td>\n",
       "      <td>0.019406</td>\n",
       "      <td>0.015205</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008201</td>\n",
       "      <td>0.003023</td>\n",
       "      <td>0.011535</td>\n",
       "      <td>-0.008696</td>\n",
       "      <td>-0.006763</td>\n",
       "      <td>-0.002119</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>0.016253</td>\n",
       "      <td>-0.003845</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.045790</td>\n",
       "      <td>-0.004882</td>\n",
       "      <td>-0.029868</td>\n",
       "      <td>-0.000906</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.019189</td>\n",
       "      <td>0.034408</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>-0.013011</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011713</td>\n",
       "      <td>-0.005105</td>\n",
       "      <td>-0.003003</td>\n",
       "      <td>-0.018553</td>\n",
       "      <td>0.033998</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>-0.012937</td>\n",
       "      <td>0.004715</td>\n",
       "      <td>0.006163</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.030210</td>\n",
       "      <td>-0.003547</td>\n",
       "      <td>-0.029647</td>\n",
       "      <td>0.004381</td>\n",
       "      <td>-0.015021</td>\n",
       "      <td>-0.017025</td>\n",
       "      <td>0.026504</td>\n",
       "      <td>-0.033041</td>\n",
       "      <td>-0.000372</td>\n",
       "      <td>0.016892</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020287</td>\n",
       "      <td>0.005612</td>\n",
       "      <td>-0.001324</td>\n",
       "      <td>-0.017195</td>\n",
       "      <td>0.018050</td>\n",
       "      <td>0.009064</td>\n",
       "      <td>-0.033053</td>\n",
       "      <td>0.008564</td>\n",
       "      <td>0.007816</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.017661</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>-0.005678</td>\n",
       "      <td>-0.011856</td>\n",
       "      <td>0.034336</td>\n",
       "      <td>-0.003125</td>\n",
       "      <td>0.019840</td>\n",
       "      <td>0.077678</td>\n",
       "      <td>-0.011791</td>\n",
       "      <td>-0.027356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014332</td>\n",
       "      <td>-0.015012</td>\n",
       "      <td>-0.008219</td>\n",
       "      <td>-0.002600</td>\n",
       "      <td>0.023903</td>\n",
       "      <td>-0.003288</td>\n",
       "      <td>0.046034</td>\n",
       "      <td>-0.011117</td>\n",
       "      <td>0.010053</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.006494</td>\n",
       "      <td>-0.011239</td>\n",
       "      <td>-0.027089</td>\n",
       "      <td>0.014070</td>\n",
       "      <td>-0.046269</td>\n",
       "      <td>-0.009452</td>\n",
       "      <td>-0.010248</td>\n",
       "      <td>-0.117692</td>\n",
       "      <td>0.021509</td>\n",
       "      <td>0.051211</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032783</td>\n",
       "      <td>0.025134</td>\n",
       "      <td>0.010483</td>\n",
       "      <td>-0.009364</td>\n",
       "      <td>-0.013481</td>\n",
       "      <td>0.008869</td>\n",
       "      <td>-0.068844</td>\n",
       "      <td>0.020472</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.027000</td>\n",
       "      <td>-0.026708</td>\n",
       "      <td>-0.030103</td>\n",
       "      <td>0.005016</td>\n",
       "      <td>-0.009542</td>\n",
       "      <td>-0.012196</td>\n",
       "      <td>0.007530</td>\n",
       "      <td>-0.033858</td>\n",
       "      <td>0.006622</td>\n",
       "      <td>0.019057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016460</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.011368</td>\n",
       "      <td>-0.021313</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.004780</td>\n",
       "      <td>-0.021526</td>\n",
       "      <td>0.025197</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.006076</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>-0.016305</td>\n",
       "      <td>0.013066</td>\n",
       "      <td>-0.035905</td>\n",
       "      <td>-0.005674</td>\n",
       "      <td>-0.004641</td>\n",
       "      <td>-0.082531</td>\n",
       "      <td>0.003839</td>\n",
       "      <td>0.027448</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024313</td>\n",
       "      <td>0.012237</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>-0.001625</td>\n",
       "      <td>-0.003968</td>\n",
       "      <td>0.010324</td>\n",
       "      <td>-0.048183</td>\n",
       "      <td>-0.002195</td>\n",
       "      <td>0.003067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 259 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    EMB_000   EMB_001   EMB_002   EMB_003   EMB_004   EMB_005   EMB_006  \\\n",
       "0 -0.031787  0.005793 -0.017358  0.001166 -0.011912 -0.014894  0.023695   \n",
       "1  0.012843 -0.003486 -0.014708  0.018442 -0.051791 -0.005231 -0.015779   \n",
       "2 -0.063415 -0.041987 -0.051871 -0.003035  0.003508 -0.021524  0.021369   \n",
       "3 -0.008741 -0.024126 -0.016491  0.003069 -0.003847 -0.000397 -0.012842   \n",
       "4 -0.045790 -0.004882 -0.029868 -0.000906  0.000048 -0.019189  0.034408   \n",
       "5 -0.030210 -0.003547 -0.029647  0.004381 -0.015021 -0.017025  0.026504   \n",
       "6 -0.017661  0.003340 -0.005678 -0.011856  0.034336 -0.003125  0.019840   \n",
       "7 -0.006494 -0.011239 -0.027089  0.014070 -0.046269 -0.009452 -0.010248   \n",
       "8 -0.027000 -0.026708 -0.030103  0.005016 -0.009542 -0.012196  0.007530   \n",
       "9 -0.006076  0.002377 -0.016305  0.013066 -0.035905 -0.005674 -0.004641   \n",
       "\n",
       "    EMB_007   EMB_008   EMB_009  ...  EMB_119_nan  EMB_120_nan  EMB_121_nan  \\\n",
       "0 -0.019463 -0.009830  0.007817  ...    -0.014636     0.005233    -0.001315   \n",
       "1 -0.124171  0.021208  0.050181  ...    -0.029428     0.029274     0.008022   \n",
       "2 -0.010615  0.013744  0.018987  ...    -0.021161    -0.001802     0.019800   \n",
       "3 -0.018499  0.019406  0.015205  ...    -0.008201     0.003023     0.011535   \n",
       "4  0.003233 -0.013011  0.001010  ...    -0.011713    -0.005105    -0.003003   \n",
       "5 -0.033041 -0.000372  0.016892  ...    -0.020287     0.005612    -0.001324   \n",
       "6  0.077678 -0.011791 -0.027356  ...     0.014332    -0.015012    -0.008219   \n",
       "7 -0.117692  0.021509  0.051211  ...    -0.032783     0.025134     0.010483   \n",
       "8 -0.033858  0.006622  0.019057  ...    -0.016460     0.002959     0.011368   \n",
       "9 -0.082531  0.003839  0.027448  ...    -0.024313     0.012237     0.000493   \n",
       "\n",
       "   EMB_122_nan  EMB_123_nan  EMB_124_nan  EMB_125_nan  EMB_126_nan  \\\n",
       "0    -0.011508     0.020522     0.006062    -0.023483    -0.003045   \n",
       "1    -0.004988    -0.028250     0.003807    -0.072039     0.013300   \n",
       "2    -0.032344     0.031109     0.008710    -0.012333     0.041268   \n",
       "3    -0.008696    -0.006763    -0.002119     0.002639     0.016253   \n",
       "4    -0.018553     0.033998     0.011407    -0.012937     0.004715   \n",
       "5    -0.017195     0.018050     0.009064    -0.033053     0.008564   \n",
       "6    -0.002600     0.023903    -0.003288     0.046034    -0.011117   \n",
       "7    -0.009364    -0.013481     0.008869    -0.068844     0.020472   \n",
       "8    -0.021313     0.002418     0.004780    -0.021526     0.025197   \n",
       "9    -0.001625    -0.003968     0.010324    -0.048183    -0.002195   \n",
       "\n",
       "   EMB_127_nan  target_encoded_nan  \n",
       "0     0.002571                   1  \n",
       "1     0.001254                   1  \n",
       "2    -0.001675                   1  \n",
       "3    -0.003845                   1  \n",
       "4     0.006163                   1  \n",
       "5     0.007816                   1  \n",
       "6     0.010053                   1  \n",
       "7     0.000189                   0  \n",
       "8     0.001196                   0  \n",
       "9     0.003067                   0  \n",
       "\n",
       "[10 rows x 259 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asset 실행\n",
    "preprocess_asset_structure=run(step, pipeline, alo.asset_structure)\n",
    "\n",
    "# preprocess asset의 결과 dataframe은 preprocess_asset_structure.data['dataframe']으로 확인할 수 있습니다.  \n",
    "preprocess_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b09311e-a704-425b-8203-180f3033d808",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [4] Train2 asset (과거 train asset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6a3a7-2204-42b5-8cc8-a5b5f740f457",
   "metadata": {},
   "source": [
    "추출된 임베딩을 활용하여 ML모델을 학습합니다.\n",
    "#### 주요 Parameter\n",
    "- model_type: 목적에 맞는 학습 방식을 선택합니다. (classification/regression)\n",
    "- data_split_method: HPO를 위한 데이터 분할 방식을 선택합니다. (cross_validate/train_test_split)\n",
    "- evaluation_metric: classification의 경우 accuracy, precision, recall, f1-score / regression의 경우 mse, r2, mae, rmse 중 선택합니다.\n",
    "- model_list: lightgbm, random-forest, gbm, Catboost 중 복수 선택 가능합니다.\n",
    "- num_hpo: 설정 범위 내 hpo 횟수를 결정합니다.\n",
    "- param_range: Search 범위를 지정합니다.\n",
    "- shap_ratio: shap value 뽑을 데이터를 sampling 하는 비율을 결정합니다.\n",
    "\n",
    "#### Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39f0149f-bfb4-4223-835a-769fd57594bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_type': 'classification',\n",
       " 'data_split_method': 'cross_validate',\n",
       " 'evaluation_metric': 'accuracy',\n",
       " 'model_list': ['lgb'],\n",
       " 'num_hpo': 3,\n",
       " 'param_range': {'rf': {'max_depth': 6, 'n_estimators': [300, 500]},\n",
       "  'gbm': {'max_depth': [5, 7], 'n_estimators': [300, 500]},\n",
       "  'ngb': {'col_sample': [0.6, 0.8], 'n_estimators': [100, 300]},\n",
       "  'lgb': {'max_depth': [5, 9], 'n_estimators': [300, 500]},\n",
       "  'cb': {'max_depth': [5, 9], 'n_estimators': [100, 500]}},\n",
       " 'shap_ratio': 0.001}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - readiness(1) - train1(2) - preprocess(3) - train2(4) - output(5))\n",
    "step = 4 \n",
    "alo.asset_structure = copy.deepcopy(preprocess_asset_structure)\n",
    "alo.asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 train2 asset argument를 원하는 값으로 수정합니다.\n",
    "alo.asset_structure.args['model_list'] = ['lgb']\n",
    "alo.asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c0b28-561d-4815-b4fc-f2078e1a0f59",
   "metadata": {},
   "source": [
    "#### Train2 asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c2a5dcd-b0b8-455d-8b68-3483238a5aae",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "[2024-02-16 06:19:26,207][USER][INFO][train_pipeline][train2]: yaml에 입력된 task가 실행됩니다: classification\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "font: ['NanumBarunGothic']\n",
      "\u001b[92m[2024-02-16 06:19:26,169][ASSET][INFO][train_pipeline][train2]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/\u001b[0m\n",
      "\u001b[92m[2024-02-16 06:19:26,192][ASSET][INFO][train_pipeline][train2]: Successfully got << output path >> for saving your data into csv or jpg file: \n",
      " /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/output/train2/ \n",
      " - [NOTE] The names of output file must be fixed as << output.csv, output.jpg >> \u001b[0m\n",
      "\u001b[94m[2024-02-16 06:19:26,195][ASSET][INFO][train_pipeline][train2]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2024-02-16 06:19:26\n",
      "- current step      : train2\n",
      "- asset branch.     : tcr_v1.1.4\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : 2.1\n",
      "- load envs. keys   : dict_keys(['project_home', 'solution_metadata_version', 'artifacts', 'alo_version', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'pipeline', 'step', 'num_step', 'asset_branch', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['model_type', 'data_split_method', 'evaluation_metric', 'model_list', 'num_hpo', 'param_range', 'shap_ratio'])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'center_node_column', 'columns_map', 'preprocess'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[92m[2024-02-16 06:19:26,198][ASSET][INFO][train_pipeline][train2]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/\u001b[0m\n",
      "\u001b[92m[2024-02-16 06:19:26,201][ASSET][INFO][train_pipeline][train2]: Successfully got << output path >> for saving your data into csv or jpg file: \n",
      " /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/output/train2/ \n",
      " - [NOTE] The names of output file must be fixed as << output.csv, output.jpg >> \u001b[0m\n",
      "\u001b[92m[2024-02-16 06:19:26,204][ASSET][INFO][train_pipeline][train2]: Successfully got << report path >> for saving your << report.html >> file: \n",
      " /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/report/\u001b[0m\n",
      "해당 column 은 Training 과정에 사용되지 않습니다. (column_name: ['EMB_052', 'EMB_020', 'EMB_078', 'EMB_018', 'EMB_077', 'EMB_088', 'EMB_028', 'EMB_106', 'EMB_039', 'EMB_105', 'EMB_000', 'EMB_046', 'EMB_069', 'EMB_009', 'EMB_076', 'EMB_007', 'EMB_056', 'EMB_045', 'EMB_050', 'EMB_087', 'EMB_075', 'EMB_015', 'EMB_104', 'EMB_041', 'EMB_048', 'EMB_101', 'EMB_111', 'EMB_116', 'EMB_107', 'EMB_127', 'EMB_113', 'EMB_068', 'EMB_103', 'EMB_070', 'EMB_100', 'EMB_124', 'EMB_049', 'EMB_006', 'EMB_054', 'EMB_044', 'EMB_074', 'EMB_108', 'EMB_012', 'EMB_038', 'EMB_057', 'EMB_034', 'EMB_062', 'EMB_122', 'EMB_025', 'EMB_051', 'EMB_055', 'EMB_080', 'EMB_084', 'EMB_005', 'EMB_083', 'EMB_004', 'EMB_063', 'EMB_024', 'EMB_033', 'EMB_059', 'EMB_120', 'EMB_089', 'EMB_030', 'EMB_114', 'EMB_097', 'EMB_014', 'EMB_036', 'EMB_042', 'EMB_060', 'EMB_109', 'EMB_061', 'EMB_095', 'EMB_110', 'EMB_065', 'EMB_079', 'EMB_091', 'EMB_053', 'EMB_011', 'EMB_058', 'EMB_019', 'EMB_023', 'EMB_098', 'EMB_066', 'EMB_047', 'EMB_016', 'EMB_029', 'EMB_021', 'EMB_090', 'EMB_043', 'EMB_072', 'EMB_125', 'EMB_126', 'EMB_067', 'EMB_002', 'target_encoded', 'target', 'EMB_092', 'EMB_118', 'EMB_037', 'EMB_093', 'EMB_082', 'EMB_115', 'EMB_094', 'EMB_026', 'EMB_027', 'EMB_112', 'EMB_121', 'EMB_064', 'EMB_040', 'EMB_073', 'EMB_013', 'EMB_096', 'EMB_022', 'EMB_123', 'EMB_010', 'EMB_008', 'EMB_086', 'EMB_001', 'EMB_099', 'EMB_071', 'EMB_017', 'EMB_119', 'EMB_035', 'EMB_085', 'EMB_031', 'EMB_102', 'target_encoded_nan', 'EMB_117', 'EMB_081', 'EMB_032', 'EMB_003'])\n",
      "[INFO] 모델 학습을 시작합니다.\n",
      "\n",
      "crossvalidate 방법을 이용하여 3개 조합의 모델에 대해 총 12번 학습을 시작합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:14<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습이 완료되었습니다.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@scoring_classification func. - label list:  {0, 1}@scoring_classification func. - label list: @scoring_classification func. - label list: \n",
      "  {0, 1}{0, 1}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[INFO] 평가 지표는 ( accuracy ) 를 사용합니다. \n",
      "모델 정보 로그를 저장합니다. (저장위치: /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/model_selection.json)\n",
      "\n",
      "Top 1 model file is saved: /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/best_model_top0.pkl\n",
      "[Score] accuracy: 0.8359\n",
      "[Hyper-parameters] max_depth: 5, n_estimators: 300, n_jobs: 1, verbose: -1, \n",
      "\n",
      "Top 2 model file is saved: /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/best_model_top1.pkl\n",
      "[Score] accuracy: 0.8340\n",
      "[Hyper-parameters] max_depth: 7, n_estimators: 400, n_jobs: 1, verbose: -1, \n",
      "\n",
      "Top 3 model file is saved: /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/best_model_top2.pkl\n",
      "[Score] accuracy: 0.8322\n",
      "[Hyper-parameters] max_depth: 9, n_estimators: 500, n_jobs: 1, verbose: -1, \n",
      "\n",
      "Following model is the best: LGBMClassifier_set0 / accuracy:0.8359\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Summary_plot for Train data 를 저장했습니다.\n",
      "\n",
      "ignore columns와 X로 지정한 데이터 프레임을 합치는 과정중에 에러가 발생했습니다. 확인 부탁드립니다.\n",
      "\u001b[94m[2024-02-16 06:20:05,310][ASSET][INFO][train_pipeline][train2]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2024-02-16 06:20:05\n",
      "- current step      : train2\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'center_node_column', 'columns_map', 'preprocess'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2024-02-16 06:20:05,313][PROCESS][INFO]: ==================== Finish pipeline: train_pipeline / step: train2\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_000_nan</th>\n",
       "      <th>EMB_001_nan</th>\n",
       "      <th>EMB_002_nan</th>\n",
       "      <th>EMB_003_nan</th>\n",
       "      <th>EMB_004_nan</th>\n",
       "      <th>EMB_005_nan</th>\n",
       "      <th>EMB_006_nan</th>\n",
       "      <th>EMB_007_nan</th>\n",
       "      <th>EMB_008_nan</th>\n",
       "      <th>EMB_009_nan</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_126_nan_shapley</th>\n",
       "      <th>EMB_127_nan_shapley</th>\n",
       "      <th>target_encoded_nan</th>\n",
       "      <th>pred_target_encoded_nan</th>\n",
       "      <th>pred_target_encoded_nan_best0</th>\n",
       "      <th>pred_target_encoded_nan_best1</th>\n",
       "      <th>pred_target_encoded_nan_best2</th>\n",
       "      <th>prob_0</th>\n",
       "      <th>prob_1</th>\n",
       "      <th>train_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.074153</td>\n",
       "      <td>-0.055988</td>\n",
       "      <td>-0.052818</td>\n",
       "      <td>0.005892</td>\n",
       "      <td>0.015515</td>\n",
       "      <td>-0.021260</td>\n",
       "      <td>0.026566</td>\n",
       "      <td>0.017620</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>-0.001954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034957</td>\n",
       "      <td>0.122719</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.999946</td>\n",
       "      <td>1th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.068774</td>\n",
       "      <td>-0.049618</td>\n",
       "      <td>-0.058634</td>\n",
       "      <td>0.009538</td>\n",
       "      <td>0.005547</td>\n",
       "      <td>-0.021479</td>\n",
       "      <td>0.025170</td>\n",
       "      <td>-0.010637</td>\n",
       "      <td>0.005298</td>\n",
       "      <td>0.009066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006115</td>\n",
       "      <td>-0.162118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011164</td>\n",
       "      <td>0.988836</td>\n",
       "      <td>0th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.033640</td>\n",
       "      <td>-0.032654</td>\n",
       "      <td>-0.017567</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>-0.008067</td>\n",
       "      <td>0.017446</td>\n",
       "      <td>0.049978</td>\n",
       "      <td>-0.011376</td>\n",
       "      <td>-0.024331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017734</td>\n",
       "      <td>0.073409</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.999555</td>\n",
       "      <td>2th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.055558</td>\n",
       "      <td>-0.024381</td>\n",
       "      <td>-0.035403</td>\n",
       "      <td>0.003343</td>\n",
       "      <td>0.007629</td>\n",
       "      <td>-0.023969</td>\n",
       "      <td>0.032309</td>\n",
       "      <td>0.009788</td>\n",
       "      <td>-0.018074</td>\n",
       "      <td>-0.006553</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000854</td>\n",
       "      <td>0.174334</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.521614</td>\n",
       "      <td>0.478386</td>\n",
       "      <td>1th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.019305</td>\n",
       "      <td>-0.013052</td>\n",
       "      <td>-0.031120</td>\n",
       "      <td>0.008044</td>\n",
       "      <td>-0.031994</td>\n",
       "      <td>-0.012280</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>-0.081808</td>\n",
       "      <td>0.015399</td>\n",
       "      <td>0.039381</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009817</td>\n",
       "      <td>-0.097222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.606196</td>\n",
       "      <td>0.393804</td>\n",
       "      <td>3th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001473</td>\n",
       "      <td>-0.021499</td>\n",
       "      <td>-0.013175</td>\n",
       "      <td>0.013053</td>\n",
       "      <td>-0.029639</td>\n",
       "      <td>-0.006258</td>\n",
       "      <td>-0.016850</td>\n",
       "      <td>-0.078868</td>\n",
       "      <td>0.014410</td>\n",
       "      <td>0.029548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005287</td>\n",
       "      <td>0.088159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.717698</td>\n",
       "      <td>0.282302</td>\n",
       "      <td>3th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.045707</td>\n",
       "      <td>-0.011426</td>\n",
       "      <td>-0.021226</td>\n",
       "      <td>-0.004355</td>\n",
       "      <td>0.013403</td>\n",
       "      <td>-0.017541</td>\n",
       "      <td>0.031196</td>\n",
       "      <td>0.036596</td>\n",
       "      <td>-0.013267</td>\n",
       "      <td>-0.011521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050397</td>\n",
       "      <td>-0.216051</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014050</td>\n",
       "      <td>0.985950</td>\n",
       "      <td>0th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.060994</td>\n",
       "      <td>-0.010418</td>\n",
       "      <td>-0.041536</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.003026</td>\n",
       "      <td>-0.027531</td>\n",
       "      <td>0.046743</td>\n",
       "      <td>0.006817</td>\n",
       "      <td>-0.020207</td>\n",
       "      <td>-0.004573</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037533</td>\n",
       "      <td>-0.238891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034753</td>\n",
       "      <td>0.965247</td>\n",
       "      <td>2th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.063151</td>\n",
       "      <td>-0.013606</td>\n",
       "      <td>-0.026188</td>\n",
       "      <td>-0.015054</td>\n",
       "      <td>0.031623</td>\n",
       "      <td>-0.023023</td>\n",
       "      <td>0.047041</td>\n",
       "      <td>0.071892</td>\n",
       "      <td>-0.024079</td>\n",
       "      <td>-0.023515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003898</td>\n",
       "      <td>-0.198318</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006411</td>\n",
       "      <td>0.993589</td>\n",
       "      <td>0th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.010719</td>\n",
       "      <td>0.022915</td>\n",
       "      <td>-0.000830</td>\n",
       "      <td>0.012984</td>\n",
       "      <td>-0.020518</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>-0.043937</td>\n",
       "      <td>-0.001184</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074513</td>\n",
       "      <td>0.022036</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.931072</td>\n",
       "      <td>0.068928</td>\n",
       "      <td>1th_test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EMB_000_nan  EMB_001_nan  EMB_002_nan  EMB_003_nan  EMB_004_nan  \\\n",
       "0    -0.074153    -0.055988    -0.052818     0.005892     0.015515   \n",
       "1    -0.068774    -0.049618    -0.058634     0.009538     0.005547   \n",
       "2    -0.033640    -0.032654    -0.017567     0.003200     0.020300   \n",
       "3    -0.055558    -0.024381    -0.035403     0.003343     0.007629   \n",
       "4    -0.019305    -0.013052    -0.031120     0.008044    -0.031994   \n",
       "5     0.001473    -0.021499    -0.013175     0.013053    -0.029639   \n",
       "6    -0.045707    -0.011426    -0.021226    -0.004355     0.013403   \n",
       "7    -0.060994    -0.010418    -0.041536     0.001866     0.003026   \n",
       "8    -0.063151    -0.013606    -0.026188    -0.015054     0.031623   \n",
       "9     0.010719     0.022915    -0.000830     0.012984    -0.020518   \n",
       "\n",
       "   EMB_005_nan  EMB_006_nan  EMB_007_nan  EMB_008_nan  EMB_009_nan  ...  \\\n",
       "0    -0.021260     0.026566     0.017620     0.001553    -0.001954  ...   \n",
       "1    -0.021479     0.025170    -0.010637     0.005298     0.009066  ...   \n",
       "2    -0.008067     0.017446     0.049978    -0.011376    -0.024331  ...   \n",
       "3    -0.023969     0.032309     0.009788    -0.018074    -0.006553  ...   \n",
       "4    -0.012280     0.001667    -0.081808     0.015399     0.039381  ...   \n",
       "5    -0.006258    -0.016850    -0.078868     0.014410     0.029548  ...   \n",
       "6    -0.017541     0.031196     0.036596    -0.013267    -0.011521  ...   \n",
       "7    -0.027531     0.046743     0.006817    -0.020207    -0.004573  ...   \n",
       "8    -0.023023     0.047041     0.071892    -0.024079    -0.023515  ...   \n",
       "9     0.000423     0.004345    -0.043937    -0.001184     0.011903  ...   \n",
       "\n",
       "   EMB_126_nan_shapley  EMB_127_nan_shapley  target_encoded_nan  \\\n",
       "0             0.034957             0.122719                   1   \n",
       "1             0.006115            -0.162118                   1   \n",
       "2             0.017734             0.073409                   1   \n",
       "3            -0.000854             0.174334                   1   \n",
       "4            -0.009817            -0.097222                   0   \n",
       "5             0.005287             0.088159                   0   \n",
       "6             0.050397            -0.216051                   1   \n",
       "7            -0.037533            -0.238891                   0   \n",
       "8             0.003898            -0.198318                   1   \n",
       "9             0.074513             0.022036                   0   \n",
       "\n",
       "   pred_target_encoded_nan  pred_target_encoded_nan_best0  \\\n",
       "0                        1                              1   \n",
       "1                        1                              1   \n",
       "2                        1                              1   \n",
       "3                        0                              0   \n",
       "4                        0                              0   \n",
       "5                        0                              0   \n",
       "6                        1                              1   \n",
       "7                        1                              1   \n",
       "8                        1                              1   \n",
       "9                        0                              0   \n",
       "\n",
       "   pred_target_encoded_nan_best1  pred_target_encoded_nan_best2    prob_0  \\\n",
       "0                              1                              1  0.000054   \n",
       "1                              1                              1  0.011164   \n",
       "2                              1                              1  0.000445   \n",
       "3                              0                              0  0.521614   \n",
       "4                              0                              0  0.606196   \n",
       "5                              0                              0  0.717698   \n",
       "6                              1                              1  0.014050   \n",
       "7                              1                              1  0.034753   \n",
       "8                              1                              1  0.006411   \n",
       "9                              0                              0  0.931072   \n",
       "\n",
       "     prob_1  train_test  \n",
       "0  0.999946    1th_test  \n",
       "1  0.988836    0th_test  \n",
       "2  0.999555    2th_test  \n",
       "3  0.478386    1th_test  \n",
       "4  0.393804    3th_test  \n",
       "5  0.282302    3th_test  \n",
       "6  0.985950    0th_test  \n",
       "7  0.965247    2th_test  \n",
       "8  0.993589    0th_test  \n",
       "9  0.068928    1th_test  \n",
       "\n",
       "[10 rows x 264 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACboAAAMWCAYAAAA9daJ8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzde1zUZd7/8TcOxxHEs6i5oqAohphhmorZOh4zy9008UcZZFr6MzHL2qWSkixXQlLbEs08395130bomhnDarTdaqx5ylstSqzEUyUIxmFkfn/4Y3KaAQdSGHdfz7/i87m+1/UZ+/f9uC4Pq9VqFQAAAAAAAAAAAAAAAAAAbqpRQw8AAAAAAAAAAAAAAAAAAEBNCLoBAAAAAAAAAAAAAAAAANwaQTcAAAAAAAAAAAAAAAAAgFsj6AYAAAAAAAAAAAAAAAAAcGsE3QAAAAAAAAAAAAAAAAAAbo2gGwAAAAAAAAAAAAAAAADArRF0AwAAAAAAAAAAAAAAAAC4NYJuAAAAAAAAAAAAAAAAAAC3RtANAAAAAAAAAAAAAAAAAODWCLrhqtLT01VRUdHQYwAAAAAAAAAAAAAAAAD4N0XQDQAAAAAAAAAAAAAAAADg1gi6AQAAAAAAAAAAAAAAAADcGkE3AAAAAAAAAAAAAAAAAIBbI+gGAAAAAAAAAAAAAAAAAHBrBN0AAAAAAAAAAAAAAAAAAG6NoBsAAAAAAAAAAAAAAAAAwK0RdAMAAAAAAAAAAAAAAAAAuDWCbgAAAAAAAAAAAAAAAAAAt0bQDQAAAAAAAAAAAAAAAADg1gi6AQAAAAAAAAAAAAAAAADcGkE3AAAAAAAAAAAAAAAAAIBbI+gGAAAAAAAAAAAAAAAAAHBrBN0AAAAAAAAAAAAAAAAAAG6NoBsAAAAAAAAAAAAAAAAAwK0RdAMAAAAAAAAAAAAAAAAAuDWCbgAAAAAAAAAAAAAAAAAAt0bQDQAAAAAAAAAAAAAAAADg1gi6AQAAAAAAAAAAAAAAAADcGkE3AAAAAAAAAAAAAAAAAIBbI+gGAAAAAAAAAAAAAAAAAHBrBN0AAAAAAAAAAAAAAAAAAG7Nw2q1Wht6CLg3jxRLQ48AAAAAAADwL8X61H0NPQIAAAAAAMC/DmtGQ0+AesCNbgAAAAAAAAAAAAAAAAAAt0bQDQAAAAAAAAAAAAAAAADg1gi6AQAAAAAAAAAAAAAAAADcGkE3AAAAAAAAAAAAAAAAAIBbI+gGAAAAAAAAAAAAAAAAAHBrnrX9YNmyZVq+fHmNa3JzczVlyhTt3btX4eHhWrNmTbVr33//fc2bN0+SlJmZqXbt2kmS7fsrBQQEqF+/fkpISFCbNm1qNXdxcbGWLFmiHTt2qKSkROHh4Zo+fboiIyPt1jk7t8qkSZM0Y8YM29/79u3TW2+9pQMHDqiiokI33XST7r77bk2cOFEGg6FW8wEAAAAAAAAAAAAAAAAAnKt10K1KYmKiWrZsWeMag8Ggw4cPq6CgQG3btnW6xmw2y2Aw6NKlSw69wMBAJSUlSZIsFovy8/O1YcMGTZw4UevXr1dQUJBLs1osFs2YMUNff/21Jk+erFatWmnjxo169NFHtWLFCvXo0cNu/W233aaYmBiHfTp06GD7708//VSzZs1Sq1atNHXqVAUGBiorK0uvvfaajh07ZgvvAQAAAAAAAAAAAAAAAAB+mzoH3aKiouyCX8507txZX331lcxms2JjYx36RUVF2rNnj0JCQnTs2DGHvo+Pj6Kjo+1qvXv3Vnx8vDZu3KiEhASXZt28ebMOHjyohQsX6s4775QkDRo0SOPGjdOrr76qlStX2q1v3bq1w7m/tnjxYvn7+2v16tVq0aKFJOmuu+7Sn/70J33wwQf6P//n/6hbt24uzQcAAAAAAAAAAAAAAAAAqF6j67l5QECAevToIbPZ7LS/c+dOWSwW9e/f3+U9e/bsKT8/P+Xl5bn8zdatWxUUFGQLuUmS0WjUmDFjdODAAX333Xcu7yVdDuh99dVXio6OtoXcqowdO1aSqn3+FAAAAAAAAAAAAAAAAABQO9c16CZJJpNJhw4d0qlTpxx6WVlZCg0NVceOHV3ez2q1qrKyUgEBAS6tt1gsOnTokG655RaH3q233ipJ+vzzz10+/0q+vr4ONaPRWKs9Nm/erKioKH355Zdau3at7rnnHvXv31/333+/Pv74Y7u158+f18qVKxUTE6MBAwZoyJAhtidZr5SUlKQhQ4bowoULevnllzV06FANGDBAjz32mI4fP17r3wkAAAAAAAAAAAAAAAAADalegm6SlJ2dbVcvLi7Wnj17bH1XHTlyRGVlZS7fAnfq1ClVVFTopptucuhV1b799lun35aXlzutN2nSRJ07d9bu3bsd1nzyySeSpF69erk0X5WUlBStXbtWMTExSkxMVOPGjTVnzhydOHHCtuYvf/mL3nnnHd1xxx164YUX9NhjjykvL0/Tpk1zmMNiseixxx7T4cOHNXPmTM2aNUvffPONZs2aJYvFUqvZAAAAAAAAAAAAAAAAAKAhedb1w9LSUl28eNGh7uXlJS8vL9vfQUFBioiIUFZWliZOnGir79ixQxUVFTKZTDp48KDTM6xWq+2MsrIyHT58WKmpqerTp49GjBjh0pyFhYWSpKZNmzr0qmpVa6ps375dH3zwgS5duiRfX1/dcsstmjRpkqKiomxr/vznP+vxxx/XU089penTpyswMFAfffSRVq1apZiYGIWHh7s0X5Uvv/xSGzZsUFBQkCRpwIABGj58uD744ANNnTpVkjRq1Cg999xz8vPzs30XEBCgxMREHThwwG6+kpIS+fj46K9//at8fHwkSS1atNBTTz2lffv22a0FAAAAAAAAAAAAAAAAAHdW56BbTEyM03p8fLymTZtmVxsyZIjS0tJ05swZtW7dWpJkNpsVGhqq4ODgaoNuZ8+e1aBBg+xqXbt21XPPPSdPT9dGr7rpzNvb26FXVSstLbXV7rrrLo0ZM0YtW7ZUeXm5vv76a7333nt67LHH9NJLL2nYsGGSLt/YtnLlSs2YMcMuwJeQkKDY2FiXZrtSXFycLeQmXQ7hdezYUXl5ebbawIEDHb5r3ry5pMv/Vr82e/ZsW8itamZJysvLI+gGAAAAAAAAAAAAAAAA4IZR56Db/PnzbaG1K10Z1qpiMpmUlpam7OxsTZgwQcXFxdq9e7fi4+NrPKNZs2ZauHChJKmiokInT55URkaGxo0bp6SkJA0dOvSqc1YF4i5duuTQq3rC88oQ3D333GO3ZtCgQZowYYIeeeQRLVy4UL///e/l6empffv2afbs2erYsaMef/xx+fv763/+53/0+uuv6/z58/q///f/XnW2K91+++0OtSZNmujChQt2tX/84x/6+9//riNHjujEiRO2G+9+/RxpYGCgw61yTZo0kSSHPQEAAAAAAAAAAAAAAADAndU56Na9e3d16NDBpbVt2rRRRESEzGazJkyYoJycHJWXl8tkMtX4nZeXl+0WsiqjR49WXFyckpOT1adPH6dPkl7J399fklRcXOzQKykpsVtTHV9fX91///1KSkpSXl6egoOD9fTTTys4OFjp6ekyGAySpOjoaHXu3FmvvPKKIiIidMcdd9S475WMRqNDrVGjRnYBvaSkJG3ZskW33nqrfv/73+umm26St7e3nnzySZf2q5rTWegPAAAAAAAAAAAAAAAAANxVo/o6aOjQodq/f7/OnTunrKwshYSEKDg4uNb7GAwGmUwmlZSU6NChQ1dd37ZtW3l4eOjUqVMOvYKCAklSu3btrrpPy5YtJV2+WW7fvn364YcfNHbsWFt4rMo999wjLy8vZWVlufJzXLZz505t2bJFjz76qJYtW6b4+HgNGzZMnTt3vqbnAAAAAAAAAAAAAAAAAIC7qbegm8lkktVq1bZt27Rr1y6Xnh2tjtVqlXQ5dHY1vr6+6tKliw4ePOjQO3DggCQpIiLiqvscPXpUBoNBv/vd73T+/HlJcgi5SZKHh4caNWpkW3Ot7N27V5J033332dWPHTt2Tc8BAAAAAAAAAAAAAAAAAHdTb0G3Vq1aKTIyUmvXrlVZWdlVny2tjsVi0fbt2+Xt7a3IyEiXvhk2bJiOHj2q/fv32+2zadMmderUSWFhYZKkQ4cOOb357fTp01qzZo2io6PVpEkTdenSRZKUmZmpyspKu7V/+9vfVFZWZtvzWvHy8pIkFRYW2molJSVKT0+/pucAAAAAAAAAAAAAAAAAgLvxrOuHubm5On78uNNedHS007rJZFJKSorLz5aWlZUpJydH0uVb3E6fPq3MzEwdPXpUM2fOVPPmzV2adfz48dq8ebNmz56tyZMnq1mzZtq0aZPy8/O1ePFieXh4SLocdFuyZImGDBmi3r17KzAwUMePH9fatWtlNBr11FNPSZI6d+6sMWPGKDMzUw8//LDuvvtuGY1G7d27V++//75atWqlmJgYl2ZzVXR0tNasWaOnn35asbGxunTpktauXStfX99reg4AAAAAAAAAAAAAAAAAuJs6B91eeumlanu5ublO6yaTSampqS7f5lZYWKhZs2bZ/jYajQoLC1NKSooGDx7s8qxGo1Hp6el67bXXtHz5cpWWliosLExLlizRbbfdZlv3xz/+UZWVlfrwww+1c+dOlZaWqk2bNrrrrrsUHx+vZs2a2dY+++yzCg8PV2ZmphYtWqSKigq1adNGf/jDH/Twww+rRYsWLs/nisjISCUnJ+vtt9/Wyy+/rBYtWuiuu+7SoEGD9MADD1zTswAAAAAAAAAAAAAAAADAnXhYrVZrQw/xWxQXF6u6n2AwGGQ0Gut5ol+UlZWpvLy82n7jxo3VqFG9vR5bZx4ploYeAQAAAAAA4F+K9an7GnoEAAAAAACAfx3WjIaeAPWgzje6uYuYmBgVFBQ47fXu3Vvp6en1PNEvVq1apeXLl1fbz8zMVLt27epxIgAAAAAAAAAAAAAAAAC48dzwQbcFCxZUe2uav79/PU9j75577lHfvn2r7bds2bIepwEAAAAAAAAAAAAAAACAG9MN/3Qprr/09HTFxcXJy8uroUcBAAAAAAAAAAAAAAAA8G+oUUMPAAAAAAAAAAAAAAAAAABATQi6AQAAAAAAAAAAAAAAAADcGkE3AAAAAAAAAAAAAAAAAIBbI+gGAAAAAAAAAAAAAAAAAHBrBN0AAAAAAAAAAAAAAAAAAG6NoBsAAAAAAAAAAAAAAAAAwK0RdAMAAAAAAAAAAAAAAAAAuDWCbgAAAAAAAAAAAAAAAAAAt0bQDQAAAAAAAAAAAAAAAADg1jysVqu1oYeAe/NIsTT0CAAAAADwb8X61H0NPQIAAAAA/HuxZjT0BAAAALgKbnQDAAAAAAAAAAAAAAAAALg1gm4AAAAAAAAAAAAAAAAAALdG0A0AAAAAAAAAAAAAAAAA4NYIugEAAAAAAAAAAAAAAAAA3BpBNwAAAAAAAAAAAAAAAACAW/Os7QfLli3T8uXLa1yTm5urKVOmaO/evQoPD9eaNWuqXfv+++9r3rx5kqTMzEy1a9dOkmzfXykgIED9+vVTQkKC2rRpU6u5i4uLtWTJEu3YsUMlJSUKDw/X9OnTFRkZabeutLRUK1eu1Pbt23X69GkZjUb17t1bU6dOVWhoqCRp8+bNeuGFF2o8b+7cubr77rtrNSMAAAAAAAAAAAAAAAAAwFGtg25VEhMT1bJlyxrXGAwGHT58WAUFBWrbtq3TNWazWQaDQZcuXXLoBQYGKikpSZJksViUn5+vDRs2aOLEiVq/fr2CgoJcmtVisWjGjBn6+uuvNXnyZLVq1UobN27Uo48+qhUrVqhHjx6SpEuXLunxxx/X3r17ddddd+n2229XQUGB1q1bp7i4OL311lvq2rWrbrvtNi1atMjpWfn5+UpLS5Ovr69LswEAAAAAAAAAAAAAAAAAalbnoFtUVJQ6dOhQ45rOnTvrq6++ktlsVmxsrEO/qKhIe/bsUUhIiI4dO+bQ9/HxUXR0tF2td+/eio+P18aNG5WQkODSrJs3b9bBgwe1cOFC3XnnnZKkQYMGady4cXr11Ve1cuVKSdLOnTu1d+9excXFafr06bbvBw4cqAceeEBvvPGGFi1apDZt2lR7o9yePXtkNBo1YMAAl2YDAAAAAAAAAAAAAAAAANSs0fXcPCAgQD169JDZbHba37lzpywWi/r37+/ynj179pSfn5/y8vJc/mbr1q0KCgqyhdwkyWg0asyYMTpw4IC+++47SbI9lTp27Fi777t06aKbb75Zn3/+eY3nlJWVaevWrRo2bJiMRqPL8wEAAAAAAAAAAAAAAAAAqnddg26SZDKZdOjQIZ06dcqhl5WVpdDQUHXs2NHl/axWqyorKxUQEODSeovFokOHDumWW25x6N16662S5BBgc/bsqCvBNbPZrMLCQt17770uzSZJubm5ioqKUk5OjjIzMzV+/Hj1799fY8eOVUZGht3an3/+We+8844mTZqk6OhoDR48WJMnT9b+/fvt1i1btkxRUVE6f/68Xn/9dY0aNUoDBgzQpEmTdODAAZdnAwAAAAAAAAAAAAAAAAB3UC9BN0nKzs62qxcXF2vPnj22vquOHDmisrIyl2+BO3XqlCoqKnTTTTc59Kpq3377rSSpV69ekqSPP/7Ybt358+d18OBBp2G5K7333nu2299qa926dUpJSdGoUaP0/PPPq2PHjkpOTlZubq5tzVtvvaU33nhDvXr10vPPP68nnnhChYWFmj59us6dO+ew59NPP62srCw98sgjmjNnji5evKiEhARduHCh1vMBAAAAAAAAAAAAAAAAQEPxrOuHpaWlunjxokPdy8tLXl5etr+DgoIUERGhrKwsTZw40VbfsWOHKioqZDKZdPDgQadnWK1W2xllZWU6fPiwUlNT1adPH40YMcKlOQsLCyVJTZs2dehV1arWDBkyRHfeeacWLVokDw8PDRgwQCdPnlRaWpq8vb01c+bMas85fvy4Pv/8c82ZM8eluX5t//79WrVqlbp16ybpckBw+PDh2rJli6KioiRJ/fr1U2xsrN1v6dSpk+Li4vTpp59qzJgxdnueOXNGq1evVpMmTSRJYWFhio2N1Y4dO3T33XfXaU4AAAAAAAAAAAAAAAAAqG91DrrFxMQ4rcfHx2vatGl2tSFDhigtLU1nzpxR69atJV1+5jM0NFTBwcHVBt3Onj2rQYMG2dW6du2q5557Tp6ero1eXl4uSfL29nboVdVKS0slSR4eHnrllVeUlpamefPm2dZ17txZq1evVlBQULXnbNq0ST4+Pho5cqRLc/3a2LFjbSE3SfL09NTNN9+svLw8W60q8HalZs2aSbr8b/Vr06dPt4XcJKlbt27y9fW12xMAAAAAAAAAAAAAAAAA3F2dg27z58+3hdau5CwMZjKZlJaWpuzsbE2YMEHFxcXavXu34uPjazyjWbNmWrhwoSSpoqJCJ0+eVEZGhsaNG6ekpCQNHTr0qnNWBeIuXbrk0LNYLJJ+CbxZLBa98MIL+vvf/66HH35YPXv21NmzZ7V+/XpNnTpVixcvVseOHR32KS8v19/+9jeZTCYFBARcdSZnbr/9dodakyZNdPz4cbvavn379NFHH+mLL75Qfn6+7RnSqt9ytT0DAwNVVFRUpxkBAAAAAAAAAAAAAAAAoCHUOejWvXt3dejQwaW1bdq0UUREhMxmsyZMmKCcnByVl5fLZDLV+J2Xl5d69eplVxs9erTi4uKUnJysPn36OH2S9Er+/v6SpOLiYodeSUmJ3Zr//M//1AcffKDFixerf//+tnXDhg1TTEyMEhMTtW7dOod9zGazCgsLNXbs2BpnqYnRaHSoGQwGVVZW2v5etmyZli9fru7du2vgwIG6//771b59+2oDg40bN3aoNWrUyG5PAAAAAAAAAAAAAAAAAHB3jerroKFDh2r//v06d+6csrKyFBISouDg4FrvYzAYZDKZVFJSokOHDl11fdu2beXh4aFTp0459AoKCiRJ7dq1kyRlZ2erffv2diE36XIIbdSoUTpy5Ii+++47h33ee+89derUySGUdy0dO3ZMK1as0L333qs1a9Zo6tSpGjlypMLDw6/bmQAAAAAAAAAAAAAAAADgDuot6GYymWS1WrVt2zbt2rXLpWdHq2O1WiVdfs70anx9fdWlSxcdPHjQoXfgwAFJUkREhCTp/PnzatTI+T+JwWCQJP3000929ePHj2vv3r269957XZ6/Lvbu3Sur1ao//OEP8vDwsNWPHj16Xc8FAAAAAAAAAAAAAAAAgIZWb0G3Vq1aKTIyUmvXrlVZWdlVny2tjsVi0fbt2+Xt7a3IyEiXvhk2bJiOHj2q/fv32+2zadMmderUSWFhYZKkLl266LvvvlNubq7d96Wlpfrggw/k7e2tTp062fXee+89eXt766677qrT73GVl5eXJKmoqMjuNyxduvS6ngsAAAAAAAAAAAAAAAAADc2zrh/m5ubq+PHjTnvR0dFO6yaTSSkpKS4/W1pWVqacnBxJl29xO336tDIzM3X06FHNnDlTzZs3d2nW8ePHa/PmzZo9e7YmT56sZs2aadOmTcrPz9fixYttN6Q98sgj+p//+R8lJCRo/Pjx6t69u3744Qf913/9l/Lz8/Xoo4/K39/ftm95ebn+9re/afDgwWratKlLs9RV37595ePjo+TkZMXFxcnPz0/vvvuuS7faAQAAAAAAAAAAAAAAAMCNrM5Bt5deeqna3q9vRKtiMpmUmprq8m1uhYWFmjVrlu1vo9GosLAwpaSkaPDgwS7PajQalZ6ertdee03Lly9XaWmpwsLCtGTJEt122222dSEhIVqzZo3eeustbdu2TevXr5evr6/CwsI0ZcoUDRs2zG7f7OxsnT9/XmPHjnV5lrq66aablJqaqjfeeENpaWlq3LixTCaTJk2apJEjR1738wEAAAAAAAAAAAAAAACgoXhYrVZrQw/xWxQXF6u6n2AwGGQ0Gut5ol9UVFSotLS02r6fn588PeucNaw3HimWhh4BAAAAAP6tWJ+6r6FHAAAAAIB/L9aMhp4AAAAAV+H+KauriImJUUFBgdNe7969lZ6eXs8T/WLbtm164YUXqu2/+eabioqKqseJAAAAAAAAAAAAAAAAAODGc8Pf6Hb48GGVl5c77fn7+ys0NLSeJ/rFjz/+qBMnTlTbDw0Nlb+/fz1OVDfc6AYAAAAA9Ysb3QAAAACgnnGjGwAAgNu74YNuuP7S09MVFxcnLy+vhh4FAAAAAAAAAAAAAAAAwL+hRg09AAAAAAAAAAAAAAAAAAAANSHoBgAAAAAAAAAAAAAAAABwawTdAAAAAAAAAAAAAAAAAABujaAbAAAAAAAAAAAAAAAAAMCtEXQDAAAAAAAAAAAAAAAAALg1gm4AAAAAAAAAAAAAAAAAALdG0A0AAAAAAAAAAAAAAAAA4NYIugEAAAAAAAAAAAAAAAAA3BpBNwAAAAAAAAAAAAAAAACAWyPoBgAAAAAAAAAAAAAAAABwax5Wq9Xa0EPAvXmkWBp6BAAAALgB61P3NfQIAAAAcBfWjIaeAAAAAAAA/JvhRjcAAAAAAAAAAAAAAAAAgFsj6AYAAAAAAAAAAAAAAAAAcGsE3QAAAAAAAAAAAAAAAAAAbo2gGwAAAAAAAAAAAAAAAADArXnW9oNly5Zp+fLlNa7Jzc3VlClTtHfvXoWHh2vNmjXVrn3//fc1b948SVJmZqbatWsnSbbvrxQQEKB+/fopISFBbdq0qdXcxcXFWrJkiXbs2KGSkhKFh4dr+vTpioyMtFtXXl6uNWvW6IMPPlBBQYGaNm2q7t27a9q0aQoJCbFb+9133+mvf/2rdu/erbKyMoWGhuqRRx7RgAEDajUbAAAAAAAAAAAAAAAAAKB6tQ66VUlMTFTLli1rXGMwGHT48GEVFBSobdu2TteYzWYZDAZdunTJoRcYGKikpCRJksViUX5+vjZs2KCJEydq/fr1CgoKcmlWi8WiGTNm6Ouvv9bkyZPVqlUrbdy4UY8++qhWrFihHj16SJIqKioUExOjn376Sffcc4/Cw8P1ww8/aN26dZo0aZLWrVun4OBgSZdDbpMmTZKHh4cmT56sli1bauvWrZo1a5b+8pe/aPDgwS7NBgAAAAAAAAAAAAAAAACoWZ2DblFRUerQoUONazp37qyvvvpKZrNZsbGxDv2ioiLt2bNHISEhOnbsmEPfx8dH0dHRdrXevXsrPj5eGzduVEJCgkuzbt68WQcPHtTChQt15513SpIGDRqkcePG6dVXX9XKlSslXQ7E/e53v9Nbb72lpk2b2r6/9dZbFRMTo02bNumJJ56QJC1ZskTFxcX6j//4D3Xu3FmSZDKZ9Mwzz2jhwoWKjo6WwWBwaT4AAAAAAAAAAAAAAAAAQPUaXc/NAwIC1KNHD5nNZqf9nTt3ymKxqH///i7v2bNnT/n5+SkvL8/lb7Zu3aqgoCBbyE2SjEajxowZowMHDui7776TJPn5+WnRokV2ITdJ6tKli/z9/XX8+HFJlwNxn3zyifr3728LuUmSh4eHHnroIZ0+fVr79u1zeT4AAAAAAAAAAAAAAAAAQPWua9BNunzL2aFDh3Tq1CmHXlZWlkJDQ9WxY0eX97NaraqsrFRAQIBL6y0Wiw4dOqRbbrnFoXfrrbdKkj7//POr7lFWVqbGjRtLkn766SeVlZU5nbvqadMjR464NN/mzZsVFRWlL7/8UmvXrtU999yj/v376/7779fHH39st/b8+fNauXKlYmJiNGDAAA0ZMsT2JOuVkpKSNGTIEF24cEEvv/yyhg4dqgEDBuixxx6zhfUAAAAAAAAAAAAAAAAA4EZRL0E3ScrOzrarFxcXa8+ePba+q44cOaKysjKXb4E7deqUKioqdNNNNzn0qmrffvttjXtkZWWpoqLCFozz8fGRdPk3/FpBQYGky2G42khJSdHatWsVExOjxMRENW7cWHPmzNGJEydsa/7yl7/onXfe0R133KEXXnhBjz32mPLy8jRt2jSVl5fb7WexWPTYY4/p8OHDmjlzpmbNmqVvvvlGs2bNksViqdVsAAAAAAAAAAAAAAAAANCQPOv6YWlpqS5evOhQ9/LykpeXl+3voKAgRUREKCsrSxMnTrTVd+zYoYqKCplMJh08eNDpGVar1XZGWVmZDh8+rNTUVPXp00cjRoxwac7CwkJJcniO9Mpa1RpnSkpKtGTJErVu3VqjR4+WJDVp0kTt27fXrl27dOnSJRkMBtv6zZs3X3VPZ7788ktt2LBBQUFBkqQBAwZo+PDh+uCDDzR16lRJ0qhRo/Tcc8/Jz8/P9l1AQIASExN14MABRUVF2c3t4+Ojv/71r7ZgXosWLfTUU09p3759dmsBAAAAAAAAAAAAAAAAwJ3VOegWExPjtB4fH69p06bZ1YYMGaK0tDSdOXNGrVu3liSZzWaFhoYqODi42qDb2bNnNWjQILta165d9dxzz8nT07XRq2468/b2duhV1UpLS6v9fsGCBTp9+rRee+01+fr62uoxMTFKSUnRn//8Z82YMUN+fn7avHmzPv74YwUGBtqF31wRFxdnC7lJl0N4HTt2VF5enq02cOBAh++aN28u6fK/1a/Nnj3bFnKTpF69ekmS8vLyCLoBAAAAAAAAAAAAAAAAuGHUOeg2f/58W2jtSleGtaqYTCalpaUpOztbEyZMUHFxsXbv3q34+Pgaz2jWrJkWLlwoSaqoqNDJkyeVkZGhcePGKSkpSUOHDr3qnFWBuEuXLjn0qp7wdBaCk6StW7dq69ateuCBBzRgwAC73v3336/CwkKtXr1aZrNZktStWzelpaVp4sSJCgwMvOpsV7r99tsdak2aNNGFCxfsav/4xz/097//XUeOHNGJEydsN979+jnSwMBAhYeHO+wnyWFPAAAAAAAAAAAAAAAAAHBndQ66de/eXR06dHBpbZs2bRQRESGz2awJEyYoJydH5eXlMplMNX7n5eVlu4WsyujRoxUXF6fk5GT16dPH6ZOkV/L395ckFRcXO/RKSkrs1lzpf//3f/XSSy/p1ltv1fTp0x36Hh4emjp1qh544AHl5+fL399fHTp00OnTp/Xzzz+rY8eONc71a0aj0aHWqFEju4BeUlKStmzZoltvvVW///3vddNNN8nb21tPPvmkS/tV3TLnLPQHAAAAAAAAAAAAAAAAAO6qUX0dNHToUO3fv1/nzp1TVlaWQkJCFBwcXOt9DAaDTCaTSkpKdOjQoauub9u2rTw8PHTq1CmHXkFBgSSpXbt2dvUff/xRTz75pFq0aKEFCxbU+Eyq0Wi0C/3t2bNHknTLLbe4/JtcsXPnTm3ZskWPPvqoli1bpvj4eA0bNkydO3e+pucAAAAAAAAAAAAAAAAAgLupt6CbyWSS1WrVtm3btGvXLpeeHa2O1WqVdPk506vx9fVVly5ddPDgQYfegQMHJEkRERG2msVi0Zw5c1RUVKRXX331qjfG/dq7776rnj17qm3btrX67mr27t0rSbrvvvvs6seOHbum5wAAAAAAAAAAAAAAAACAu6m3oFurVq0UGRmptWvXqqys7KrPllbHYrFo+/bt8vb2VmRkpEvfDBs2TEePHtX+/fvt9tm0aZM6deqksLAwW33BggXav3+/5s6dqy5dutRqtjVr1uh///d/NWXKlFp95wovLy9JUmFhoa1WUlKi9PT0a34WAAAAAAAAAAAAAAAAALiT6t/kvIrc3FwdP37caS86Otpp3WQyKSUlxeVnS8vKypSTkyPp8i1up0+fVmZmpo4ePaqZM2eqefPmLs06fvx4bd68WbNnz9bkyZPVrFkzbdq0Sfn5+Vq8eLE8PDwkSdu2bdN7772nPn36yMfHx3Z2FV9fX/Xp00eStGHDBn399dfq1auXPD09tXPnTn300UeaOnWq+vXr59JctREdHa01a9bo6aefVmxsrC5duqS1a9fK19f3mp8FAAAAAAAAAAAAAAAAAO6kzkG3l156qdpebm6u07rJZFJqaqrLt7kVFhZq1qxZtr+NRqPCwsKUkpKiwYMHuzyr0WhUenq6XnvtNS1fvlylpaUKCwvTkiVLdNttt9nW5efnS5I+++wzffbZZw77tG3bVps3b5YkdenSRdu2bdNHH30kSQoJCdHChQt15513ujxXbURGRio5OVlvv/22Xn75ZbVo0UJ33XWXBg0apAceeOC6nAkAAAAAAAAAAAAAAAAA7sDDarVaG3qI36K4uFjV/QSDwSCj0VjPE/2irKxM5eXl1fYbN26sRo3q7fXYOvNIsTT0CAAAAHAD1qfua+gRAAAA4C6sGQ09AQAAAAAA+DdT5xvd3EVMTIwKCgqc9nr37q309PR6nugXq1at0vLly6vtZ2Zmql27dvU4EQAAAAAAAAAAAAAAAADceG74oNuCBQuqvTXN39+/nqexd88996hv377V9lu2bFmP0wAAAAAAAAAAAAAAAADAjemGD7qFh4c39AjVCgoKUlBQUEOPAQAAAAAAAAAAAAAAAAA3NA+r1Wpt6CHg3tLT0xUXFycvL6+GHgUAAAAAAAAAAAAAAADAv6FGDT0AAAAAAAAAAAAAAAAAAAA1IegGAAAAAAAAAAAAAAAAAHBrBN0AAAAAAAAAAAAAAAAAAG6NoBsAAAAAAAAAAAAAAAAAwK0RdAMAAAAAAAAAAAAAAAAAuDWCbgAAAAAAAAAAAAAAAAAAt0bQDQAAAAAAAAAAAAAAAADg1gi6AQAAAAAAAAAAAAAAAADcGkE3AAAAAAAAAAAAAAAAAIBb87BardaGHgLuzSPF0tAjAADwL8n61H0NPQIAAP+arBkNPQEAAAAAAAAA4BrjRjcAAAAAAAAAAAAAAAAAgFsj6AYAAAAAAAAAAAAAAAAAcGsE3QAAAAAAAAAAAAAAAAAAbo2gGwAAAAAAAAAAAAAAAADArRF0AwAAAAAAAAAAAAAAAAC4Nc/afrBs2TItX768xjW5ubmaMmWK9u7dq/DwcK1Zs6bate+//77mzZsnScrMzFS7du0kyfb9lQICAtSvXz8lJCSoTZs2tR1dJSUlmjFjhg4cOKA333xTUVFR1a7dvn27nnvuOUVGRio9Pd3pmo8//lhr1qzR0aNHVVlZqU6dOmn8+PEaM2ZMrWcDAAAAAAAAAAAAAAAAADhX66BblcTERLVs2bLGNQaDQYcPH1ZBQYHatm3rdI3ZbJbBYNClS5cceoGBgUpKSpIkWSwW5efna8OGDZo4caLWr1+voKAgl+c9cuSIXnzxReXl5dW4rqKiQqtWrdKKFStqXJeRkaHk5GSFhIRo5syZ8vLy0ubNm/Xiiy/q+++/12OPPebybAAAAAAAAAAAAAAAAACA6tU56BYVFaUOHTrUuKZz58766quvZDabFRsb69AvKirSnj17FBISomPHjjn0fXx8FB0dbVfr3bu34uPjtXHjRiUkJLg877Rp0xQeHq4HH3xQzz77bLXr/vu//1vr16/Xn/70J23dutXpGovFoiVLlqhDhw5avXq1fH19JUl33323Jk+erNWrV2v8+PFq0aKFy/MBAAAAAAAAAAAAAAAAAJxrdD03DwgIUI8ePWQ2m532d+7cKYvFov79+7u8Z8+ePeXn53fVm9l+bcmSJVq6dOlVb6Hr37+/Nm/erHvvvbfaNV999ZUKCws1YsQIW8hNkho1aqQxY8bIYrHo4MGDtZoPAAAAAAAAAAAAAAAAAODcdQ26SZLJZNKhQ4d06tQph15WVpZCQ0PVsWNHl/ezWq2qrKxUQEBArebo0aOHS+t+97vfubz3lSG3Ko0bN67VXMuWLVNUVJTOnz+v119/XaNGjdKAAQM0adIkHThwwG7t6dOntWTJEv3xj39U//79NXz4cD3zzDMO/7ZTpkzRAw88oDNnzigxMVGDBw/WHXfcoSeffFLnzp2r1XwAAAAAAAAAAAAAAAAA0NDqJegmSdnZ2Xb14uJi7dmzx9Z31ZEjR1RWVlarW+CutU6dOqlp06b65JNPZLVa7Xo5OTny9vZ2OVhX5emnn1ZWVpYeeeQRzZkzRxcvXlRCQoIuXLhgW/PMM88oOztbo0eP1rx58/TAAw9o9+7devLJJx32KyoqUnx8vH766Sc988wzeuSRR/TZZ58pMTGxbj8aAAAAAAAAAAAAAAAAABqIZ10/LC0t1cWLFx3qXl5e8vLysv0dFBSkiIgIZWVlaeLEibb6jh07VFFRIZPJVO0zn1ar1XZGWVmZDh8+rNTUVPXp00cjRoyo6+i/mY+Pj5555hklJibqxRdf1KRJk+Tl5aWMjAx98MEHeuKJJ9SqVata7XnmzBmtXr1aTZo0kSSFhYUpNjZWO3bs0N133y1JmjRpkgYOHChPz1/+t1ksFi1dulTff/+92rdvb6t///33MplMmj9/vho1upxn9PDw0KJFi3Ty5Em1a9fut/4zAAAAAAAAAAAAAAAAAEC9qHPQLSYmxmk9Pj5e06ZNs6sNGTJEaWlpOnPmjFq3bi1JMpvNCg0NVXBwcLVBt7Nnz2rQoEF2ta5du+q5556zC3s1BJPJpKZNm2r27NnavHmzJMnT01PJyckaPnx4rfebPn26LeQmSd26dZOvr6/y8vJstcGDB9t9Y7Va1bx5c0mX/62uDLp5enrqySeftIXcJKlXr16SpLy8PIJuAAAAAAAAAAAAAAAAAG4YdU6LzZ8/3xZau1JQUJBDzWQyKS0tTdnZ2ZowYYKKi4u1e/duxcfH13hGs2bNtHDhQklSRUWFTp48qYyMDI0bN05JSUkaOnRoXcf/zbKzszV37lxFRUVp1KhR8vLyktlsVlJSkgoLCzV+/Pha7Xf77bc71AIDA1VUVGT7u7KyUmazWR9//LG+/PJLffvttyorK5N0+Wa3K4WGhqply5YO+0my2xMAAAAAAAAAAAAAAAAA3F2dg27du3dXhw4dXFrbpk0bRUREyGw2a8KECcrJyVF5eblMJlON33l5edluIasyevRoxcXFKTk5WX369FHTpk3r+Avq7ty5c3r22Wc1ePBgzZ8/31YfPHiwWrVqpYULF+qWW25Rly5dXN6zcePGDrVGjRqpsrJS0uUg2+OPP67PPvtMAwcO1KhRo9S+fXudP39eL7/8ssO3RqPR6X6SbHsCAAAAAAAAAAAAAAAAwI2g0dWXXBtDhw7V/v37de7cOWVlZSkkJETBwcG13sdgMMhkMqmkpESHDh269oO64JNPPlF5ebnuu+8+h959990nq9WqrKysa3rmf/3Xf2nPnj164YUXtGjRIj344IMaMmSI3XOlAAAAAAAAAAAAAAAAAPCvqN6CbiaTSVarVdu2bdOuXbt+07OjVqtV0uXnTBvC+fPnJf1yQ9qVqmo//fTTNT3zn//8p4xGo0aNGmVXP3bs2DU9BwAAAAAAAAAAAAAAAADcTb0F3Vq1aqXIyEitXbtWZWVlV322tDoWi0Xbt2+Xt7e3IiMjr/GUrgkNDZUkZWRkOPSqamFhYdf0TC8vL1VUVKikpMRWO3funNatW3dNzwEAAAAAAAAAAAAAAAAAd+NZ1w9zc3N1/Phxp73o6GindZPJpJSUFJefLS0rK1NOTo6ky7e4nT59WpmZmTp69Khmzpyp5s2b13X832TAgAG67bbbtGXLFv34448ymUzy9PTUP/7xD23fvl0hISG66667rumZd9xxh7Zv366EhAT98Y9/1IULF7Ry5Uq1bt1aP/744zU9CwAAAAAAAAAAAAAAAADcSZ2Dbi+99FK1vdzcXKd1k8mk1NRUl29zKyws1KxZs2x/G41GhYWFKSUlRYMHD67VvNeSh4eH0tLStH79en344YdasGCBrFar2rZtq0mTJumhhx6Sr6/vNT1z+PDh+umnn/TOO+9o3rx5CgoK0oMPPqh27dpp9uzZ1/QsAAAAAAAAAAAAAAAAAHAnHlar1drQQ/wWxcXFqu4nGAwGGY3Gep7oF6WlpaqoqKi2HxAQUI/T1J1HiqWhRwAA4F+S9an7GnoEAAD+NVkzGnoCAAAAAAAAAMA1Vucb3dxFTEyMCgoKnPZ69+6t9PT0ep7oF6+88oq2bNlSbb+6m+8AAAAAAAAAAAAAAAAAAL+44YNuCxYsUHl5udOev79/PU9jLz4+Xvfee2+DzgAAAAAAAAAAAAAAAAAAN7ob/ulSXH/p6emKi4uTl5dXQ48CAAAAAAAAAAAAAAAA4N9Qo4YeAAAAAAAAAAAAAAAAAACAmhB0AwAAAAAAAAAAAAAAAAC4NYJuAAAAAAAAAAAAAAAAAAC3RtANAAAAAAAAAAAAAAAAAODWCLoBAAAAAAAAAAAAAAAAANwaQTcAAAAAAAAAAAAAAAAAgFsj6AYAAAAAAAAAAAAAAAAAcGsE3QAAAAAAAAAAAAAAAAAAbo2gGwAAAAAAAAAAAAAAAADArXlYrVZrQw8B9+aRYmnoEQCgXlmfuq+hRwCA+mfNaOgJAAAAAAAAAAAAgGpxoxsAAAAAAAAAAAAAAAAAwK0RdAMAAAAAAAAAAAAAAAAAuDWCbgAAAAAAAAAAAAAAAAAAt0bQDQAAAAAAAAAAAAAAAADg1gi6AQAAAAAAAAAAAAAAAADcmmdtP1i2bJmWL19e45rc3FxNmTJFe/fuVXh4uNasWVPt2vfff1/z5s2TJGVmZqpdu3aSZPv+SgEBAerXr58SEhLUpk2bWs1dXFysJUuWaMeOHSopKVF4eLimT5+uyMjIGr/LyMhQcnKyevfurfT0dLve3//+d7333nv64osvdPHiRbVv31533XWXHnjgAXl61vqfFgAAAAAAAAAAAAAAAADgRJ3TWImJiWrZsmWNawwGgw4fPqyCggK1bdvW6Rqz2SyDwaBLly459AIDA5WUlCRJslgsys/P14YNGzRx4kStX79eQUFBLs1qsVg0Y8YMff3115o8ebJatWqljRs36tFHH9WKFSvUo0cPp9/99NNPWrJkiQIDAx16W7Zs0QsvvKA77rhDM2bMkJ+fnz755BO9/vrrysvLU3JyskuzAQAAAAAAAAAAAAAAAABqVuegW1RUlDp06FDjms6dO+urr76S2WxWbGysQ7+oqEh79uxRSEiIjh075tD38fFRdHS0Xa13796Kj4/Xxo0blZCQ4NKsmzdv1sGDB7Vw4ULdeeedkqRBgwZp3LhxevXVV7Vy5Uqn36WlpSkiIkIXL1506Pn7+2v58uXq1auXrTZ8+HAZDAZt2bJFDz/8sDp16uTSfAAAAAAAAAAAAAAAAACA6jW6npsHBASoR48eMpvNTvs7d+6UxWJR//79Xd6zZ8+e8vPzU15ensvfbN26VUFBQbaQmyQZjUaNGTNGBw4c0HfffefwTW5urrKysvTkk0863XPw4MF2IbcqVcG8o0ePujwfAAAAAAAAAAAAAAAAAKB61zXoJkkmk0mHDh3SqVOnHHpZWVkKDQ1Vx44dXd7ParWqsrJSAQEBLq23WCw6dOiQbrnlFoferbfeKkn6/PPP7eoVFRV6+eWX9eCDD+qmm25yeTbp8i10kuTn5+fS+pMnTyoqKkr/+Z//qY8//lgPPvigBgwYoLvuuktvvfWWw1x/+9vfNGXKFN1xxx2Kjo7WAw88oI8//thu3ebNmxUVFaUvv/xSa9eu1T333KP+/fvr/vvvd1gLAAAAAAAAAAAAAAAAAO6uXoJukpSdnW1XLy4u1p49e2x9Vx05ckRlZWUu3wJ36tQpVVRUOA2sVdW+/fZbu/qqVatksVj00EMP1Wo2Sfr444/l7e2tm2++uVbfZWdna86cOerbt6/mzp2rqKgovfHGG9qyZYttTWZmpubPn6+OHTvqT3/6k/785z/Ly8tLTz75pNMb5FJSUrR27VrFxMQoMTFRjRs31pw5c3TixIla/y4AAAAAAAAAAAAAAAAAaCiedf2wtLRUFy9edKh7eXnJy8vL9ndQUJAiIiKUlZWliRMn2uo7duxQRUWFTCaTDh486PQMq9VqO6OsrEyHDx9Wamqq+vTpoxEjRrg0Z2FhoSSpadOmDr2qWtUaSTpx4oTefvttLViwwHY729VcvHhRp0+fVmZmpjZt2qTp06erRYsWLn1b5Z///KdSU1M1aNAgSdKwYcP05ZdfKjMzU6NHj5YkdevWTe+++67atWtn+y4qKkqjRo3Sjh07FBYWZrfnl19+qQ0bNigoKEiSNGDAAA0fPlwffPCBpk6dWqv5AAAAAAAAAAAAAAAAAKCh1DnoFhMT47QeHx+vadOm2dWGDBmitLQ0nTlzRq1bt5Ykmc1mhYaGKjg4uNqg29mzZ23Brypdu3bVc889J09P10YvLy+XJHl7ezv0qmqlpaW22iuvvKK+ffsqOjrapf0l6cknn9SePXvUqFEjPfroo4qLi3P52yoDBw50+K2RkZHavn277e8ePXo4fOfj4yOj0aizZ8869OLi4mwhN+lysK9jx47Ky8ur9XwAAAAAAAAAAAAAAAAA0FDqHHSbP3++LbR2pSuDVVVMJpPS0tKUnZ2tCRMmqLi4WLt371Z8fHyNZzRr1kwLFy6UJFVUVOjkyZPKyMjQuHHjlJSUpKFDh151zqpA3KVLlxx6FotF0i+Btw8++ED79+/Xu+++e9V9r/TEE08oPz9f+/bt09tvv60vvvhCCxYscBquq06/fv0cak2aNNGFCxfsaseOHdO2bdt04MAB5efn66effrL7LVe6/fbbXdoTAAAAAAAAAAAAAAAAANxZnYNu3bt3V4cOHVxa26ZNG0VERMhsNmvChAnKyclReXm5TCZTjd95eXmpV69edrXRo0crLi5OycnJ6tOnj9MnSa/k7+8vSSouLnbolZSU2NZUVFQoLS1N48aNk9Fo1Pnz523rqkJk58+fl8FgUEBAgN0+oaGhCg0N1ZAhQxQZGalnnnlG77zzjmJjY2uc7UqNGzd2qDVq1EiVlZW2v9977z298sor6tChg+68807dc889at++vRITE53uaTQane7pLPQHAAAAAAAAAAAAAAAAAO6qzkG32ho6dKgWLVqkc+fOKSsrSyEhIQoODq71PgaDQSaTSYsXL9ahQ4c0cODAGte3bdtWHh4eOnXqlEOvoKBAktSuXTuVlpbqhx9+0Lp167Ru3Tqne5lMJnXt2lUbNmyo9rzf//738vHx0eeff16roNvV/PDDD0pJSVHfvn2Vmprq8tOtAAAAAAAAAAAAAAAAAHCjq7e0lMlkUmpqqrZt26Zdu3YpLi6uzntZrVZJl58zvRpfX1916dJFBw8edOgdOHBAkhQRESE/Pz+9/vrrTvdIS0uTJCUkJDi9ee3XDAaDS7PVxhdffKGysjKNGTPGLuT2448/6uzZs9f0LAAAAAAAAAAAAAAAAABwJ/UWdGvVqpUiIyO1du1alZWVXfXZ0upYLBZt375d3t7eioyMdOmbYcOGaenSpdq/f7/tG4vFok2bNqlTp04KCwuTh4eH+vbt6/T7qqdKr+ynpaVp4sSJat26td3aDz/8UBcvXlTv3r3r8vOq5eXlJUkqKiqy1axWq1577bVreg4AAAAAAAAAAAAAAAAAuJs6B91yc3N1/Phxp73o6GindZPJpJSUFJefLS0rK1NOTo6ky6Gu06dPKzMzU0ePHtXMmTPVvHlzl2YdP368Nm/erNmzZ2vy5Mlq1qyZNm3apPz8fC1evFgeHh4u7XOlEydO6A9/+INGjhypyMhI+fj4aN++ffrv//5vhYaGavz48bXesyYRERFq0aKFXn/9df38889q1aqVtmzZou+++05Go/GangUAAAAAAAAAAAAAAAAA7qTOQbeXXnqp2l5ubq7TetXzpa7e5lZYWKhZs2bZ/jYajQoLC1NKSooGDx7s8qxGo1Hp6el67bXXtHz5cpWWliosLExLlizRbbfd5vI+V1qwYIHef/99bd++XVlZWfr555/Vpk0bxcTE6OGHH77m4TN/f3+lpaVp8eLFSk9Pl6enp6Kjo/X888/r/vvvv6ZnAQAAAAAAAAAAAAAAAIA78bBardaGHuK3KC4uVnU/wWAwNOhtZxaLRT///HO1fV9fX9uTpO7MI8XS0CMAQL2yPnVfQ48AAPXPmtHQEwAAAAAAAAAAAADVqvONbu4iJiZGBQUFTnu9e/dWenp6PU/0i3379unRRx+ttj937lzdfffd9TgRAAAAAAAAAAAAAAAAANx4bvig24IFC1ReXu605+/vX8/T2OvWrZtWrFhRbf93v/tdPU4DAAAAAAAAAAAAAAAAADemG/7pUlx/6enpiouLuyGeWQUAAAAAAAAAAAAAAADwr6dRQw8AAAAAAAAAAAAAAAAAAEBNCLoBAAAAAAAAAAAAAAAAANwaQTcAAAAAAAAAAAAAAAAAgFsj6AYAAAAAAAAAAAAAAAAAcGsE3QAAAAAAAAAAAAAAAAAAbo2gGwAAAAAAAAAAAAAAAADArRF0AwAAAAAAAAAAAAAAAAC4NYJuAAAAAAAAAAAAAAAAAAC3RtANAAAAAAAAAAAAAAAAAODWPKxWq7Whh4B780ixNPQIAOCU9an7GnoEAHDOmtHQEwAAAAAAAAAAAAD/UrjRDQAAAAAAAAAAAAAAAADg1gi6AQAAAAAAAAAAAAAAAADcGkE3AAAAAAAAAAAAAAAAAIBbI+gGAAAAAAAAAAAAAAAAAHBrBN0AAAAAAAAAAAAAAAAAAG7Ns7YfLFu2TMuXL69xTW5urqZMmaK9e/cqPDxca9asqXbt+++/r3nz5kmSMjMz1a5dO0myfX+lgIAA9evXTwkJCWrTpk2t5i4uLtaSJUu0Y8cOlZSUKDw8XNOnT1dkZGSN32VkZCg5OVm9e/dWenq6Q99qtWrTpk367//+b504cUI+Pj7q0qWLZs2apbCwsFrNCAAAAAAAAAAAAAAAAABwVOugW5XExES1bNmyxjUGg0GHDx9WQUGB2rZt63SN2WyWwWDQpUuXHHqBgYFKSkqSJFksFuXn52vDhg2aOHGi1q9fr6CgIJdmtVgsmjFjhr7++mtNnjxZrVq10saNG/Xoo49qxYoV6tGjh9PvfvrpJy1ZskSBgYFO+1arVX/+85/197//XWPHjtVDDz2koqIi/eMf/9A333xD0A0AAAAAAAAAAAAAAAAAroE6B92ioqLUoUOHGtd07txZX331lcxms2JjYx36RUVF2rNnj0JCQnTs2DGHvo+Pj6Kjo+1qvXv3Vnx8vDZu3KiEhASXZt28ebMOHjyohQsX6s4775QkDRo0SOPGjdOrr76qlStXOv0uLS1NERERunjxotP++vXrtWPHDi1dulRRUVG2+n333Ser1erSbAAAAAAAAAAAAAAAAACAmjW6npsHBASoR48eMpvNTvs7d+6UxWJR//79Xd6zZ8+e8vPzU15ensvfbN26VUFBQbaQmyQZjUaNGTNGBw4c0HfffefwTW5urrKysvTkk0863fPixYt6++239dBDD9mF3Kp4eHi4PB8AAAAAAAAAAAAAAAAAoHrXNegmSSaTSYcOHdKpU6ccellZWQoNDVXHjh1d3s9qtaqyslIBAQEurbdYLDp06JBuueUWh96tt94qSfr888/t6hUVFXr55Zf14IMP6qabbnK67yeffKKSkhKNHz9eklRZWanKykqXf8eVoqKilJaWpn379mnKlCkaMGCAhg8frldffVUWi8W2rrKyUjt27NDjjz+uIUOGaMCAARo/frzef/99u/1yc3MVFRWlnJwcZWZmavz48erfv7/Gjh2rjIyMOs0IAAAAAAAAAAAAAAAAAA2lXoJukpSdnW1XLy4u1p49e2x9Vx05ckRlZWUu3wJ36tQpVVRUOA2sVdW+/fZbu/qqVatksVj00EMPVbvvZ599pq5du6qkpEQJCQmKjo5W//799fjjj+ubb75x/Qf9fwcPHtRjjz2mzp07KykpScOGDdN//Md/2D2runv3bj399NNq0qSJZs2apblz56p9+/aaN2+edu7c6bDnunXrlJKSolGjRun5559Xx44dlZycrNzc3FrPBwAAAAAAAAAAAAAAAAANxbOuH5aWlurixYsOdS8vL3l5edn+DgoKUkREhLKysjRx4kRbfceOHaqoqJDJZNLBgwednmG1Wm1nlJWV6fDhw0pNTVWfPn00YsQIl+YsLCyUJDVt2tShV1WrWiNJJ06c0Ntvv60FCxbIx8en2n2PHz+ugIAAPfrooxo2bJhiYmJUUFCgxYsXa/Lkydq4caNatWrl0oyStH//fj311FO6//77JUlDhw7V999/r82bN2vKlCmSpHbt2mnt2rXq2rWr7btBgwZp5MiRys7O1h133OGw56pVq9StWzdJl0OHw4cP15YtW5w+twoAAAAAAAAAAAAAAAAA7qjOQbeYmBin9fj4eE2bNs2uNmTIEKWlpenMmTNq3bq1JMlsNis0NFTBwcHVBt3Onj2rQYMG2dW6du2q5557Tp6ero1eXl4uSfL29nboVdVKS0tttVdeeUV9+/ZVdHR0jfv+9NNPys/P1yOPPGILoklScHCwJk+erLfffltz5sxxaUZJCg0NtT2DWqVXr176+OOPVVJSosaNGzt94vXSpUtq2bKlzp4969AbO3asLeQmSZ6enrr55puVl5fn8lwAAAAAAAAAAAAAAAAA0NDqHHSbP3++LbR2paCgIIeayWRSWlqasrOzNWHCBBUXF2v37t2Kj4+v8YxmzZpp4cKFkqSKigqdPHlSGRkZGjdunJKSkjR06NCrzlkViLt06ZJDz2KxSPol8PbBBx9o//79evfdd6+6b2VlpRo1amS7ga1Kr1691LlzZ/3jH/+46h5X6tu3rzw8POxqTZo0kSRduHBBjRs3liR999132rp1q/bu3av8/HxbwK1Zs2YOe95+++0OtSZNmuj48eO1mg0AAAAAAAAAAAAAAAAAGlKdg27du3dXhw4dXFrbpk0bRUREyGw2a8KECcrJyVF5eblMJlON33l5ealXr152tdGjRysuLk7Jycnq06eP0ydJr+Tv7y9JKi4uduiVlJTY1lRUVCgtLU3jxo2T0WjU+fPnbeuqAnHnz5+XwWBQQECAmjRpIk9PTwUGBjrs27FjR3388cc1zvVrRqPRoWYwGCRdDtVJ0ieffKJnnnlGgYGBMplMGjFihNq3b68lS5bUas+q/QAAAAAAAAAAAAAAAADgRlDnoFttDR06VIsWLdK5c+eUlZWlkJAQBQcH13ofg8Egk8mkxYsX69ChQxo4cGCN69u2bSsPDw+dOnXKoVdQUCBJateunUpLS/XDDz9o3bp1WrdundO9TCaTunbtqg0bNqhZs2a2Z1F/zWKxuPy0qqsqKio0b948/e53v9Nbb70lPz8/W+9anwUAAAAAAAAAAAAAAAAA7qTeElImk0mpqanatm2bdu3apbi4uDrvZbVaJV0Of12Nr6+vunTpooMHDzr0Dhw4IEmKiIiQn5+fXn/9dad7pKWlSZISEhJsT4h2795dn332mS5evGh3c1plZaWOHTumTp061eo3XU1+fr5++OEHxcbG2oXcLBaLvv76a3Xt2vWangcAAAAAAAAAAAAAAAAA7qLegm6tWrVSZGSk1q5dq7Kysqs+W1odi8Wi7du3y9vbW5GRkS59M2zYMC1dulT79++3fWOxWLRp0yZ16tRJYWFh8vDwUN++fZ1+HxAQIEl2/WHDhmnFihVau3atpk6daqtv3bpVp0+fVkxMTJ1+X3Wqbm0rKiqyqy9fvtzps6wAAAAAAAAAAAAAAAAA8K+izkG33NxcHT9+3GkvOjraad1kMiklJcXlZ0vLysqUk5Mj6fItbqdPn1ZmZqaOHj2qmTNnqnnz5i7NOn78eG3evFmzZ8/W5MmT1axZM23atEn5+flavHixPDw8XNrnSsHBwYqNjdXy5cv1448/6rbbbtNXX32lt99+Wz169ND48eNrvWdNOnTooM6dO2vDhg3y8fFRcHCwdu7cqV27dqlNmzbX9CwAAAAAAAAAAAAAAAAAcCd1Drq99NJL1fZyc3Od1queL3X1NrfCwkLNmjXL9rfRaFRYWJhSUlI0ePBgl2c1Go1KT0/Xa6+9puXLl6u0tFRhYWFasmSJbrvtNpf3+bXHH39crVu31rvvvqvNmzeradOmGj9+vKZOnSpvb+867+uMwWBQamqqFi1apA0bNqiyslJ9+vTRihUr9Oyzz17TswAAAAAAAAAAAAAAAADAnXhYrVZrQw/xWxQXF6u6n2AwGGQ0Gut5ol9UVlaqpKSk2r63t7d8fHzqcaK68UixNPQIAOCU9an7GnoEAHDOmtHQEwAAAAAAAAAAAAD/Uup8o5u7iImJUUFBgdNe7969lZ6eXs8T/eLUqVMaM2ZMtf1HHnlEU6dOrceJAAAAAAAAAAAAAAAAAODGc8MH3RYsWKDy8nKnPX9//3qexl7Lli21YsWKavtBQUH1OA0AAAAAAAAAAAAAAAAA3Jhu+KdLcf2lp6crLi5OXl5eDT0KAAAAAAAAAAAAAAAAgH9DjRp6AAAAAAAAAAAAAAAAAAAAakLQDQAAAAAAAAAAAAAAAADg1gi6AQAAAAAAAAAAAAAAAADcGkE3AAAAAAAAAAAAAAAAAIBbI+gGAAAAAAAAAAAAAAAAAHBrBN0AAAAAAAAAAAAAAAAAAG6NoBsAAAAAAAAAAAAAAAAAwK0RdAMAAAAAAAAAAAAAAAAAuDWCbgAAAAAAAAAAAAAAAAAAt0bQDQAAAAAAAAAAAAAAAADg1jysVqu1oYeAe/NIsTT0CAB+A+tT9zX0CAB+C2tGQ08AAAAAAAAAAAAAAA2OG90AAAAAAAAAAAAAAAAAAG6NoBsAAAAAAAAAAAAAAAAAwK0RdAMAAAAAAAAAAAAAAAAAuDWCbgAAAAAAAAAAAAAAAAAAt+ZZ2w+WLVum5cuX17gmNzdXU6ZM0d69exUeHq41a9ZUu/b999/XvHnzJEmZmZlq166dJNm+v1JAQID69eunhIQEtWnTprajq6SkRDNmzNCBAwf05ptvKioqymFNcXGx3nrrLZnNZp07d04tWrRQr169NH36dAUFBUmSNm/erBdeeKHGs+bOnau777671jMCAAAAAAAAAAAAAAAAAOzVOuhWJTExUS1btqxxjcFg0OHDh1VQUKC2bds6XWM2m2UwGHTp0iWHXmBgoJKSkiRJFotF+fn52rBhgyZOnKj169fbgmeuOHLkiF588UXl5eVVu+bcuXN64IEH5OHhoXvuuUchISH6/vvvtWbNGu3evVvvvPOOmjZtqttuu02LFi1yukd+fr7S0tLk6+vr8mwAAAAAAAAAAAAAAAAAgOrVOegWFRWlDh061Limc+fO+uqrr2Q2mxUbG+vQLyoq0p49exQSEqJjx4459H18fBQdHW1X6927t+Lj47Vx40YlJCS4PO+0adMUHh6uBx98UM8++6zTNYWFhRo4cKBmz55tF1QLDg7W7NmztW3bNk2YMEFt2rSp9ka5PXv2yGg0asCAAS7PBgAAAAAAAAAAAAAAAACoXqPruXlAQIB69Oghs9nstL9z505ZLBb179/f5T179uwpPz+/Gm9mc2bJkiVaunRpjbfQhYSEKDEx0eE2tt69e0uSvvnmmxrPKCsr09atWzVs2DAZjcZazQcAAAAAAAAAAAAAAAAAcO66Bt0kyWQy6dChQzp16pRDLysrS6GhoerYsaPL+1mtVlVWViogIKBWc/To0aNW669UUlIiSfL3969xndlsVmFhoe69916X987NzVVUVJRycnKUmZmp8ePHq3///ho7dqwyMjLs1v7888965513NGnSJEVHR2vw4MGaPHmy9u/fb7du2bJlioqK0vnz5/X6669r1KhRGjBggCZNmqQDBw64PBsAAAAAAAAAAAAAAAAAuIN6CbpJUnZ2tl29uLhYe/bssfVddeTIEZWVldXqFrjfauvWrZJ+udmtOu+99566dOmim2++udZnrFu3TikpKRo1apSef/55dezYUcnJycrNzbWteeutt/TGG2+oV69eev755/XEE0+osLBQ06dP17lz5xz2fPrpp5WVlaVHHnlEc+bM0cWLF5WQkKALFy7Uej4AAAAAAAAAAAAAAAAAaCiedf2wtLRUFy9edKh7eXnJy8vL9ndQUJAiIiKUlZWliRMn2uo7duxQRUWFTCaTDh486PQMq9VqO6OsrEyHDx9Wamqq+vTpoxEjRtR19Fo5deqUVq1apbCwsBrDdcePH9fnn3+uOXPm1Omc/fv3a9WqVerWrZukywHB4cOHa8uWLYqKipIk9evXT7GxsWratKntu06dOikuLk6ffvqpxowZY7fnmTNntHr1ajVp0kSSFBYWptjYWO3YsUN33313neYEAAAAAAAAAAAAAAAAgPpW56BbTEyM03p8fLymTZtmVxsyZIjS0tJ05swZtW7dWtLlZz5DQ0MVHBxcbdDt7NmzGjRokF2ta9eueu655+TpWefRXVZZWam5c+eqoqJCc+fOlYeHR7VrN23aJB8fH40cObJOZ40dO9YWcpMkT09P3XzzzcrLy7PVqgJvV2rWrJmky/9WvzZ9+nRbyE2SunXrJl9fX7s9AQAAAAAAAAAAAAAAAMDd1TktNn/+fFto7UpBQUEONZPJpLS0NGVnZ2vChAkqLi7W7t27FR8fX+MZzZo108KFCyVJFRUVOnnypDIyMjRu3DglJSVp6NChdR3fJStXrtQ///lPzZkzR127dq12XXl5uf72t7/JZDIpICCgTmfdfvvtDrUmTZro+PHjdrV9+/bpo48+0hdffKH8/HzbM6QWi8WlPQMDA1VUVFSnGQEAAAAAAAAAAAAAAACgIdQ56Na9e3d16NDBpbVt2rRRRESEzGazJkyYoJycHJWXl8tkMtX4nZeXl3r16mVXGz16tOLi4pScnKw+ffrYPeN5LeXk5Cg9PV0jR47U+PHja1xrNptVWFiosWPH1vk8o9HoUDMYDKqsrLT9vWzZMi1fvlzdu3fXwIEDdf/996t9+/bVBgYbN27sUGvUqJHdngAAAAAAAAAAAAAAAADg7hrV10FDhw7V/v37de7cOWVlZSkkJETBwcG13sdgMMhkMqmkpESHDh269oNKOn78uJ599ll169ZNzz777FXXv/fee+rUqZNDKO9aOnbsmFasWKF7771Xa9as0dSpUzVy5EiFh4dftzMBAAAAAAAAAAAAAAAAwB3UW9DNZDLJarVq27Zt2rVr1296dtRqtUq6/JzptXbhwgU98cQT8vX1VUpKinx8fGpcf/z4ce3du1f33nvvNZ/lSnv37pXVatUf/vAHeXh42OpHjx69rucCAAAAAAAAAAAAAAAAQEOr89OltdWqVStFRkZq7dq1Kisru+qzpdWxWCzavn27vL29FRkZeU1nrKysVGJiogoKCrRs2TK1bt36qt+899578vb21l133XVNZ/k1Ly8vSVJRUZGtZrFYtHTp0ut6LgAAAAAAAAAAAAAAAAA0tDoH3XJzc3X8+HGnvejoaKd1k8mklJQUl58tLSsrU05OjqTLt7idPn1amZmZOnr0qGbOnKnmzZvXdXynVq9erU8//VTDhw9XYWGh7ewqgYGB6tmzp+3v8vJy/e1vf9PgwYPVtGnTazrLr/Xt21c+Pj5KTk5WXFyc/Pz89O67716XW+0AAAAAAAAAAAAAAAAAwJ3UOej20ksvVdvLzc11WjeZTEpNTXX5NrfCwkLNmjXL9rfRaFRYWJhSUlI0ePDgWs3rivz8fEnShx9+qA8//NCh37t3b6Wnp9v+zs7O1vnz5zV27NhrPsuv3XTTTUpNTdUbb7yhtLQ0NW7cWCaTSZMmTdLIkSOv+/kAAAAAAAAAAAAAAAAA0FA8rFartaGH+C2Ki4tV3U8wGAwyGo31PNEvKioqVFpaWm3fz89Pnp719npsnXmkWBp6BAC/gfWp+xp6BAC/hTWjoScAAAAAAAAAAAAAgAbn/imrq4iJiVFBQYHT3q9vYKtv27Zt0wsvvFBt/80331RUVFQ9TgQAAAAAAAAAAAAAAAAAN54b/ka3w4cPq7y83GnP399foaGh9TzRL3788UedOHGi2n5oaKj8/f3rcaK64UY34MbGjW7ADY4b3QAAAAAAAAAAAADgxr/RLTw8vKFHqFbz5s3VvHnzhh4DAAAAAAAAAAAAAAAAAG5oN/yNbrj+0tPTFRcXJy8vr4YeBQAAAAAAAAAAAAAAAMC/oUYNPQAAAAAAAAAAAAAAAAAAADUh6AYAAAAAAAAAAAAAAAAAcGsE3QAAAAAAAAAAAAAAAAAAbo2gGwAAAAAAAAAAAAAAAADArRF0AwAAAAAAAAAAAAAAAAC4NYJuAAAAAAAAAAAAAAAAAAC3RtANAAAAAAAAAAAAAAAAAODWCLoBAAAAAAAAAAAAAAAAANwaQTcAAAAAAAAAAAAAAAAAgFvzsFqt1oYeAu7NI8XS0CMA/xasT93X0CMA/z6sGQ09AQAAAAAAAAAAAACgFrjRDQAAAAAAAAAAAAAAAADg1gi6AQAAAAAAAAAAAAAAAADcGkE3AAAAAAAAAAAAAAAAAIBbI+gGAAAAAAAAAAAAAAAAAHBrBN0AAAAAAAAAAAAAAAAAAG7Ns7YfLFu2TMuXL69xTW5urqZMmaK9e/cqPDxca9asqXbt+++/r3nz5kmSMjMz1a5dO0myfX+lgIAA9evXTwkJCWrTpk1tR1dJSYlmzJihAwcO6M0331RUVJTTdZ988onS09OVl5enwMBAjRw5UlOnTpW3t7fDWovFoj/96U/6+9//rrlz5+ruu++u9VwAAAAAAAAAAAAAAAAAgOrVOuhWJTExUS1btqxxjcFg0OHDh1VQUKC2bds6XWM2m2UwGHTp0iWHXmBgoJKSkiRdDpTl5+drw4YNmjhxotavX6+goCCX5z1y5IhefPFF5eXl1bhu586devLJJ3X77bdr7ty5+uabb/T222/rxIkTWrhwod3a7777TsnJyfr8889dngMAAAAAAAAAAAAAAAAAUDt1DrpFRUWpQ4cONa7p3LmzvvrqK5nNZsXGxjr0i4qKtGfPHoWEhOjYsWMOfR8fH0VHR9vVevfurfj4eG3cuFEJCQkuzztt2jSFh4frwQcf1LPPPut0jcViUUpKisLCwrRo0SIZDAZJUpMmTfTqq69q165d6tevn239008/LYPBoEWLFunxxx93eRYAAAAAAAAAAAAAAAAAgOsaXc/NAwIC1KNHD5nNZqf9nTt3ymKxqH///i7v2bNnT/n5+V31ZrZfW7JkiZYuXVrjLXSff/65CgoKNG7cOFvITZLGjh0rX19fbd261W59YmKiVq9ereDg4FrNAgAAAAAAAAAAAAAAAABw3XUNukmSyWTSoUOHdOrUKYdeVlaWQkND1bFjR5f3s1qtqqysVEBAQK3m6NGjx1XX7N+/X5J066232tV9fX0VHh7u8ERpeHi4PDw8ajXHr+Xm5ioqKko5OTnKzMzU+PHj1b9/f40dO1YZGRl2a3/++We98847mjRpkqKjozV48GBNnjzZNneVZcuWKSoqSufPn9frr7+uUaNGacCAAZo0aZIOHDjwm+YFAAAAAAAAAAAAAAAAgPpWL0E3ScrOzrarFxcXa8+ePba+q44cOaKysrJa3QLnqhMnTshgMKht27YOvfbt2+v06dOqqKi45udK0rp165SSkqJRo0bp+eefV8eOHZWcnKzc3FzbmrfeektvvPGGevXqpeeff15PPPGECgsLNX36dJ07d85hz6efflpZWVl65JFHNGfOHF28eFEJCQm6cOHCdfkNAAAAAAAAAAAAAAAAAHA9eNb1w9LSUl28eNGh7uXlJS8vL9vfQUFBioiIUFZWliZOnGir79ixQxUVFTKZTDp48KDTM6xWq+2MsrIyHT58WKmpqerTp49GjBhR19GrVVhYKH9/f7tnS6s0a9ZMlZWVKioqUosWLa752fv379eqVavUrVs3SZcDgsOHD9eWLVsUFRUlSerXr59iY2PVtGlT23edOnVSXFycPv30U40ZM8ZuzzNnzmj16tVq0qSJJCksLEyxsbHasWOH7r777mv+GwAAAAAAAAAAAAAAAADgeqhz0C0mJsZpPT4+XtOmTbOrDRkyRGlpaTpz5oxat24tSTKbzQoNDVVwcHC1QbezZ89q0KBBdrWuXbvqueeek6dnnUevVnl5uby9vZ32quqlpaXX/FxJGjt2rC3kJkmenp66+eablZeXZ6tVBd6u1KxZM0mX/61+bfr06baQmyR169ZNvr6+dnsCAAAAAAAAAAAAAAAAgLurc1ps/vz5ttDalYKCghxqJpNJaWlpys7O1oQJE1RcXKzdu3crPj6+xjOaNWumhQsXSpIqKip08uRJZWRkaNy4cUpKStLQoUPrOr5Tnp6eunTpktOexWKRpGqDcL/V7bff7lBr0qSJjh8/blfbt2+fPvroI33xxRfKz8+3PUNaNd/V9gwMDFRRUdG1GRoAAAAAAAAAAAAAAAAA6kGdg27du3dXhw4dXFrbpk0bRUREyGw2a8KECcrJyVF5eblMJlON33l5ealXr152tdGjRysuLk7Jycnq06eP3TOev5W/v79KSkqc9qrqjRs3vmbnXcloNDrUDAaDKisrbX8vW7ZMy5cvV/fu3TVw4EDdf//9at++fbWBQWezNmrUyG5PAAAAAAAAAAAAAAAAAHB3jerroKFDh2r//v06d+6csrKyFBISouDg4FrvYzAYZDKZVFJSokOHDl3TGdu1a6eysjL9+OOPDr2CggI1a9bMaSCtPhw7dkwrVqzQvffeqzVr1mjq1KkaOXKkwsPDG2QeAAAAAAAAAAAAAAAAAKgv9RZ0M5lMslqt2rZtm3bt2vWbnh21Wq2SLj9nei1FRERIkg4ePGhXv3Tpkr744gtbvyHs3btXVqtVf/jDH+Th4WGrHz16tMFmAgAAAAAAAAAAAAAAAID6UG9Bt1atWikyMlJr165VWVnZVZ8trY7FYtH27dvl7e2tyMjIazpjv379FBgYqHfffdeuvn37dv30008aMWLENT2vNry8vCRJRUVFtprFYtHSpUsbaiQAAAAAAAAAAAAAAAAAqBeedf0wNzdXx48fd9qLjo52WjeZTEpJSXH52dKysjLl5ORIunyL2+nTp5WZmamjR49q5syZat68eV3Hd8rX11czZsxQcnKy5syZo2HDhunEiRNauXKloqKi6hzOuxb69u0rHx8fJScnKy4uTn5+fnr33Xev+a12AAAAAAAAAAAAAAAAAOBu6hx0e+mll6rt5ebmOq2bTCalpqa6HBgrLCzUrFmzbH8bjUaFhYUpJSVFgwcPrtW8rrr33nvl4+OjNWvW6Pnnn1eTJk00duxYTZs2TY0a1dsFeA5uuukmpaam6o033lBaWpoaN24sk8mkSZMmaeTIkQ02FwAAAAAAAAAAAAAAAABcbx5Wq9Xa0EP8FsXFxaruJxgMBhmNxnqe6BcVFRUqLS2ttu/n5ydPzzpnDeuNR4qloUcA/i1Yn7qvoUcA/n1YMxp6AgAAAAAAAAAAAABALbh/yuoqYmJiVFBQ4LTXu3dvpaen1/NEv9i2bZteeOGFavtvvvmmoqKi6nEiAAAAAAAAAAAAAAAAALjx3PA3uh0+fFjl5eVOe/7+/goNDa3niX7x448/6sSJE9X2Q0ND5e/vX48T1Q03ugH1gxvdgHrEjW4AAAAAAAAAAAAAcEO54YNuuP7S09MVFxcnLy+vhh4FAAAAAAAAAAAAAAAAwL+hRg09AAAAAAAAAAAAAAAAAAAANSHoBgAAAAAAAAAAAAAAAABwawTdAAAAAAAAAAAAAAAAAABujaAbAAAAAAAAAAAAAAAAAMCtEXQDAAAAAAAAAAAAAAAAALg1gm4AAAAAAAAAAAAAAAAAALdG0A0AAAAAAAAAAAAAAAAA4NYIugEAAAAAAAAAAAAAAAAA3BpBNwAAAAAAAAAAAAAAAACAW/OwWq3Whh4C7s0jxdLQIwBuyfrUfQ09AuC+rBkNPQEAAAAAAAAAAAAA4F8IN7oBAAAAAAAAAAAAAAAAANwaQTcAAAAAAAAAAAAAAAAAgFsj6AYAAAAAAAAAAAAAAAAAcGsE3QAAAAAAAAAAAAAAAAAAbo2gGwAAAAAAAAAAAAAAAADArXnW9oNly5Zp+fLlNa7Jzc3VlClTtHfvXoWHh2vNmjXVrn3//fc1b948SVJmZqbatWsnSbbvrxQQEKB+/fopISFBbdq0qe3oKikp0YwZM3TgwAG9+eabioqKclhTXFyst956S2azWefOnVOLFi3Uq1cvTZ8+XUFBQdXu/dNPP+m+++5TYWGh3e8AAAAAAAAAAAAAAAAAAPw2tQ66VUlMTFTLli1rXGMwGHT48GEVFBSobdu2TteYzWYZDAZdunTJoRcYGKikpCRJksViUX5+vjZs2KCJEydq/fr1NQbPfu3IkSN68cUXlZeXV+2ac+fO6YEHHpCHh4fuuecehYSE6Pvvv9eaNWu0e/duvfPOO2ratKnTbxctWiQPDw+X5wEAAAAAAAAAAAAAAAAAuKbOQbeoqCh16NChxjWdO3fWV199JbPZrNjYWId+UVGR9uzZo5CQEB07dsyh7+Pjo+joaLta7969FR8fr40bNyohIcHleadNm6bw8HA9+OCDevbZZ52uKSws1MCBAzV79mz5+vra6sHBwZo9e7a2bdumCRMmOHyXm5ur7OxsPfTQQ3rzzTddngkAAAAAAAAAAAAAAAAAcHWNrufmAQEB6tGjh8xms9P+zp07ZbFY1L9/f5f37Nmzp/z8/Gq8mc2ZJUuWaOnSpTXeQhcSEqLExES7kJt0OVwnSd98843DN+Xl5Zo/f74eeuihOj2nCgAAAAAAAAAAAAAAAACo2XUNukmSyWTSoUOHdOrUKYdeVlaWQkND1bFjR5f3s1qtqqysVEBAQK3m6NGjR63WX6mkpESS5O/v79BbtWqVJOnBBx+s0965ubmKiopSTk6OMjMzNX78ePXv319jx45VRkaG3dqff/5Z77zzjiZNmqTo6GgNHjxYkydP1v79++3WLVu2TFFRUTp//rxef/11jRo1SgMGDNCkSZN04MCBOs0JAAAAAAAAAAAAAAAAAA2lXoJukpSdnW1XLy4u1p49e2x9Vx05ckRlZWW1ugXut9q6daukX252q5Kfn69Vq1bpqaeekre39286Y926dUpJSdGoUaP0/PPPq2PHjkpOTlZubq5tzVtvvaU33nhDvXr10vPPP68nnnhChYWFmj59us6dO+ew59NPP62srCw98sgjmjNnji5evKiEhARduHDhN80KAAAAAAAAAAAAAAAAAPXJs64flpaW6uLFiw51Ly8veXl52f4OCgpSRESEsrKyNHHiRFt9x44dqqiokMlk0sGDB52eYbVabWeUlZXp8OHDSk1NVZ8+fTRixIi6jl4rp06d0qpVqxQWFuYQrluwYIEGDhyo22+//Tefs3//fq1atUrdunWTdDkgOHz4cG3ZskVRUVGSpH79+ik2NlZNmza1fdepUyfFxcXp008/1ZgxY+z2PHPmjFavXq0mTZpIksLCwhQbG6sdO3bo7rvv/s0zAwAAAAAAAAAAAAAAAEB9qHPQLSYmxmk9Pj5e06ZNs6sNGTJEaWlpOnPmjFq3bi1JMpvNCg0NVXBwcLVBt7Nnz2rQoEF2ta5du+q5556Tp2edR3dZZWWl5s6dq4qKCs2dO1ceHh623tatW3Xw4EG9++671+SssWPH2kJukuTp6ambb75ZeXl5tlpV4O1KzZo1k3T53+rXpk+fbgu5SVK3bt3k6+trtycAAAAAAAAAAAAAAAAAuLs6p8Xmz59vC61dKSgoyKFmMpmUlpam7OxsTZgwQcXFxdq9e7fi4+NrPKNZs2ZauHChJKmiokInT55URkaGxo0bp6SkJA0dOrSu47tk5cqV+uc//6k5c+aoa9eutnpRUZHS0tL08MMPO/29deHsVrgmTZro+PHjdrV9+/bpo48+0hdffKH8/HzbM6QWi8WlPQMDA1VUVHRNZgYAAAAAAAAAAAAAAACA+lDnoFv37t3VoUMHl9a2adNGERERMpvNmjBhgnJyclReXi6TyVTjd15eXurVq5ddbfTo0YqLi1NycrL69Olj94zntZSTk6P09HSNHDlS48ePt+stX75cvr6+Gj16tM6fP2+rVz2zWlRUJKPRqICAABkMBpfOMxqNDjWDwaDKykrb38uWLdPy5cvVvXt3DRw4UPfff7/at29fbWCwcePGDrVGjRrZ7QkAAAAAAAAAAAAAAAAA7u76v//5/w0dOlSLFi3SuXPnlJWVpZCQEAUHB9d6H4PBIJPJpMWLF+vQoUMaOHDgNZ/1+PHjevbZZ9WtWzc9++yzDv2TJ0/q5MmTGjFihNPvY2NjJUnr169XWFjYNZnp2LFjWrFihe69914lJibanlF1dpMbAAAAAAAAAAAAAAAAAPwrqbegm8lkUmpqqrZt26Zdu3YpLi6uzntZrVZJl58zvdYuXLigJ554Qr6+vkpJSZGPj4/DmkcffdThljdJ2rVrl9auXat58+apefPmLt9454q9e/fKarXqD3/4gy3kJklHjx69ZmcAAAAAAAAAAAAAAAAAgDuqt6Bbq1atFBkZqbVr16qsrOyqz5ZWx2KxaPv27fL29lZkZOQ1nbGyslKJiYkqKCjQsmXL1Lp1a6frunTp4rR+5swZSVJkZKTatWt3TWfz8vKSdPlZ1CoWi0VLly69pucAAAAAAAAAAAAAAAAAgLupc9AtNzdXx48fd9qLjo52WjeZTEpJSXH52dKysjLl5ORIunyL2+nTp5WZmamjR49q5syZat68eV3Hd2r16tX69NNPNXz4cBUWFtrOrhIYGKiePXte0zNd1bdvX/n4+Cg5OVlxcXHy8/PTu+++e11utQMAAAAAAAAAAAAAAAAAd1LnoNtLL71UbS83N9dpver5UldvcyssLNSsWbNsfxuNRoWFhSklJUWDBw+u1byuyM/PlyR9+OGH+vDDDx36vXv3Vnp6+jU/1xU33XSTUlNT9cYbbygtLU2NGzeWyWTSpEmTNHLkyAaZCQAAAAAAAAAAAAAAAADqg4fVarU29BC/RXFxsar7CQaDQUajsZ4n+kVFRYVKS0ur7fv5+cnTs95ej60zjxRLQ48AuCXr/2Pv3uOirvP+/z9xGIQRNDwkmq6oKIohHrA8hKvLaGlla5sHXFoviDxxc8Us++66Gm5oB4lQKxXMLU/r5t5cQ6/yMjDNvVrz4mdyiFWLEnMFD5UoGIfB+f3hjclpBhxIYdx93P+S1+v9eb9f49/P2/v9zGPNPQLgvqw7m3sCAAAAAAAAAAAAAMC/EfdPWd1AVFSUiouLnfaa8wY2SdqzZ4+WLl1aZ3/t2rUKDw9vwokAAAAAAAAAAAAAAAAA4PZz29/oVlBQoKqqKqc9X19fBQUFNfFEP/j222916tSpOvtBQUHy9fVtwokahxvdAOe40Q2oBze6AQAAAAAAAAAAAABuots+6IZbLy0tTTExMTIajc09CgAAAAAAAAAAAAAAAID/QC2aewAAAAAAAAAAAAAAAAAAAOpD0A0AAAAAAAAAAAAAAAAA4NYIugEAAAAAAAAAAAAAAAAA3BpBNwAAAAAAAAAAAAAAAACAWyPoBgAAAAAAAAAAAAAAAABwawTdAAAAAAAAAAAAAAAAAABujaAbAAAAAAAAAAAAAAAAAMCtEXQDAAAAAAAAAAAAAAAAALg1gm4AAAAAAAAAAAAAAAAAALfmYbVarc09BNybR7KluUcAbgnrM4819wjAzWfd2dwTAAAAAAAAAAAAAABw03GjGwAAAAAAAAAAAAAAAADArRF0AwAAAAAAAAAAAAAAAAC4NYJuAAAAAAAAAAAAAAAAAAC3RtANAAAAAAAAAAAAAAAAAODWCLoBAAAAAAAAAAAAAAAAANyaZ0M/WLdundLT0+tdk52drRkzZujIkSMKCQnRxo0b61z77rvv6vnnn5ckZWRkqHPnzpJk+/56fn5+Gjp0qBISEtSxY8cGzV1WVqbVq1dr//79Ki8vV0hIiOLj4xUWFma37uuvv9aWLVv08ccf6/z58/Lz89OQIUM0e/ZsdenSxWFfi8Wi3/3ud/rwww/13HPP6eGHH27QXAAAAAAAAAAAAAAAAACA+jU46FZr0aJFat++fb1rDAaDCgoKVFxcrE6dOjldk5WVJYPBoJqaGodemzZtlJiYKOlaoKyoqEhbt27VtGnTtGXLFgUEBLg0q8Vi0dy5c/Xll18qLi5OHTp00LZt2zRr1iytX79e/fr1kyRVVFRo6tSp6tSpkyZOnKiuXbvq1KlT2rJliw4fPqytW7eqQ4cOtn1Pnz6tpKQkffrppy7NAQAAAAAAAAAAAAAAAABouEYH3cLDw9W1a9d61/To0UNffPGFsrKyFB0d7dC/dOmSDh8+rJ49e+rEiRMO/ZYtWyoiIsKuNmjQIMXGxmrbtm1KSEhwadZdu3YpLy9PK1as0OjRoyVJI0eO1KRJk/TKK69ow4YNkiQPDw9Nnz5dTzzxhAwGg+37/v37a9asWdq2bZvmzp1rqz/77LMyGAx69dVX9dvf/talWQAAAAAAAAAAAAAAAAAADdPiVm7u5+enfv36KSsry2n/wIEDslgsGj58uMt79u/fXz4+PiosLHT5m/fee08BAQG2kJskmUwmTZgwQbm5uTp9+rSka8G6GTNm2IXcpGuhvlatWunYsWN29UWLFuntt99WYGCgy7MAAAAAAAAAAAAAAAAAABrmlgbdJMlsNis/P18lJSUOvczMTAUFBalbt24u72e1WnX16lX5+fm5tN5isSg/P18DBw506A0ePFiSXHp61Gg0ymQy2dVCQkLk4eHh0hx1yc7OVnh4uA4ePKiMjAxNnjxZw4cP18SJE7Vz5067td9//73eeecdTZ8+XRERERo1apTi4uKUk5Njt27dunUKDw/XxYsX9frrr2v8+PEaMWKEpk+frtzc3J80LwAAAAAAAAAAAAAAAAA0tSYJuknSvn377OplZWU6fPiwre+qY8eOqbKy0uVb4EpKSlRdXa0uXbo49GprX3/9db175OXl6eLFi07DcjfL5s2blZycrPHjx2vJkiXq1q2bkpKSlJ2dbVvz5ptvas2aNRowYICWLFmip556SqWlpYqPj9eFCxcc9nz22WeVmZmpJ598UgsXLtSVK1eUkJCgy5cv37LfAQAAAAAAAAAAAAAAAAA3m2djP6yoqNCVK1cc6kajUUaj0fZ3QECAQkNDlZmZqWnTptnq+/fvV3V1tcxms/Ly8pyeYbVabWdUVlaqoKBAKSkpGjJkiB544AGX5iwtLZUk3XHHHQ692lrtmutZLBZ98803+vTTT7Vq1SoFBwdr4sSJLp3ZGDk5OXrrrbfUp08fSdcCgvfff792796t8PBwSdLQoUMVHR1t91u6d++umJgYffzxx5owYYLdnufOndPbb7+t1q1bS5KCg4MVHR2t/fv36+GHH75lvwUAAAAAAAAAAAAAAAAAbqZGB92ioqKc1mNjYzVnzhy7WmRkpFJTU3Xu3DndeeedkqSsrCwFBQUpMDCwzqDb+fPnNXLkSLta7969tXjxYnl6ujZ6VVWVJMnLy8uhV1urqKhw6O3du1dLliyRJA0YMEAvvfSSfHx8XDqzMSZOnGgLuUmSp6en7r77bhUWFtpqtYG36/n7+0u69n/1Y/Hx8baQmyT16dNH3t7ednsCAAAAAAAAAAAAAAAAgLtrdNBt+fLlttDa9QICAhxqZrNZqamp2rdvn6ZOnaqysjJ98sknio2NrfcMf39/rVixQpJUXV2tM2fOaOfOnZo0aZISExM1ZsyYG85ZG4irqalx6FksFknOQ3DDhg3T6tWrVVRUpHfeeUe//vWvlZKSopCQkBue2RjDhg1zqLVu3VonT560qx09elQffPCBPvvsMxUVFdmeIa39LTfas02bNrp06dLNGRoAAAAAAAAAAAAAAAAAmkCjg259+/ZV165dXVrbsWNHhYaGKisrS1OnTtXBgwdVVVUls9lc73dGo1EDBgywqz300EOKiYlRUlKShgwZ4vRJ0uv5+vpKksrKyhx65eXldmuu5+/vr2HDhmnYsGF68MEHNWXKFC1dulR/+ctf6j2vsUwmk0PNYDDo6tWrtr/XrVun9PR09e3bV/fdd5+mTJmiu+66q87AYKtWrRxqLVq0sNsTAAAAAAAAAAAAAAAAANxdi6Y6aMyYMcrJydGFCxeUmZmpnj17KjAwsMH7GAwGmc1mlZeXKz8//4brO3XqJA8PD5WUlDj0iouLJUmdO3eudw8/Pz+NGDFChYWFthvUmtqJEye0fv16/fKXv9TGjRs1c+ZMjRs37pbdMAcAAAAAAAAAAAAAAAAA7qLJgm5ms1lWq1V79uzRoUOHXHp2tC5Wq1XStedMb8Tb21u9evVSXl6eQy83N1eSFBoaesN9DAaDJKmqqqoho940R44ckdVq1aOPPioPDw9b/fjx480yDwAAAAAAAAAAAAAAAAA0lSYLunXo0EFhYWHatGmTKisrb/hsaV0sFov27t0rLy8vhYWFufTN2LFjdfz4ceXk5Njts2PHDnXv3l3BwcGSpD179uijjz5y+P7ixYv68MMPFRgYqHbt2jVq7p/KaDRKki5dumSrWSwWvfbaa80yDwAAAAAAAAAAAAAAAAA0Fc/Gfpidna2TJ0867UVERDitm81mJScnu/xsaWVlpQ4ePCjp2i1uZ8+eVUZGho4fP6558+apbdu2Ls06efJk7dq1SwsWLFBcXJz8/f21Y8cOFRUVadWqVbYb0oxGoxYsWKDBgwfr5z//uTp27Kji4mJt27ZNpaWlWrp0qUvn3Qr33nuvWrZsqaSkJMXExMjHx0fbt2936VY7AAAAAAAAAAAAAAAAALidNTrotmzZsjp72dnZTutms1kpKSku3+ZWWlqq+fPn2/42mUwKDg5WcnKyRo0a5fKsJpNJaWlpWrlypdLT01VRUaHg4GCtXr1a99xzj21dZGSk1q9fr+3bt2vz5s365ptvZDKZNGDAAL388svq27evy2febF26dFFKSorWrFmj1NRUtWrVSmazWdOnT9e4ceOabS4AAAAAAAAAAAAAAAAAuNU8rFartbmH+CnKyspU108wGAwymUxNPNEPqqurVVFRUWffx8dHnp6Nzho2GY9kS3OPANwS1mcea+4RgJvPurO5JwAAAAAAAAAAAAAA4KZz/5TVDURFRam4uNhpb9CgQUpLS2viiX6wZ8+eep87Xbt2rcLDw5twIgAAAAAAAAAAAAAAAAC4/dz2N7oVFBSoqqrKac/X11dBQUFNPNEPvv32W506darOflBQkHx9fZtwosbhRjf8u+JGN/xb4kY3AAAAAAAAAAAAAMC/ods+6IZbLy0tTTExMTIajc09CgAAAAAAAAAAAAAAAID/QC2aewAAAAAAAAAAAAAAAAAAAOpD0A0AAAAAAAAAAAAAAAAA4NYIugEAAAAAAAAAAAAAAAAA3BpBNwAAAAAAAAAAAAAAAACAWyPoBgAAAAAAAAAAAAAAAABwawTdAAAAAAAAAAAAAAAAAABujaAbAAAAAAAAAAAAAAAAAMCtEXQDAAAAAAAAAAAAAAAAALg1gm4AAAAAAAAAAAAAAAAAALfmYbVarc09BNybR7KluUcAbgrrM4819wjAzWPd2dwTAAAAAAAAAAAAAADQZLjRDQAAAAAAAAAAAAAAAADg1gi6AQAAAAAAAAAAAAAAAADcGkE3AAAAAAAAAAAAAAAAAIBbI+gGAAAAAAAAAAAAAAAAAHBrBN0AAAAAAAAAAAAAAAAAAG7Ns6EfrFu3Tunp6fWuyc7O1owZM3TkyBGFhIRo48aNda5999139fzzz0uSMjIy1LlzZ0myfX89Pz8/DR06VAkJCerYsWOD5i4rK9Pq1au1f/9+lZeXKyQkRPHx8QoLC7Nb9/XXX2vLli36+OOPdf78efn5+WnIkCGaPXu2unTpYrf29OnTeuONN/TJJ5+osrJSQUFBevLJJzVixIgGzQYAAAAAAAAAAAAAAAAAqFuDg261Fi1apPbt29e7xmAwqKCgQMXFxerUqZPTNVlZWTIYDKqpqXHotWnTRomJiZIki8WioqIibd26VdOmTdOWLVsUEBDg0qwWi0Vz587Vl19+qbi4OHXo0EHbtm3TrFmztH79evXr10+SVFFRoalTp6pTp06aOHGiunbtqlOnTmnLli06fPiwtm7dqg4dOki6FnKbPn26PDw8FBcXp/bt2+u9997T/Pnz9fLLL2vUqFEuzQYAAAAAAAAAAAAAAAAAqF+jg27h4eHq2rVrvWt69OihL774QllZWYqOjnboX7p0SYcPH1bPnj114sQJh37Lli0VERFhVxs0aJBiY2O1bds2JSQkuDTrrl27lJeXpxUrVmj06NGSpJEjR2rSpEl65ZVXtGHDBkmSh4eHpk+frieeeEIGg8H2ff/+/TVr1ixt27ZNc+fOlSStXr1aZWVl+vOf/6wePXpIksxms/7f//t/WrFihSIiIuz2AAAAAAAAAAAAAAAAAAA0Totbubmfn5/69eunrKwsp/0DBw7IYrFo+PDhLu/Zv39/+fj4qLCw0OVv3nvvPQUEBNhCbpJkMpk0YcIE5ebm6vTp05KuBetmzJjhEFALDw9Xq1atdOzYMUnXboj7+9//ruHDh9tCbtK1oNx//dd/6ezZszp69KjL8wEAAAAAAAAAAAAAAAAA6nZLg27StVvO8vPzVVJS4tDLzMxUUFCQunXr5vJ+VqtVV69elZ+fn0vrLRaL8vPzNXDgQIfe4MGDJUmffvrpDfcxGo0ymUySpO+++06VlZVO5w4MDJQkWyjuRnbt2qXw8HB9/vnn2rRpkx555BENHz5cU6ZM0UcffWS39uLFi9qwYYOioqI0YsQIRUZG2p5kvV5iYqIiIyN1+fJlvfDCCxozZoxGjBih2bNn6+TJky7NBQAAAAAAAAAAAAAAAADuokmCbpK0b98+u3pZWZkOHz5s67vq2LFjqqysdPkWuJKSElVXV6tLly4Ovdra119/Xe8eeXl5unjxoi0s17JlS0nXfsOPFRcXS7oWhmuI5ORkbdq0SVFRUVq0aJFatWqlhQsX6tSpU7Y1L7/8st555x39/Oc/19KlSzV79mwVFhZqzpw5qqqqstvPYrFo9uzZKigo0Lx58zR//nx99dVXmj9/viwWS4NmAwAAAAAAAAAAAAAAAIDm5NnYDysqKnTlyhWHutFolNFotP0dEBCg0NBQZWZmatq0abb6/v37VV1dLbPZrLy8PKdnWK1W2xmVlZUqKChQSkqKhgwZogceeMClOUtLSyVJd9xxh0Ovtla75noWi0XffPONPv30U61atUrBwcGaOHGiJKl169a66667dOjQIdXU1Ng9dbpr164696zP559/rq1btyogIECSNGLECN1///16//33NXPmTEnS+PHjtXjxYvn4+Ni+8/Pz06JFi5Sbm6vw8HBbvby8XC1bttQbb7xhC+a1a9dOzzzzjI4ePWq3FgAAAAAAAAAAAAAAAADcWaODblFRUU7rsbGxmjNnjl0tMjJSqampOnfunO68805JUlZWloKCghQYGFhn0O38+fMaOXKkXa13795avHixPD1dG732pjMvLy+HXm2toqLCobd3714tWbJEkjRgwAC99NJLdgGzqKgoJScn6/e//73mzp0rHx8f7dq1Sx999JHatGljF35zRUxMjC3kJl0L4XXr1k2FhYW22n333efwXdu2bSVd+7/6sQULFthCbrW/Q5IKCwsJugEAAAAAAAAAAAAAAAC4bTQ66LZ8+XJbaO1614e1apnNZqWmpmrfvn2aOnWqysrK9Mknnyg2NrbeM/z9/bVixQpJUnV1tc6cOaOdO3dq0qRJSkxM1JgxY244Z20grqamxqFX+4SnsxDcsGHDtHr1ahUVFemdd97Rr3/9a6WkpCgkJESSNGXKFJWWlurtt99WVlaWJKlPnz5KTU3VtGnT1KZNmxvO9uPzfqx169a6fPmyXe1///d/9eGHH+rYsWM6deqU7ca7Hz9H2qZNG9us1+8nyWFPAAAAAAAAAAAAAAAAAHBnjQ669e3bV127dnVpbceOHRUaGqqsrCxNnTpVBw8eVFVVlcxmc73fGY1G2y1ktR566CHFxMQoKSlJQ4YMcfok6fV8fX0lSWVlZQ698vJyuzXX8/f317BhwzRs2DA9+OCDmjJlipYuXaq//OUvkiQPDw/NnDlTjz/+uIqKiuTr66uuXbvq7Nmz+v7779WtW7d65/oxk8nkUGvRooVdQC8xMVG7d+/W4MGD9Ytf/EJdunSRl5eXnn76aZf2q71lzlnoDwAAAAAAAAAAAAAAAADcVYumOmjMmDHKycnRhQsXlJmZqZ49eyowMLDB+xgMBpnNZpWXlys/P/+G6zt16iQPDw+VlJQ49IqLiyVJnTt3rncPPz8/jRgxQoWFhQ63oZlMJrvQ3+HDhyVJAwcOdOn3uOrAgQPavXu3Zs2apXXr1ik2NlZjx45Vjx49buo5AAAAAAAAAAAAAAAAAOBumizoZjabZbVatWfPHh06dMilZ0frYrVaJV17zvRGvL291atXL+Xl5Tn0cnNzJUmhoaE33Kf2NrSqqqp6123fvl39+/dXp06dbrhnQxw5ckSS9Nhjj9nVT5w4cVPPAQAAAAAAAAAAAAAAAAB302RBtw4dOigsLEybNm1SZWXlDZ8trYvFYtHevXvl5eWlsLAwl74ZO3asjh8/rpycHLt9duzYoe7duys4OFiStGfPHn300UcO31+8eFEffvihAgMD1a5duzrP2bhxo/75z39qxowZDfxVN2Y0GiVJpaWltlp5ebnS0tJu+lkAAAAAAAAAAAAAAAAA4E48G/thdna2Tp486bQXERHhtG42m5WcnOzys6WVlZU6ePCgpGu3uJ09e1YZGRk6fvy45s2bp7Zt27o06+TJk7Vr1y4tWLBAcXFx8vf3144dO1RUVKRVq1bJw8ND0rUw2YIFCzR48GD9/Oc/V8eOHVVcXKxt27aptLRUS5cute25detWffnllxowYIA8PT114MABffDBB5o5c6aGDh3q0lwNERERoY0bN+rZZ59VdHS0ampqtGnTJnl7e9/0swAAAAAAAAAAAAAAAADAnTQ66LZs2bI6e9nZ2U7rZrNZKSkpLt/mVlpaqvnz59v+NplMCg4OVnJyskaNGuXyrCaTSWlpaVq5cqXS09NVUVGh4OBgrV69Wvfcc49tXWRkpNavX6/t27dr8+bN+uabb2QymTRgwAC9/PLL6tu3r21tr169tGfPHn3wwQeSpJ49e2rFihUaPXq0y3M1RFhYmJKSkvSnP/1JL7zwgtq1a6cHH3xQI0eO1OOPP35LzgQAAAAAAAAAAAAAAAAAd+BhtVqtzT3ET1FWVqa6foLBYJDJZGriiX5QWVmpqqqqOvutWrVSixZN9npso3kkW5p7BOCmsD7zWHOPANw81p3NPQEAAAAAAAAAAAAAAE2m0Te6uYuoqCgVFxc77Q0aNEhpaWlNPNEP3nrrLaWnp9fZz8jIUOfOnZtwIgAAAAAAAAAAAAAAAAC4/dz2QbeXXnqpzlvTfH19m3gae4888ojuvffeOvvt27dvwmkAAAAAAAAAAAAAAAAA4PZ02z9dilsvLS1NMTExMhqNzT0KAAAAAAAAAAAAAAAAgP9ALZp7AAAAAAAAAAAAAAAAAAAA6kPQDQAAAAAAAAAAAAAAAADg1gi6AQAAAAAAAAAAAAAAAADcGkE3AAAAAAAAAAAAAAAAAIBbI+gGAAAAAAAAAAAAAAAAAHBrBN0AAAAAAAAAAAAAAAAAAG6NoBsAAAAAAAAAAAAAAAAAwK0RdAMAAAAAAAAAAAAAAAAAuDWCbgAAAAAAAAAAAAAAAAAAt0bQDQAAAAAAAAAAAAAAAADg1jysVqu1uYeAe/NItjT3CECdrM881twjAPWz7mzuCQAAAAAAAAAAAAAAuO1xoxsAAAAAAAAAAAAAAAAAwK0RdAMAAAAAAAAAAAAAAAAAuDWCbgAAAAAAAAAAAAAAAAAAt0bQDQAAAAAAAAAAAAAAAADg1jwb+sG6deuUnp5e75rs7GzNmDFDR44cUUhIiDZu3Fjn2nfffVfPP/+8JCkjI0OdO3eWJNv31/Pz89PQoUOVkJCgjh07NmjusrIyrV69Wvv371d5eblCQkIUHx+vsLAwu3XOzq01ffp0zZ07V5J09epV7d69W7t379aJEydUVVWlwMBAPfroo/rVr34lDw+PBs0HAAAAAAAAAAAAAAAAAHCuwUG3WosWLVL79u3rXWMwGFRQUKDi4mJ16tTJ6ZqsrCwZDAbV1NQ49Nq0aaPExERJksViUVFRkbZu3app06Zpy5YtCggIcGlWi8WiuXPn6ssvv1RcXJw6dOigbdu2adasWVq/fr369etnt/6ee+5RVFSUwz5du3a1/Xv9+vXasGGDxowZowkTJqhFixbau3evXnzxRZ09e1bx8fEuzQYAAAAAAAAAAAAAAAAAqF+jg27h4eF2wS9nevTooS+++EJZWVmKjo526F+6dEmHDx9Wz549deLECYd+y5YtFRERYVcbNGiQYmNjtW3bNiUkJLg0665du5SXl6cVK1Zo9OjRkqSRI0dq0qRJeuWVV7Rhwwa79XfeeafDuT8WEBCgrVu3qkePHrba+PHjNXPmTG3evFnTp0+Xr6+vS/MBAAAAAAAAAAAAAAAAAOrW4lZu7ufnp379+ikrK8tp/8CBA7JYLBo+fLjLe/bv318+Pj4qLCx0+Zv33ntPAQEBtpCbJJlMJk2YMEG5ubk6ffq0y3vVmjBhgl3IrVZERISqq6sbNB8AAAAAAAAAAAAAAAAAoG63NOgmSWazWfn5+SopKXHoZWZmKigoSN26dXN5P6vVqqtXr8rPz8+l9RaLRfn5+Ro4cKBDb/DgwZKkTz/91OXzb8TLy0vStSCdK7KzsxUeHq6DBw8qIyNDkydP1vDhwzVx4kTt3LnTbu3333+vd955R9OnT1dERIRGjRqluLg45eTk2K1bt26dwsPDdfHiRb3++usaP368RowYoenTpys3N/em/E4AAAAAAAAAAAAAAAAAaCpNEnSTpH379tnVy8rKdPjwYVvfVceOHVNlZaXLt8CVlJSourpaXbp0cejV1r7++mun31ZVVTVoNkn6+9//rrZt2yowMLBB323evFnJyckaP368lixZom7duikpKUnZ2dm2NW+++abWrFmjAQMGaMmSJXrqqadUWlqq+Ph4XbhwwWHPZ599VpmZmXryySe1cOFCXblyRQkJCbp8+XKDfxcAAAAAAAAAAAAAAAAANBfPxn5YUVGhK1euONSNRqOMRqPt74CAAIWGhiozM1PTpk2z1ffv36/q6mqZzWbl5eU5PcNqtdrOqKysVEFBgVJSUjRkyBA98MADLs1ZWloqSbrjjjscerW12jW19u7dq/fff181NTXy9vbWwIEDNX36dIWHhzs9o6ysTP/617/05z//Wf/4xz/0xz/+0e7/wBU5OTl666231KdPH0nXAoL333+/du/ebTt36NChio6Otvst3bt3V0xMjD7++GNNmDDBbs9z587p7bffVuvWrSVJwcHBio6O1v79+/Xwww83aD4AAAAAAAAAAAAAAAAAaC6NDrpFRUU5rcfGxmrOnDl2tcjISKWmpurcuXO68847JUlZWVkKCgpSYGBgnUG38+fPa+TIkXa13r17a/HixfL0dG302lvZap8UvV5traKiwlZ78MEHNWHCBLVv315VVVX68ssv9be//U2zZ8/WsmXLNHbsWId9oqOjdfr0aXl5eekPf/iDxo0b59Js15s4caIt5CZJnp6euvvuu1VYWGirOQva+fv7S7r2f/Vj8fHxtpCbJPXp00fe3t52ewIAAAAAAAAAAAAAAACAu2t00G358uW20Nr1AgICHGpms1mpqanat2+fpk6dqrKyMn3yySeKjY2t9wx/f3+tWLFCklRdXa0zZ85o586dmjRpkhITEzVmzJgbzlkbiKupqXHoWSwWSfYhuEceecRuzciRIzV16lQ9+eSTWrFihX7xi184hOyWLVumf/3rXzp06JCWL1+u/Px8LVq06IazXW/YsGEOtdatW+vkyZN2taNHj+qDDz7QZ599pqKiItszpLW/5UZ7tmnTRpcuXWrQbAAAAAAAAAAAAAAAAADQnBoddOvbt6+6du3q0tqOHTsqNDRUWVlZmjp1qg4ePKiqqiqZzeZ6vzMajRowYIBd7aGHHlJMTIySkpI0ZMgQp0+SXs/X11fStedFf6y8vNxuTV28vb01ZcoUJSYmqrCwUMHBwXb9fv36qV+/fho7dqy6d++ulStXaujQoYqMjKx33+uZTCaHmsFg0NWrV21/r1u3Tunp6erbt6/uu+8+TZkyRXfddVedgcFWrVo51Fq0aGG3JwAAAAAAAAAAAAAAAAC4uxZNddCYMWOUk5OjCxcuKDMzUz179lRgYGCD9zEYDDKbzSovL1d+fv4N13fq1EkeHh4qKSlx6BUXF0uSOnfufMN92rdvL+nazXL1qX229OjRozfcsyFOnDih9evX65e//KU2btyomTNnaty4cQoJCbmp5wAAAAAAAAAAAAAAAACAu2myoJvZbJbVatWePXt06NAhl54drYvVapV049CZdO02tl69eikvL8+hl5ubK0kKDQ294T7Hjx+XwWDQz372s3rXtWhx7b+0qqrqhns2xJEjR2S1WvXoo4/Kw8PDbi4AAAAAAAAAAAAAAAAA+HfWZEG3Dh06KCwsTJs2bVJlZeUNny2ti8Vi0d69e+Xl5aWwsDCXvhk7dqyOHz+unJwcu3127Nih7t27254izc/Pd3rz29mzZ7Vx40ZFRESodevWkqQXX3xRly9fdlj7t7/9TZI0ePDgBv+2+hiNRknSpUuX7H7Da6+9dlPPAQAAAAAAAAAAAAAAAAB349nYD7Ozs3Xy5EmnvYiICKd1s9ms5ORkl58trays1MGDByVdu8Xt7NmzysjI0PHjxzVv3jy1bdvWpVknT56sXbt2acGCBYqLi5O/v7927NihoqIirVq1ynZDWn5+vlavXq3IyEgNGjRIbdq00cmTJ7Vp0yaZTCY988wztj1zcnL0yCOP6MEHH1RISIg8PDx06NAh/fd//7fuueceRUZGujSbq+699161bNlSSUlJiomJkY+Pj7Zv3+7SrXYAAAAAAAAAAAAAAAAAcDtrdNBt2bJldfays7Od1s1ms1JSUly+za20tFTz58+3/W0ymRQcHKzk5GSNGjXK5VlNJpPS0tK0cuVKpaenq6KiQsHBwVq9erXuuece27pf/epXunr1qv7nf/5HBw4cUEVFhTp27KgHH3xQsbGx8vf3t61dv369/vrXv+rDDz9URkaGqqqq1KVLF82ePVuPP/64DAaDy/O5okuXLkpJSdGaNWuUmpqqVq1ayWw2a/r06Ro3btxNPQsAAAAAAAAAAAAAAAAA3ImH1Wq1NvcQP0VZWZnq+gkGg0Emk6mJJ/pBdXW1Kioq6uz7+PjI07PRWcMm45Fsae4RgDpZn3msuUcA6mfd2dwTAAAAAAAAAAAAAABw23P/lNUNREVFqbi42Glv0KBBSktLa+KJfrBnzx4tXbq0zv7atWsVHh7ehBMBAAAAAAAAAAAAAAAAwO3ntr/RraCgQFVVVU57vr6+CgoKauKJfvDtt9/q1KlTdfaDgoLk6+vbhBM1Dje6wZ1xoxvcHje6AQAAAAAAAAAAAADwk932N7qFhIQ09wh1atu2rdq2bdvcYwAAAAAAAAAAAAAAAADAbe22v9ENt15aWppiYmJkNBqbexQAAAAAAAAAAAAAAAAA/4FaNPcAAAAAAAAAAAAAAAAAAADUh6AbAAAAAAAAAAAAAAAAAMCtEXQDAAAAAAAAAAAAAAAAALg1gm4AAAAAAAAAAAAAAAAAALdG0A0AAAAAAAAAAAAAAAAA4NYIugEAAAAAAAAAAAAAAAAA3BpBNwAAAAAAAAAAAAAAAACAWyPoBgAAAAAAAAAAAAAAAABwawTdAAAAAAAAAAAAAAAAAABuzcNqtVqbewi4N49kS3OPgP9g1mcea+4R8J/MurO5JwAAAAAAAAAAAAAAAOJGNwAAAAAAAAAAAAAAAACAmyPoBgAAAAAAAAAAAAAAAABwawTdAAAAAAAAAAAAAAAAAABujaAbAAAAAAAAAAAAAAAAAMCtEXQDAAAAAAAAAAAAAAAAALg1z4Z+sG7dOqWnp9e7Jjs7WzNmzNCRI0cUEhKijRs31rn23Xff1fPPPy9JysjIUOfOnSXJ9v31/Pz8NHToUCUkJKhjx44NmrusrEyrV6/W/v37VV5erpCQEMXHxyssLKze73bu3KmkpCQNGjRIaWlpTte8/PLLeuedd/Tkk09q5syZDZoLAAAAAAAAAAAAAAAAAFC/Bgfdai1atEjt27evd43BYFBBQYGKi4vVqVMnp2uysrJkMBhUU1Pj0GvTpo0SExMlSRaLRUVFRdq6daumTZumLVu2KCAgwKVZLRaL5s6dqy+//FJxcXHq0KGDtm3bplmzZmn9+vXq16+f0+++++47rV69Wm3atHHa/+abb7RixQp9+OGHLs0BAAAAAAAAAAAAAAAAAGi4RgfdwsPD1bVr13rX9OjRQ1988YWysrIUHR3t0L906ZIOHz6snj176sSJEw79li1bKiIiwq42aNAgxcbGatu2bUpISHBp1l27dikvL08rVqzQ6NGjJUkjR47UpEmT9Morr2jDhg1Ov0tNTVVoaKiuXLnitP/iiy+qsLBQ6enpio2NdWkWAAAAAAAAAAAAAAAAAEDDtLiVm/v5+alfv37Kyspy2j9w4IAsFouGDx/u8p79+/eXj4+PCgsLXf7mvffeU0BAgC3kJkkmk0kTJkxQbm6uTp8+7fBNdna2MjMz9fTTT9e57xNPPKF33nlH/fv3d3kWAAAAAAAAAAAAAAAAAEDD3NKgmySZzWbl5+erpKTEoZeZmamgoCB169bN5f2sVquuXr0qPz8/l9ZbLBbl5+dr4MCBDr3BgwdLkj799FO7enV1tV544QX95je/UZcuXercu0+fPvL0bPSleJKkM2fOKDw8XH/5y1/00Ucf6Te/+Y1GjBihBx98UG+++abDXP/93/+tGTNm6Oc//7kiIiL0+OOP66OPPrJbt2vXLoWHh+vzzz/Xpk2b9Mgjj2j48OGaMmWKw1oAAAAAAAAAAAAAAAAAcHdNEnSTpH379tnVy8rKdPjwYVvfVceOHVNlZaXLt8CVlJSourraaWCttvb111/b1d966y1ZLBb913/9V4Nm+yn27dunhQsX6t5779Vzzz2n8PBwrVmzRrt377atycjI0PLly9WtWzf97ne/0+9//3sZjUY9/fTTOn78uMOeycnJ2rRpk6KiorRo0SK1atVKCxcu1KlTp5rsdwEAAAAAAAAAAAAAAADAT9Xo68gqKip05coVh7rRaJTRaLT9HRAQoNDQUGVmZmratGm2+v79+1VdXS2z2ay8vDynZ1itVtsZlZWVKigoUEpKioYMGaIHHnjApTlLS0slSXfccYdDr7ZWu0aSTp06pT/96U966aWX1LJlS5fOuBn+v//v/1NKSopGjhwpSRo7dqw+//xzZWRk6KGHHpJ07Qa57du3q3PnzrbvwsPDNX78eO3fv1/BwcF2e37++efaunWrAgICJEkjRozQ/fffr/fff18zZ85sol8GAAAAAAAAAAAAAAAAAD9No4NuUVFRTuuxsbGaM2eOXS0yMlKpqak6d+6c7rzzTklSVlaWgoKCFBgYWGfQ7fz587bgV63evXtr8eLFLj8ZWlVVJUny8vJy6NXWKioqbLUXX3xR9957ryIiIlza/2a57777HH5rWFiY9u7da/u7X79+Dt+1bNlSJpNJ58+fd+jFxMTYQm7StWBft27dVFhYeBMnBwAAAAAAAAAAAAAAAIBbq9FBt+XLl9tCa9e7PlhVy2w2KzU1Vfv27dPUqVNVVlamTz75RLGxsfWe4e/vrxUrVkiSqqurdebMGe3cuVOTJk1SYmKixowZc8M5awNxNTU1Dj2LxSLph8Db+++/r5ycHG3fvv2G+95sQ4cOdai1bt1aly9ftqudOHFCe/bsUW5uroqKivTdd99J+uG3XG/YsGEu7QkAAAAAAAAAAAAAAAAA7qzRQbe+ffuqa9euLq3t2LGjQkNDlZWVpalTp+rgwYOqqqqS2Wyu9zuj0agBAwbY1R566CHFxMQoKSlJQ4YMcfok6fV8fX0lSWVlZQ698vJy25rq6mqlpqZq0qRJMplMunjxom1dbYjs4sWLMhgM8vPzu8EvbrhWrVo51Fq0aKGrV6/a/v7b3/6mF198UV27dtXo0aP1yCOP6K677tKiRYuc7mkymZzu6Sz0BwAAAAAAAAAAAAAAAADuqtFBt4YaM2aMXn31VV24cEGZmZnq2bOnAgMDG7yPwWCQ2WzWqlWrlJ+fr/vuu6/e9Z06dZKHh4dKSkocesXFxZKkzp07q6KiQt988402b96szZs3O93LbDard+/e2rp1a4Pn/qm++eYbJScn695771VKSorLT7cCAAAAAAAAAAAAAAAAwO2uydJSZrNZKSkp2rNnjw4dOqSYmJhG72W1WiVde870Rry9vdWrVy/l5eU59HJzcyVJoaGh8vHx0euvv+50j9TUVElSQkKC05vXmsJnn32myspKTZgwwS7k9u233+r8+fPNMhMAAAAAAAAAAAAAAAAANIUmC7p16NBBYWFh2rRpkyorK2/4bGldLBaL9u7dKy8vL4WFhbn0zdixY/Xaa68pJyfH9o3FYtGOHTvUvXt3BQcHy8PDQ/fee6/T72ufKq2r3xSMRqMk6dKlS7aa1WrVypUrm2skAAAAAAAAAAAAAAAAAGgSjQ66ZWdn6+TJk057ERERTutms1nJyckuP1taWVmpgwcPSroW6jp79qwyMjJ0/PhxzZs3T23btnVp1smTJ2vXrl1asGCB4uLi5O/vrx07dqioqEirVq2Sh4eHS/s0p9DQULVr106vv/66vv/+e3Xo0EG7d+/W6dOnZTKZmns8AAAAAAAAAAAAAAAAALhlGh10W7ZsWZ297Oxsp/Xa50tdvc2ttLRU8+fPt/1tMpkUHBys5ORkjRo1yuVZTSaT0tLStHLlSqWnp6uiokLBwcFavXq17rnnHpf3aU6+vr5KTU3VqlWrlJaWJk9PT0VERGjJkiWaMmVKc48HAAAAAAAAAAAAAAAAALeMh9VqtTb3ED9FWVmZ6voJBoOhWW87s1gs+v777+vse3t7254kdWceyZbmHgH/wazPPNbcI+A/mXVnc08AAAAAAAAAAAAAAAD0E250cxdRUVEqLi522hs0aJDS0tKaeKIfHD16VLNmzaqz/9xzz+nhhx9uwokAAAAAAAAAAAAAAAAA4PZz2wfdXnrpJVVVVTnt+fr6NvE09vr06aP169fX2f/Zz37WhNMAAAAAAAAAAAAAAAAAwO3ptn+6FLdeWlqaYmJibotnVgEAAAAAAAAAAAAAAAD8+2nR3AMAAAAAAAAAAAAAAAAAAFAfgm4AAAAAAAAAAAAAAAAAALdG0A0AAAAAAAAAAAAAAAAA4NYIugEAAAAAAAAAAAAAAAAA3BpBNwAAAAAAAAAAAAAAAACAWyPoBgAAAAAAAAAAAAAAAABwawTdAAAAAAAAAAAAAAAAAABujaAbAAAAAAAAAAAAAAAAAMCtEXQDAAAAAAAAAAAAAAAAALg1D6vVam3uIeDePJItzT0C/s1Zn3msuUfAvzvrzuaeAAAAAAAAAAAAAAAA/ATc6AYAAAAAAAAAAAAAAAAAcGsE3QAAAAAAAAAAAAAAAAAAbo2gGwAAAAAAAAAAAAAAAADArRF0AwAAAAAAAAAAAAAAAAC4NYJuAAAAAAAAAAAAAAAAAAC35tnQD9atW6f09PR612RnZ2vGjBk6cuSIQkJCtHHjxjrXvvvuu3r++eclSRkZGercubMk2b6/np+fn4YOHaqEhAR17NixQXOXlZVp9erV2r9/v8rLyxUSEqL4+HiFhYXZrauoqNCGDRu0d+9enT17ViaTSYMGDdLMmTMVFBTksG95ebnmzp2r3NxcrV27VuHh4Q2aCwAAAAAAAAAAAAAAAABQvwYH3WotWrRI7du3r3eNwWBQQUGBiouL1alTJ6drsrKyZDAYVFNT49Br06aNEhMTJUkWi0VFRUXaunWrpk2bpi1btiggIMClWS0Wi+bOnasvv/xScXFx6tChg7Zt26ZZs2Zp/fr16tevnySppqZGv/3tb3XkyBE9+OCDGjZsmIqLi7V582bFxMTozTffVO/evW37Hjt2TH/84x9VWFjo0hwAAAAAAAAAAAAAAAAAgIZrdNAtPDxcXbt2rXdNjx499MUXXygrK0vR0dEO/UuXLunw4cPq2bOnTpw44dBv2bKlIiIi7GqDBg1SbGystm3bpoSEBJdm3bVrl/Ly8rRixQqNHj1akjRy5EhNmjRJr7zyijZs2CBJOnDggI4cOaKYmBjFx8fbvr/vvvv0+OOPa82aNXr11Vdt9Tlz5igkJES/+c1v9Ic//MGlWQAAAAAAAAAAAAAAAAAADdPiVm7u5+enfv36KSsry2n/wIEDslgsGj58uMt79u/fXz4+Pg26Re29995TQECALeQmSSaTSRMmTFBubq5Onz4tSbanUidOnGj3fa9evXT33Xfr008/tauvXr1ar7322g1vtgMAAAAAAAAAAAAAAAAANN4tDbpJktlsVn5+vkpKShx6mZmZCgoKUrdu3Vzez2q16urVq/Lz83NpvcViUX5+vgYOHOjQGzx4sCQ5BNi8vb0d1ppMJoda7ZOnP8WuXbsUHh6uzz//XJs2bdIjjzyi4cOHa8qUKfroo4/s1l68eFEbNmxQVFSURowYocjISNuTrNdLTExUZGSkLl++rBdeeEFjxozRiBEjNHv2bJ08efInzwwAAAAAAAAAAAAAAAAATalJgm6StG/fPrt6WVmZDh8+bOu76tixY6qsrHT5FriSkhJVV1erS5cuDr3a2tdffy1JGjBggCQ5DZjl5eU5DcvdLMnJydq0aZOioqK0aNEitWrVSgsXLtSpU6dsa15++WW98847+vnPf66lS5dq9uzZKiws1Jw5c1RVVWW3n8Vi0ezZs1VQUKB58+Zp/vz5+uqrrzR//nxZLJZb9jsAAAAAAAAAAAAAAAAA4GbzbOyHFRUVunLlikPdaDTKaDTa/g4ICFBoaKgyMzM1bdo0W33//v2qrq6W2WxWXl6e0zOsVqvtjMrKShUUFCglJUVDhgzRAw884NKcpaWlkqQ77rjDoVdbq10TGRmp0aNH69VXX5WHh4dGjBihM2fOKDU1VV5eXpo3b55LZzbG559/rq1btyogIECSNGLECN1///16//33NXPmTEnS+PHjtXjxYvn4+Ni+8/Pz06JFi5Sbm6vw8HBbvby8XC1bttQbb7yhli1bSpLatWunZ555RkePHrVbCwAAAAAAAAAAAAAAAADurNFBt6ioKKf12NhYzZkzx64WGRmp1NRUnTt3TnfeeackKSsrS0FBQQoMDKwz6Hb+/HmNHDnSrta7d28tXrxYnp6ujV5705mXl5dDr7ZWUVEhSfLw8NCLL76o1NRUPf/887Z1PXr00Ntvv20Lod0KMTExdvvfcccd6tatmwoLC221++67z+G7tm3bSrr2f/VjCxYssIXcpB9urCssLCToBgAAAAAAAAAAAAAAAOC20eig2/Lly22htes5C4OZzWalpqZq3759mjp1qsrKyvTJJ58oNja23jP8/f21YsUKSVJ1dbXOnDmjnTt3atKkSUpMTNSYMWNuOGdtIK6mpsahV/uEZ23gzWKxaOnSpfrwww/1xBNPqH///jp//ry2bNmimTNnatWqVerWrdsNz2yMYcOGOdRat26ty5cv29X+93//Vx9++KGOHTumU6dO2W68+/FzpG3atFFISIjDfpIc9gQAAAAAAAAAAAAAAAAAd9booFvfvn3VtWtXl9Z27NhRoaGhysrK0tSpU3Xw4EFVVVXJbDbX+53RaLTdQlbroYceUkxMjJKSkjRkyBCnT5Jez9fXV5JUVlbm0CsvL7db85e//EXvv/++Vq1apeHDh9vWjR07VlFRUVq0aJE2b958o5/bKCaTyaHWokULu4BeYmKidu/ercGDB+sXv/iFunTpIi8vLz399NMu7WcwGCQ5D/0BAAAAAAAAAAAAAAAAgLtq0VQHjRkzRjk5Obpw4YIyMzPVs2dPBQYGNngfg8Egs9ms8vJy5efn33B9p06d5OHhoZKSEodecXGxJKlz586SpH379umuu+6yC7lJ10Jj48eP17Fjx3T69OkGz3wzHDhwQLt379asWbO0bt06xcbGauzYserRo0ezzAMAAAAAAAAAAAAAAAAATaXJgm5ms1lWq1V79uzRoUOHXHp2tC5Wq1XStedMb8Tb21u9evVSXl6eQy83N1eSFBoaKkm6ePGiWrRw/l9Sexvad99916iZf6ojR45Ikh577DG7+okTJ5pjHAAAAAAAAAAAAAAAAABoMk0WdOvQoYPCwsK0adMmVVZW3vDZ0rpYLBbt3btXXl5eCgsLc+mbsWPH6vjx48rJybHbZ8eOHerevbuCg4MlSb169dLp06eVnZ1t931FRYXef/99eXl5qXv37o2a+6cyGo2SpNLSUlutvLxcaWlpzTIPAAAAAAAAAAAAAAAAADQVz8Z+mJ2drZMnTzrtRUREOK2bzWYlJye7/GxpZWWlDh48KOnaLW5nz55VRkaGjh8/rnnz5qlt27YuzTp58mTt2rVLCxYsUFxcnPz9/bVjxw4VFRVp1apV8vDwkCQ9+eST+sc//qGEhARNnjxZffv21TfffKO//vWvKioq0qxZs+Tr6+vSmTdbRESENm7cqGeffVbR0dGqqanRpk2b5O3t3SzzAAAAAAAAAAAAAAAAAEBTaXTQbdmyZXX2fnwjWi2z2ayUlBSXb3MrLS3V/PnzbX+bTCYFBwcrOTlZo0aNcnlWk8mktLQ0rVy5Uunp6aqoqFBwcLBWr16te+65x7auZ8+e2rhxo958803t2bNHW7Zskbe3t4KDgzVjxgyNHTvW5TNvtrCwMCUlJelPf/qTXnjhBbVr104PPvigRo4cqccff7zZ5gIAAAAAAAAAAAAAAACAW83DarVam3uIn6KsrEx1/QSDwSCTydTEE/2gsrJSVVVVdfZbtWqlFi2a7PXYRvNItjT3CPg3Z33mseYeAf/urDubewIAAAAAAAAAAAAAAPATNPpGN3cRFRWl4uJip71BgwYpLS2tiSf6wVtvvaX09PQ6+xkZGercuXMTTgQAAAAAAAAAAAAAAAAAt5/bPuj20ksv1Xlrmq+vbxNPY++RRx7RvffeW2e/ffv2TTgNAAAAAAAAAAAAAAAAANyebvunS3HrpaWlKSYmRkajsblHAQAAAAAAAAAAAAAAAPAfqEVzDwAAAAAAAAAAAAAAAAAAQH0IugEAAAAAAAAAAAAAAAAA3BpBNwAAAAAAAAAAAAAAAACAWyPoBgAAAAAAAAAAAAAAAABwawTdAAAAAAAAAAAAAAAAAABujaAbAAAAAAAAAAAAAAAAAMCtEXQDAAAAAAAAAAAAAAAAALg1gm4AAAAAAAAAAAAAAAAAALdG0A0AAAAAAAAAAAAAAAAA4NY8rFartbmHgHvzSLY09wj4N2R95rHmHgH/bqw7m3sCAAAAAAAAAAAAAABwi3CjGwAAAAAAAAAAAAAAAADArRF0AwAAAAAAAAAAAAAAAAC4NYJuAAAAAAAAAAAAAAAAAAC3RtANAAAAAAAAAAAAAAAAAODWCLoBAAAAAAAAAAAAAAAAANyaZ0M/WLdundLT0+tdk52drRkzZujIkSMKCQnRxo0b61z77rvv6vnnn5ckZWRkqHPnzpJk+/56fn5+Gjp0qBISEtSxY8cGzV1WVqbVq1dr//79Ki8vV0hIiOLj4xUWFuZ0/T//+U+tX79eubm5+v7779WpUyf96le/0tSpU21rCgsLtXbtWh05ckRXrlxRx44dNXbsWMXGxsrb27tB8wEAAAAAAAAAAAAAAAAAnGtw0K3WokWL1L59+3rXGAwGFRQUqLi4WJ06dXK6JisrSwaDQTU1NQ69Nm3aKDExUZJksVhUVFSkrVu3atq0adqyZYsCAgJcmtVisWju3Ln68ssvFRcXpw4dOmjbtm2aNWuW1q9fr379+tmt379/v373u9+pX79+io+Pl7e3t/75z3/q6NGjtqDbiRMn9MQTT8jLy0vR0dHq1KmT/vGPf2jDhg06evSo1qxZI4PB4NJ8AAAAAAAAAAAAAAAAAIC6NTroFh4erq5du9a7pkePHvriiy+UlZWl6Ohoh/6lS5d0+PBh9ezZUydOnHDot2zZUhEREXa1QYMGKTY2Vtu2bVNCQoJLs+7atUt5eXlasWKFRo8eLUkaOXKkJk2apFdeeUUbNmywrT1z5oyWLFmiCRMm6P/9v/8nDw8PSdIDDzwgq9VqW7dmzRpZLBa99dZb6tmzp21Nu3bttHHjRh04cEC/+MUvXJoPAAAAAAAAAAAAAAAAAFC3Frdycz8/P/Xr109ZWVlO+wcOHJDFYtHw4cNd3rN///7y8fFRYWGhy9+89957CggIsIXcJMlkMmnChAnKzc3V6dOnbfU//elPateunZ555hlbyK3W9X8fOXJEoaGhtpBbrUcffVSS9Omnn7o8HwAAAAAAAAAAAAAAAACgbrc06CZJZrNZ+fn5KikpcehlZmYqKChI3bp1c3k/q9Wqq1evys/Pz6X1FotF+fn5GjhwoENv8ODBkn4IpVksFu3du1eTJ0+Wp6enrVYXb29vh5rJZHJprlrZ2dkKDw/XwYMHlZGRocmTJ2v48OGaOHGidu7cabf2+++/1zvvvKPp06crIiJCo0aNUlxcnHJycuzWrVu3TuHh4bp48aJef/11jR8/XiNGjND06dOVm5vboPkAAAAAAAAAAAAAAAAAoLk1SdBNkvbt22dXLysr0+HDh219Vx07dkyVlZUu3wJXUlKi6upqdenSxaFXW/v6668lSQUFBSovL1d4eLjWrl2rsWPHaujQoXr00UeVkZFh9+3AgQOVl5enixcv2tUPHjwoSRowYECDftfmzZuVnJys8ePHa8mSJerWrZuSkpKUnZ1tW/Pmm29qzZo1GjBggJYsWaKnnnpKpaWlio+P14ULFxz2fPbZZ5WZmaknn3xSCxcu1JUrV5SQkKDLly83aDYAAAAAAAAAAAAAAAAAaE6ejf2woqJCV65ccagbjUYZjUbb3wEBAQoNDVVmZqamTZtmq+/fv1/V1dUym83Ky8tzeobVarWdUVlZqYKCAqWkpGjIkCF64IEHXJqztLRUknTHHXc49GprtWu++uorSdLWrVv1r3/9S7/73e9kNBq1efNm/fGPf1R1dbV+9atfSZISEhI0c+ZMJSQk6KmnnlLnzp318ccfKyUlRaNHj9YvfvELl+arlZOTo7feekt9+vSRdC0geP/992v37t0KDw+XJA0dOlTR0dF2v6V79+6KiYnRxx9/rAkTJtjtee7cOb399ttq3bq1JCk4OFjR0dHav3+/Hn744QbNBwAAAAAAAAAAAAAAAADNpdFBt6ioKKf12NhYzZkzx64WGRmp1NRUnTt3TnfeeackKSsrS0FBQQoMDKwz6Hb+/HmNHDnSrta7d28tXrzY9rTojVRVVUmSvLy8HHq1tYqKCknSd999J0nKy8vTn//8Z1tg795779Wvf/1rvfHGG5owYYKMRqMCAwP19ttv67e//a1iY2Nte0ZFRSkhIUEeHh4uzVdr4sSJtpCbJHl6euruu+9WYWGhrVYbeLuev7+/pGv/Vz8WHx9vC7lJUp8+feTt7W23JwAAAAAAAAAAAAAAAAC4u0YH3ZYvX24LrV0vICDAoWY2m5Wamqp9+/Zp6tSpKisr0yeffGIXEHPG399fK1askCRVV1frzJkz2rlzpyZNmqTExESNGTPmhnPWBuJqamocehaLRdIPgberV69Kkh577DG7W+mMRqMefvhhrVy5Uvn5+Ro4cKBOnjyp3/72t/L29tYf/vAHdejQQTk5OdqyZYsuXbrUoDCeJA0bNsyh1rp1a508edKudvToUX3wwQf67LPPVFRUZHuGtPa33GjPNm3a6NKlSy7PBQAAAAAAAAAAAAAAAADNrdFBt759+6pr164ure3YsaNCQ0OVlZWlqVOn6uDBg6qqqpLZbK73O6PRqAEDBtjVHnroIcXExCgpKUlDhgxx+iTp9Xx9fSVJZWVlDr3y8nK7NbW3n4WGhjqs7datmyTpzJkzGjhwoH7/+9/LYDDorbfekslkkiSNGDFCYWFhmjdvnvr27aupU6fWO9v1ave4nsFgsIXvJGndunVKT09X3759dd9992nKlCm666676gwMtmrVyqHWokULuz0BAAAAAAAAAAAAAAAAwN21aKqDxowZo5ycHF24cEGZmZnq2bOnAgMDG7yPwWCQ2WxWeXm58vPzb7i+U6dO8vDwUElJiUOvuLhYktS5c2dJUtu2bSXJ6U1s1dXVkq6F777++mudOHFC48ePdwiojRgxQp06dVJmZmbDftgNnDhxQuvXr9cvf/lLbdy4UTNnztS4ceMUEhJyU88BAAAAAAAAAAAAAAAAAHfTZEE3s9ksq9WqPXv26NChQy49O1oXq9Uq6YfwWX28vb3Vq1cv5eXlOfRyc3Ml/XCDW9++fSVJx48fd1h77NgxSVKPHj108eJFSdduR3OmRYsW+u677244W0McOXJEVqtVjz76qDw8PGx1Z7MCAAAAAAAAAAAAAAAAwL+TJgu6dejQQWFhYdq0aZMqKytv+GxpXSwWi/bu3SsvLy+FhYW59M3YsWN1/Phx5eTk2O2zY8cOde/eXcHBwZKu3f4WFhamzZs36/vvv7et/fbbb5WRkaGgoCAFBQUpMDBQRqNR//M//6OKigq7sw4fPqx//etftj1vFqPRKEm6dOmS3W947bXXbuo5AAAAAAAAAAAAAAAAAOBuHN/odFF2drZOnjzptBcREeG0bjablZyc7PKzpZWVlTp48KCka7e4nT17VhkZGTp+/LjmzZtne2r0RiZPnqxdu3ZpwYIFiouLk7+/v3bs2KGioiKtWrXK7oa0p59+WnFxcXryyScVFRUli8WijRs3qqysTC+//LIkyc/PT0888YTWrl2rxx9/XI899pjatm2rgoICbd++Xa1atVJcXJxLs7nq3nvvVcuWLZWUlKSYmBj5+Pho+/btLt1qBwAAAAAAAAAAAAAAAAC3s0YH3ZYtW1ZnLzs722ndbDYrJSXF5dvcSktLNX/+fNvfJpNJwcHBSk5O1qhRo1ye1WQyKS0tTStXrlR6eroqKioUHBys1atX65577rFb27dvX6Wlpen111/Xyy+/LKvVqgEDBigpKcn2tKkkxcXF6Wc/+5n++te/6o033lBFRYXatWunyMhIxcbGqlu3bi7P54ouXbooJSVFa9asUWpqqlq1aiWz2azp06dr3LhxN/UsAAAAAAAAAAAAAAAAAHAnHlar1drcQ/wUZWVlqusnGAwGmUymJp7oB9XV1Q5Pm17Px8dHnp6Nzho2GY9kS3OPgH9D1mcea+4R8O/GurO5JwAAAAAAAAAAAAAAALeI+6esbiAqKkrFxcVOe4MGDVJaWloTT/SDPXv2aOnSpXX2165dq/Dw8CacCAAAAAAAAAAAAAAAAABuP7f9jW4FBQWqqqpy2vP19VVQUFATT/SDb7/9VqdOnaqzHxQUJF9f3yacqHG40Q23Aje64abjRjcAAAAAAAAAAAAAAP5t3fZBN9x6aWlpiomJkdFobO5RAAAAAAAAAAAAAAAAAPwHatHcAwAAAAAAAAAAAAAAAAAAUB+CbgAAAAAAAAAAAAAAAAAAt0bQDQAAAAAAAAAAAAAAAADg1gi6AQAAAAAAAAAAAAAAAADcGkE3AAAAAAAAAAAAAAAAAIBbI+gGAAAAAAAAAAAAAAAAAHBrBN0AAAAAAAAAAAAAAAAAAG6NoBsAAAAAAAAAAAAAAAAAwK0RdAMAAAAAAAAAAAAAAAAAuDWCbgAAAAAAAAAAAAAAAAAAt+ZhtVqtzT0E3JtHsqW5R8BtzvrMY809Av4dWHc29wQAAAAAAAAAAAAAAKCZcKMbAAAAAAAAAAAAAAAAAMCtEXQDAAAAAAAAAAAAAAAAALg1gm4AAAAAAAAAAAAAAAAAALdG0A0AAAAAAAAAAAAAAAAA4NY8G/rBunXrlJ6eXu+a7OxszZgxQ0eOHFFISIg2btxY59p3331Xzz//vCQpIyNDnTt3liTb99fz8/PT0KFDlZCQoI4dOzZ0dJWXl2vu3LnKzc3V2rVrFR4e7nTd3//+d6WlpamwsFBt2rTRuHHjNHPmTHl5ednWXL16Vbt379bu3bt14sQJVVVVKTAwUI8++qh+9atfycPDo8HzAQAAAAAAAAAAAAAAAAAcNTjoVmvRokVq3759vWsMBoMKCgpUXFysTp06OV2TlZUlg8Ggmpoah16bNm2UmJgoSbJYLCoqKtLWrVs1bdo0bdmyRQEBAS7Pe+zYMf3xj39UYWFhvesOHDigp59+WsOGDdNzzz2nr776Sn/605906tQprVixwrZu/fr12rBhg8aMGaMJEyaoRYsW2rt3r1588UWdPXtW8fHxLs8GAAAAAAAAAAAAAAAAAKhbo4Nu4eHh6tq1a71revTooS+++EJZWVmKjo526F+6dEmHDx9Wz549deLECYd+y5YtFRERYVcbNGiQYmNjtW3bNiUkJLg875w5cxQSEqLf/OY3+sMf/uB0jcViUXJysoKDg/Xqq6/KYDBIklq3bq1XXnlFhw4d0tChQyVJAQEB2rp1q3r06GH7fvz48Zo5c6Y2b96s6dOny9fX1+X5AAAAAAAAAAAAAAAAAADOtbiVm/v5+alfv37Kyspy2j9w4IAsFouGDx/u8p79+/eXj4/PDW9m+7HVq1frtddeq/cWuk8//VTFxcWaNGmSLeQmSRMnTpS3t7fee+89W23ChAl2IbdaERERqq6ubvB8AAAAAAAAAAAAAAAAAADnbmnQTZLMZrPy8/NVUlLi0MvMzFRQUJC6devm8n5Wq1VXr16Vn59fg+bo16/fDdfk5ORIkgYPHmxX9/b2VkhIiD799NMb7uHl5SVJMplMLs2VnZ2t8PBwHTx4UBkZGZo8ebKGDx+uiRMnaufOnXZrv//+e73zzjuaPn26IiIiNGrUKMXFxdnmrrVu3TqFh4fr4sWLev311zV+/HiNGDFC06dPV25urktzAQAAAAAAAAAAAAAAAIC7aJKgmyTt27fPrl5WVqbDhw/b+q46duyYKisrG3QLnKtOnTolg8GgTp06OfTuuusunT17VtXV1fXu8fe//11t27ZVYGBgg87evHmzkpOTNX78eC1ZskTdunVTUlKSsrOzbWvefPNNrVmzRgMGDNCSJUv01FNPqbS0VPHx8bpw4YLDns8++6wyMzP15JNPauHChbpy5YoSEhJ0+fLlBs0GAAAAAAAAAAAAAAAAAM3Js7EfVlRU6MqVKw51o9Eoo9Fo+zsgIEChoaHKzMzUtGnTbPX9+/erurpaZrNZeXl5Ts+wWq22MyorK1VQUKCUlBQNGTJEDzzwQGNHr1Npaal8fX3tni2t5e/vr6tXr+rSpUtq166dXa+srEz/+te/9Oc//1n/+Mc/9Mc//tHu/8AVOTk5euutt9SnTx9J1wKC999/v3bv3q3w8HBJ0tChQxUdHa077rjD9l337t0VExOjjz/+WBMmTLDb89y5c3r77bfVunVrSVJwcLCio6O1f/9+Pfzwww2aDwAAAAAAAAAAAAAAAACaS6ODblFRUU7rsbGxmjNnjl0tMjJSqampOnfunO68805JUlZWloKCghQYGFhn0O38+fMaOXKkXa13795avHixPD0bPXqdqqqqbE+P/lhtvaKiwqEXHR2t06dPy8vLS3/4wx80bty4Bp89ceJEW8hNkjw9PXX33XersLDQVqsNvF3P399f0rX/qx+Lj4+3hdwkqU+fPvL29rbbEwAAAAAAAAAAAAAAAADcXaPTYsuXL7eF1q4XEBDgUDObzUpNTdW+ffs0depUlZWV6ZNPPlFsbGy9Z/j7+2vFihWSpOrqap05c0Y7d+7UpEmTlJiYqDFjxjR2fKc8PT1VU1PjtGexWCTJaRBu2bJl+te//qVDhw5p+fLlys/P16JFixp09rBhwxxqrVu31smTJ+1qR48e1QcffKDPPvtMRUVFtmdIa+e70Z5t2rTRpUuXGjQbAAAAAAAAAAAAAAAAADSnRgfd+vbtq65du7q0tmPHjgoNDVVWVpamTp2qgwcPqqqqSmazud7vjEajBgwYYFd76KGHFBMTo6SkJA0ZMsTuGc+fytfXV+Xl5U57tfVWrVo59Pr166d+/fpp7Nix6t69u1auXKmhQ4cqMjLS5bNNJpNDzWAw6OrVq7a/161bp/T0dPXt21f33XefpkyZorvuuqvOwKCzWVu0aGG3JwAAAAAAAAAAAAAAAAC4uxZNddCYMWOUk5OjCxcuKDMzUz179lRgYGCD9zEYDDKbzSovL1d+fv5NnbFz586qrKzUt99+69ArLi6Wv7+/00Da9WqfLT169OhNne3EiRNav369fvnLX2rjxo2aOXOmxo0bp5CQkJt6DgAAAAAAAAAAAAAAAAC4myYLupnNZlmtVu3Zs0eHDh36Sc+OWq1WSdeeM72ZQkNDJUl5eXl29ZqaGn322We2fn1atLj2X1pVVXVTZzty5IisVqseffRReXh42OrHjx+/qecAAAAAAAAAAAAAAAAAgLtpsqBbhw4dFBYWpk2bNqmysvKGz5bWxWKxaO/evfLy8lJYWNhNnXHo0KFq06aNtm/fblffu3evvvvuOz3wwAO22osvvqjLly877PG3v/1NkjR48OCbOpvRaJQkXbp0yVazWCx67bXXbuo5AAAAAAAAAAAAAAAAAOBuPBv7YXZ2tk6ePOm0FxER4bRuNpuVnJzs8rOllZWVOnjwoKRrt7idPXtWGRkZOn78uObNm6e2bds2dnynvL29NXfuXCUlJWnhwoUaO3asTp06pQ0bNig8PNwunJeTk6NHHnlEDz74oEJCQuTh4aFDhw7pv//7v3XPPfcoMjLyps527733qmXLlkpKSlJMTIx8fHy0ffv2m36rHQAAAAAAAAAAAAAAAAC4m0YH3ZYtW1ZnLzs722ndbDYrJSXF5dvcSktLNX/+fNvfJpNJwcHBSk5O1qhRoxo0r6t++ctfqmXLltq4caOWLFmi1q1ba+LEiZozZ47tWVJJWr9+vf7617/qww8/VEZGhqqqqtSlSxfNnj1bjz/+uAwGw02dq0uXLkpJSdGaNWuUmpqqVq1ayWw2a/r06Ro3btxNPQsAAAAAAAAAAAAAAAAA3ImH1Wq1NvcQP0VZWZnq+gkGg0Emk6mJJ/pBdXW1Kioq6uz7+PjI07PRWcMm45Fsae4RcJuzPvNYc4+AfwfWnc09AQAAAAAAAAAAAAAAaCbun7K6gaioKBUXFzvtDRo0SGlpaU080Q/27NmjpUuX1tlfu3atwsPDm3AiAAAAAAAAAAAAAAAAALj93PY3uhUUFKiqqsppz9fXV0FBQU080Q++/fZbnTp1qs5+UFCQfH19m3CixuFGN/xU3OiGm4Ib3QAAAAAAAAAAAAAA+I9129/oFhIS0twj1Klt27Zq27Ztc48BAAAAAAAAAAAAAAAAALe12/5GN9x6aWlpiomJkdFobO5RAAAAAAAAAAAAAAAAAPwHatHcAwAAAAAAAAAAAAAAAAAAUB+CbgAAAAAAAAAAAAAAAAAAt0bQDQAAAAAAAAAAAAAAAADg1gi6AQAAAAAAAAAAAAAAAADcGkE3AAAAAAAAAAAAAAAAAIBbI+gGAAAAAAAAAAAAAAAAAHBrBN0AAAAAAAAAAAAAAAAAAG6NoBsAAAAAAAAAAAAAAAAAwK0RdAMAAAAAAAAAAAAAAAAAuDUPq9Vqbe4h4N48ki3NPQJuQ9ZnHmvuEXC7su5s7gkAAAAAAAAAAAAAAICb4UY3AAAAAAAAAAAAAAAAAIBbI+gGAAAAAAAAAAAAAAAAAHBrBN0AAAAAAAAAAAAAAAAAAG6NoBsAAAAAAAAAAAAAAAAAwK0RdAMAAAAAAAAAAAAAAAAAuDXPhn6wbt06paen17smOztbM2bM0JEjRxQSEqKNGzfWufbdd9/V888/L0nKyMhQ586dJcn2/fX8/Pw0dOhQJSQkqGPHjg2au6ysTKtXr9b+/ftVXl6ukJAQxcfHKywszG5dVVWVNm7cqPfff1/FxcW644471LdvX82ZM0c9e/a0W1tQUKD09HTl5OSourpad911lx566CFNnTpVnp4N/q8FAAAAAAAAAAAAAAAAADjR6DTWokWL1L59+3rXGAwGFRQUqLi4WJ06dXK6JisrSwaDQTU1NQ69Nm3aKDExUZJksVhUVFSkrVu3atq0adqyZYsCAgJcmtVisWju3Ln68ssvFRcXpw4dOmjbtm2aNWuW1q9fr379+kmSqqurFRUVpe+++06PPPKIQkJC9M0332jz5s2aPn26Nm/erMDAQEnXAnrLly/XgAEDFB8fL19fXx06dEipqan66quvtHjxYpdmAwAAAAAAAAAAAAAAAADUr9FBt/DwcHXt2rXeNT169NAXX3yhrKwsRUdHO/QvXbqkw4cPq2fPnjpx4oRDv2XLloqIiLCrDRo0SLGxsdq2bZsSEhJcmnXXrl3Ky8vTihUrNHr0aEnSyJEjNWnSJL3yyivasGGDpGuBuJ/97Gd68803dccdd9i+Hzx4sKKiorRjxw499dRTkqTz589r2bJlMpvNtnX333+/ysrKtGvXLs2bN0+tW7d2aT4AAAAAAAAAAAAAAAAAQN1a3MrN/fz81K9fP2VlZTntHzhwQBaLRcOHD3d5z/79+8vHx0eFhYUuf/Pee+8pICDAFnKTJJPJpAkTJig3N1enT5+WJPn4+OjVV1+1C7lJUq9eveTr66uTJ0/aanFxcXYht1oDBw7U1atXderUKZfnAwAAAAAAAAAAAAAAAADU7ZYG3STJbDYrPz9fJSUlDr3MzEwFBQWpW7duLu9ntVp19epV+fn5ubTeYrEoPz9fAwcOdOgNHjxYkvTpp5/ecI/Kykq1atXqhueVl5dLknx9fV2aLzs7W+Hh4Tp48KAyMjI0efJkDR8+XBMnTtTOnTvt1n7//fd65513NH36dEVERGjUqFGKi4tTTk6O3bp169YpPDxcFy9e1Ouvv67x48drxIgRmj59unJzc12aCwAAAAAAAAAAAAAAAADcRZME3SRp3759dvWysjIdPnzY6a1o9Tl27JgqKytdvgWupKRE1dXV6tKli0Ovtvb111/Xu0dmZqaqq6ttwbi61NTU6H/+53/UoUMHp+fVZ/PmzUpOTtb48eO1ZMkSdevWTUlJScrOzratefPNN7VmzRoNGDBAS5Ys0VNPPaXS0lLFx8frwoULDns+++yzyszM1JNPPqmFCxfqypUrSkhI0OXLlxs0GwAAAAAAAAAAAAAAAAA0J8/GflhRUaErV6441I1Go4xGo+3vgIAAhYaGKjMzU9OmTbPV9+/fr+rqapnNZuXl5Tk9w2q12s6orKxUQUGBUlJSNGTIED3wwAMuzVlaWipJDs+RXl+rXeNMeXm5Vq9erTvvvFMPPfRQvWdt27ZNJ0+e1IIFC+Tp2bD/2pycHL311lvq06ePpGsBwfvvv1+7d+9WeHi4JGno0KGKjo62+y3du3dXTEyMPv74Y02YMMFuz3Pnzuntt99W69atJUnBwcGKjo7W/v379fDDDzdoPgAAAAAAAAAAAAAAAABoLo0OukVFRTmtx8bGas6cOXa1yMhIpaam6ty5c7rzzjslSVlZWQoKClJgYGCdQbfz589r5MiRdrXevXtr8eLFLgfJqqqqJEleXl4OvdpaRUVFnd+/9NJLOnv2rFauXClvb+8613355Zd644031L9/f02ZMsWl2a43ceJEW8hNkjw9PXX33XersLDQVqsNvF3P399f0rX/qx+Lj4+3hdwkqU+fPvL29rbbEwAAAAAAAAAAAAAAAADcXaODbsuXL7eF1q4XEBDgUDObzUpNTdW+ffs0depUlZWV6ZNPPlFsbGy9Z/j7+2vFihWSpOrqap05c0Y7d+7UpEmTlJiYqDFjxtxwztpAXE1NjUPPYrFIch6Ck6T33ntP7733nh5//HGNGDGizjMqKir0+9//Xt7e3lq+fLlatGj4i7DDhg1zqLVu3VonT560qx09elQffPCBPvvsMxUVFdmeIa39LTfas02bNrp06VKD5wMAAAAAAAAAAAAAAACA5tLooFvfvn3VtWtXl9Z27NhRoaGhysrK0tSpU3Xw4EFVVVXJbDbX+53RaNSAAQPsag899JBiYmKUlJSkIUOGOH2S9Hq+vr6SpLKyModeeXm53Zrr/fOf/9SyZcs0ePBgxcfH13vG0qVL9dVXXyk1NdVp0M8VJpPJoWYwGHT16lXb3+vWrVN6err69u2r++67T1OmTNFdd91VZ2CwVatWDrUWLVrY7QkAAAAAAAAAAAAAAAAA7q7hV4810pgxY5STk6MLFy4oMzNTPXv2VGBgYIP3MRgMMpvNKi8vV35+/g3Xd+rUSR4eHiopKXHoFRcXS5I6d+5sV//222/19NNPq127dnrppZfqfSZ1w4YN+uCDDzR37lynN6jdLCdOnND69ev1y1/+Uhs3btTMmTM1btw4hYSE3LIzAQAAAAAAAAAAAAAAAMAdNFnQzWw2y2q1as+ePTp06JBLz47WxWq1Srr2nOmNeHt7q1evXsrLy3Po5ebmSpJCQ0NtNYvFooULF+rSpUt65ZVX6r0x7uDBg1q7dq3Gjx+v6OjoBv6Khjly5IisVqseffRReXh42OrHjx+/pecCAAAAAAAAAAAAAAAAQHNrsqBbhw4dFBYWpk2bNqmysvKGz5bWxWKxaO/evfLy8lJYWJhL34wdO1bHjx9XTk6O3T47duxQ9+7dFRwcbKu/9NJLysnJ0XPPPadevXrVuefJkyf1hz/8QX369NGiRYsa9Vsawmg0SpIuXbpkq1ksFr322mu3/GwAAAAAAAAAAAAAAAAAaE51v8l5A9nZ2Tp58qTTXkREhNO62WxWcnKyy8+WVlZW6uDBg5Ku3eJ29uxZZWRk6Pjx45o3b57atm3r0qyTJ0/Wrl27tGDBAsXFxcnf3187duxQUVGRVq1aZbshbc+ePfrb3/6mIUOGqGXLlraza3l7e2vIkCGSpKeeekoVFRV65JFHdPjwYYcze/furY4dO7o0nyvuvfdetWzZUklJSYqJiZGPj4+2b9/u0q12AAAAAAAAAAAAAAAAAHA7a3TQbdmyZXX2srOzndbNZrNSUlJcvs2ttLRU8+fPt/1tMpkUHBys5ORkjRo1yuVZTSaT0tLStHLlSqWnp6uiokLBwcFavXq17rnnHtu6oqIiSdL//d//6f/+7/8c9unUqZN27dolSTp16pQk6YUXXnB65nPPPaeHH37Y5RlvpEuXLkpJSdGaNWuUmpqqVq1ayWw2a/r06Ro3btxNOwcAAAAAAAAAAAAAAAAA3I2H1Wq1NvcQP0VZWZnq+gkGg0Emk6mJJ/pBdXW1Kioq6uz7+PjI07PRWcMm45Fsae4RcBuyPvNYc4+A25V1Z3NPAAAAAAAAAAAAAAAA3Iz7p6xuICoqSsXFxU57gwYNUlpaWhNP9IM9e/Zo6dKldfbXrl2r8PDwJpwIAAAAAAAAAAAAAAAAAG4/t/2NbgUFBaqqqnLa8/X1VVBQUBNP9INvv/3W9sSpM0FBQfL19W3CiRqHG93QGNzohkbjRjcAAAAAAAAAAAAAAPAjt33QDbdeWlqaYmJiZDQam3sUAAAAAAAAAAAAAAAAAP+BWjT3AAAAAAAAAAAAAAAAAAAA1IegGwAAAAAAAAAAAAAAAADArRF0AwAAAAAAAAAAAAAAAAC4NYJuAAAAAAAAAAAAAAAAAAC3RtANAAAAAAAAAAAAAAAAAODWCLoBAAAAAAAAAAAAAAAAANwaQTcAAAAAAAAAAAAAAAAAgFsj6AYAAAAAAAAAAAAAAAAAcGsE3QAAAAAAAAAAAAAAAAAAbs3DarVam3sIuDePZEtzjwA3Z33mseYeAbcD687mngAAAAAAAAAAAAAAANymuNENAAAAAAAAAAAAAAAAAODWCLoBAAAAAAAAAAAAAAAAANwaQTcAAAAAAAAAAAAAAAAAgFsj6AYAAAAAAAAAAAAAAAAAcGsE3QAAAAAAAAAAAAAAAAAAbs2zoR+sW7dO6enp9a7Jzs7WjBkzdOTIEYWEhGjjxo11rn333Xf1/PPPS5IyMjLUuXNnSbJ9fz0/Pz8NHTpUCQkJ6tixY4PmLisr0+rVq7V//36Vl5crJCRE8fHxCgsLs1tXUVGhDRs2aO/evTp79qxMJpMGDRqkmTNnKigoyG7toUOH9NZbb+mf//ynJOlnP/uZpkyZogcffFAeHh4Nmg8AAAAAAAAAAAAAAAAA4FyDg261Fi1apPbt29e7xmAwqKCgQMXFxerUqZPTNVlZWTIYDKqpqXHotWnTRomJiZIki8WioqIibd26VdOmTdOWLVsUEBDg0qwWi0Vz587Vl19+qbi4OHXo0EHbtm3TrFmztH79evXr10+SVFNTo9/+9rc6cuSIHnzwQQ0bNkzFxcXavHmzYmJi9Oabb6p3796SpLS0NKWnp2vkyJFasGCBPD09lZWVpcTERJWUlCguLs6l2QAAAAAAAAAAAAAAAAAA9Wt00C08PFxdu3atd02PHj30xRdfKCsrS9HR0Q79S5cu6fDhw+rZs6dOnDjh0G/ZsqUiIiLsaoMGDVJsbKy2bdumhIQEl2bdtWuX8vLytGLFCo0ePVqSNHLkSE2aNEmvvPKKNmzYIEk6cOCAjhw5opiYGMXHx9u+v++++/T4449rzZo1evXVV22zr127VoMHD7atGz9+vH7961/rz3/+s5544gludQMAAAAAAAAAAAAAAACAm6DFrdzcz89P/fr1U1ZWltP+gQMHZLFYNHz4cJf37N+/v3x8fFRYWOjyN++9954CAgJsITdJMplMmjBhgnJzc3X69GlJsj2VOnHiRLvve/Xqpbvvvluffvqprfb000/bhdxqDRo0SKWlpfr2229dng8AAAAAAAAAAAAAAAAAULdbGnSTJLPZrPz8fJWUlDj0MjMzFRQUpG7durm8n9Vq1dWrV+Xn5+fSeovFovz8fA0cONChVxtUuz7AJkne3t4Oa00mk0vnlZWVycPDQ61atXJp/a5duxQeHq7PP/9cmzZt0iOPPKLhw4drypQp+uijj+zWXrx4URs2bFBUVJRGjBihyMhI25Os10tMTFRkZKQuX76sF154QWPGjNGIESM0e/ZsnTx50qW5AAAAAAAAAAAAAAAAAMBdNEnQTZL27dtnVy8rK9Phw4dtfVcdO3ZMlZWVLt8CV1JSourqanXp0sWhV1v7+uuvJUkDBgyQJKcBs7y8PKdhuetduXJFH330kUJCQpyG5eqTnJysTZs2KSoqSosWLVKrVq20cOFCnTp1yrbm5Zdf1jvvvKOf//znWrp0qWbPnq3CwkLNmTNHVVVVdvtZLBbNnj1bBQUFmjdvnubPn6+vvvpK8+fPl8ViadBsAAAAAAAAAAAAAAAAANCcPBv7YUVFha5cueJQNxqNMhqNtr8DAgIUGhqqzMxMTZs2zVbfv3+/qqurZTablZeX5/QMq9VqO6OyslIFBQVKSUnRkCFD9MADD7g0Z2lpqSTpjjvucOjV1mrXREZGavTo0Xr11Vfl4eGhESNG6MyZM0pNTZWXl5fmzZtX71lvvPGGSktLtXjxYpdmu97nn3+urVu3KiAgQJI0YsQI3X///Xr//fc1c+ZMSdL48eO1ePFi+fj42L7z8/PTokWLlJubq/DwcFu9vLxcLVu21BtvvKGWLVtKktq1a6dnnnlGR48etVsLAAAAAAAAAAAAAAAAAO6s0UG3qKgop/XY2FjNmTPHrhYZGanU1FSdO3dOd955pyQpKytLQUFBCgwMrDPodv78eY0cOdKu1rt3by1evFienq6NXnvTmZeXl0OvtlZRUSFJ8vDw0IsvvqjU1FQ9//zztnU9evTQ22+/bQuhOXP48GH95S9/0ZgxYzRq1CiXZrteTEyM3f533HGHunXrpsLCQlvtvvvuc/iubdu2kq79X/3YggULbCE36Ycb6woLCwm6AQAAAAAAAAAAAAAAALhtNDrotnz5clto7XrOwmBms1mpqanat2+fpk6dqrKyMn3yySeKjY2t9wx/f3+tWLFCklRdXa0zZ85o586dmjRpkhITEzVmzJgbzlkbiKupqXHo1T7hWRt4s1gsWrp0qT788EM98cQT6t+/v86fP68tW7Zo5syZWrVqlbp16+awz7fffqvnnntOd911lxYtWnTDmZwZNmyYQ61169a6fPmyXe1///d/9eGHH+rYsWM6deqU7ca7Hz9H2qZNG4WEhDjsJ8lhTwAAAAAAAAAAAAAAAABwZ40OuvXt21ddu3Z1aW3Hjh0VGhqqrKwsTZ06VQcPHlRVVZXMZnO93xmNRtstZLUeeughxcTEKCkpSUOGDHH6JOn1fH19JUllZWUOvfLycrs1f/nLX/T+++9r1apVGj58uG3d2LFjFRUVpUWLFmnz5s12e1gsFi1cuFBlZWVauXKlba+GMplMDrUWLVrYBfQSExO1e/duDR48WL/4xS/UpUsXeXl56emnn3ZpP4PBIMl56A8AAAAAAAAAAAAAAAAA3FWLpjpozJgxysnJ0YULF5SZmamePXsqMDCwwfsYDAaZzWaVl5crPz//hus7deokDw8PlZSUOPSKi4slSZ07d5Yk7du3T3fddZddyE26FhobP368jh07ptOnT9v1XnrpJeXk5Oi5555T7969G/x7XHXgwAHt3r1bs2bN0rp16xQbG6uxY8eqR48et+xMAAAAAAAAAAAAAAAAAHAHTRZ0M5vNslqt2rNnjw4dOuTSs6N1sVqtkq49Z3oj3t7e6tWrl/Ly8hx6ubm5kqTQ0FBJ0sWLF9WihfP/ktrb0L777jtbbfv27frb3/6m2NjYG95O91MdOXJEkvTYY4/Z1U+cOHFLzwUAAAAAAAAAAAAAAACA5tZkQbcOHTooLCxMmzZtUmVlZaODYRaLRXv37pWXl5fCwsJc+mbs2LE6fvy4cnJy7PbZsWOHunfvruDgYElSr169dPr0aWVnZ9t9X1FRoffff19eXl7q3r27pGvBs1deeUUjR47UrFmzGvVbGsJoNEqSSktLbbXy8nKlpaXd8rMBAAAAAAAAAAAAAAAAoDl5NvbD7OxsnTx50mkvIiLCad1sNis5OdnlZ0srKyt18OBBSdducTt79qwyMjJ0/PhxzZs3T23btnVp1smTJ2vXrl1asGCB4uLi5O/vr/+/vfuOq7L8/zj+PoBsUFETXLi34sBwIaCYflOz0jTMkSO1zJWWppkLNU3NUeYemaM0B1aak3JkZu49cuTALQoKMu7fHz44P48HFFAB6/V8PHjUua7Pdd+f+z73SPl0XcuWLdOZM2c0adIkmUwmSdI777yj33//Xb169VKLFi1UpkwZXbt2TUuXLtWZM2fUtWtXubq6KioqSv369ZO9vb1eeuklbdmyxWqfFSpUUI4cOVKVX2r4+/vrm2++Ub9+/dS6dWslJCRo/vz5cnR0fGr7AAAAAAAAAAAAAAAAAICsKN2FbiNGjEix7+EZ0ZIEBwdr/PjxqZ7NLTIyUr179zZ/dnZ2VqlSpTR27FgFBgamOldnZ2dNnz5dEydO1IwZMxQTE6NSpUpp8uTJevHFF81xxYoV0zfffKNZs2ZpzZo1WrBggRwdHVWqVCl17txZL730kiTp1q1b5iVMP/nkk2T3OXXqVPn6+qY6x8fx8fFRaGio5syZo1GjRilXrlxq1KiR6tSpozZt2jy1/QAAAAAAAAAAAAAAAABAVmMyDMPI7CSeRFRUlFI6BFtbWzk7O2dwRv8vNjZW9+7dS7HfxcVFNjYZtnpsupnGxmd2CsjijA+bZ3YKeB4YKzI7AwAAAAAAAAAAAAAA8JxK94xuWUVISIguXryYbF+VKlU0ffr0DM7o/82dO1czZsxIsT8sLEz58uXLwIwAAAAAAAAAAAAAAAAA4Pnz3Be6jR49OsVZ01xdXTM4G0tNmzaVn59fiv25c+fOwGwAAAAAAAAAAAAAAAAA4Pn03C9dimdv+vTpat++vbJly5bZqQAAAAAAAAAAAAAAAAD4D7LJ7AQAAAAAAAAAAAAAAAAAAHgUCt0AAAAAAAAAAAAAAAAAAFkahW4AAAAAAAAAAAAAAAAAgCyNQjcAAAAAAAAAAAAAAAAAQJZGoRsAAAAAAAAAAAAAAAAAIEuj0A0AAAAAAAAAAAAAAAAAkKVR6AYAAAAAAAAAAAAAAAAAyNIodAMAAAAAAAAAAAAAAAAAZGkUugEAAAAAAAAAAAAAAAAAsjSTYRhGZieBrM00Nj6zU0AWZnzYPLNTQFZnrMjsDAAAAAAAAAAAAAAAwHOOGd0AAAAAAAAAAAAAAAAAAFkahW4AAAAAAAAAAAAAAAAAgCyNQjcAAAAAAAAAAAAAAAAAQJZGoRsAAAAAAAAAAAAAAAAAIEuj0A0AAAAAAAAAAAAAAAAAkKXZpXXAtGnTNGPGjEfG7Ny5U507d9auXbtUtmxZffPNNynGrly5UsOHD5ckhYWFKV++fJJkHv8gNzc3Va9eXb169VLevHnTlHdUVJQmT56s8PBwRUdHq2zZsurWrZt8fHysYvfv368vv/xShw4dkqOjowIDA9WzZ0+5urpaxG3atEnLly/XwYMHdefOHeXPn1+NGjVSmzZtZGeX5lMLAAAAAAAAAAAAAAAAAEhGuquxBg4cqNy5cz8yxtbWVocOHdLFixfl5eWVbMyGDRtka2urhIQEq77s2bNryJAhkqT4+HidOXNGCxcuVKtWrbRgwQJ5enqmKtf4+Hh1795df//9tzp16qQ8efJo8eLF6tq1q2bOnKly5cqZYw8dOqQuXbqoZMmSGjhwoK5cuaKZM2fq+PHjmjlzprmA7ccff9TQoUMVEBCg7t27y8nJSVu2bNFXX32lkydPKjQ0NFW5AQAAAAAAAAAAAAAAAAAeLd2Fbr6+vipYsOAjY4oWLaoTJ05ow4YNat26tVX/rVu3tGPHDhUrVkzHjh2z6ndwcJC/v79FW5UqVdShQwctXrxYvXr1SlWuq1at0v79+/X5558rKChIklSnTh298cYbGjdunGbPnm2OHTt2rLJnz64pU6bI2dlZklSwYEH17dtXq1at0muvvSZJcnV11YwZM1SpUiXz2AYNGsjW1lY//vijOnbsqCJFiqQqPwAAAAAAAAAAAAAAAABAymye5cbd3NxUrlw5bdiwIdn+X3/9VfHx8apZs2aqt1mxYkU5OTnp5MmTqR7z888/y9PT01zkJknOzs565ZVXtG/fPp07d06SdP78ee3bt09NmzY1F7lJUmBgoPLly6eff/7Zou3BIrckSYV5R48eTXV+AAAAAAAAAAAAAAAAAICUPdNCN0kKDg7WgQMHFBERYdW3fv16FS9eXN7e3qnenmEYSkxMlJubW6ri4+PjdeDAAVWuXNmqr2rVqpKk3bt3S5L27Nkj6f6scQ+rXLmy9u3bp/j4+Efuz8HBQZLk5OSUqvwuXLggX19ffffdd/rtt9/Utm1b1apVS40aNdKsWbMsYuPi4vTTTz+pc+fOCggIkL+/v9q0aaPffvvNIm7VqlXy9fXV8ePHNX/+fDVt2lQ1a9ZUy5YtrWIBAAAAAAAAAAAAAAAAIKvLkEI3Sdq4caNFe1RUlHbs2GHuT60jR44oNjY21bPARUREKC4uTgUKFLDqS2r7559/LP6ZUmxCQoIuXrz4yP399ttvsre3V/ny5VOVX5KNGzfqo48+kp+fnwYPHixfX199/fXX+vHHH80xYWFhGjlypLy9vfXxxx9rwIABypYtm/r27ZvsDHJjx47V/PnzFRISooEDB8rFxUUfffSRzp49m6bcAAAAAAAAAAAAAAAAACAz2aV3YExMjO7cuWPVni1bNmXLls382dPTUxUqVND69evVqlUrc3t4eLji4uIUHBys/fv3J7sPwzDM+4iNjdWhQ4c0fvx4VatWTQ0bNkxVnpGRkZKkHDlyWPUltSXFPCo2Z86c5piCBQta9N25c0eXLl1SWFiYli1bpm7duilXrlypyi/JX3/9pfHjx6tOnTqSpJdeeknHjx9XWFiYGjduLEkqXbq0lixZonz58pnH+fr66uWXX1Z4eLhKlSplsc3jx49r4cKF8vT0lCTVqlVLDRo00OrVq9WlS5c05QcAAAAAAAAAAAAAAAAAmSXdhW4hISHJtnfo0EHvvfeeRVu9evU0YcIEXb58WS+88IIkacOGDSpevLgKFy6cYqHblStXzIVfSUqWLKlBgwbJzi51qd+7d0+SZG9vb9WX1BYTEyPpfjFdSrFJxXtJsQ/q27evduzYIRsbG3Xt2lXt27dPVW4Pql27ttWx+vj4aO3atebP5cqVsxrn4OAgZ2dnXblyxaqvffv25iI36X4Bn7e3t06ePJnm/AAAAAAAAAAAAAAAAAAgs6S70G3kyJHmorUHPVhYlSQ4OFgTJkzQxo0b9eabbyoqKkp//PGHOnTo8Mh95MyZU59//rkkKS4uThcuXNCKFSv0xhtvaMiQIapfv/5j80wqiEtISLDqi4+Pl/T/hW1JsfHx8VaFdEmxD85Wl+SDDz7QmTNntGfPHs2ZM0cHDx7U6NGjky2YS0n16tWt2tzd3XX79m2LtmPHjmnNmjXat2+fzpw5oxs3bljk96AaNWqkapsAAAAAAAAAAAAAAAAAkJWlu9CtTJkyVkt4piRv3ryqUKGCNmzYoDfffFObN2/WvXv3FBwc/Mhx2bJlU6VKlSzaGjdurPbt2ys0NFTVqlVLdpnRB7m6ukqSoqKirPqio6MtYh6MdXR0fGTsg4oXL67ixYurXr168vHxUf/+/fX999+rdevWj8ztQS4uLlZtNjY2SkxMNH9evny5PvvsMxUsWFBBQUFq2rSp8ufPr4EDBya7TWdn52S3mVzRHwAAAAAAAAAAAAAAAABkVTYZtaP69etr7969unr1qtavX69ixYqpcOHCad6Ora2tgoODFR0drQMHDjw23svLSyaTSREREVZ9Fy9elCTly5fP4p+Pis2fP/8j91e3bl05ODho9+7dj80tLa5du6axY8fKz89PixcvVrdu3dSkSRNVqVLlqe4HAAAAAAAAAAAAAAAAALKaDCt0Cw4OlmEYWrNmjbZv356qZUdTYhiGpPvLmT6Oo6OjSpQoof3791v17du3T5JUoUIFi38mV0C3b98+lShRwmqmt+TY2tqmKre0OHjwoGJjY/XKK69YLKt6/fp1Xbly5anuCwAAAAAAAAAAAAAAAACykgwrdMuTJ498fHw0f/58xcbGPnbZ0pTEx8dr7dq1sre3l4+PT6rGvPTSSzp69Kj27t1rsZ1ly5apSJEiKlWqlCSpVKlSKlKkiJYtW6b4+Hhz7J49e3T06FE1aNDA3DZhwgRdvnzZal+//PKL7ty589RnWsuWLZsk6datW+Y2wzA0ceLEp7ofAAAAAAAAAAAAAAAAAMhq7B4fkrydO3fq9OnTyfb5+/sn2x4cHKyxY8emetnS2NhYbd68WdL9oq5Lly4pLCxMR48eVc+ePeXh4ZGqXFu0aKFVq1apT58+6tSpk3LmzKlly5bpzJkzmjRpkkwmkzn2gw8+UM+ePdWtWze9/vrrunHjhmbOnKmiRYuqZcuW5rizZ8/q9ddf1//+9z/5+PjIwcFBe/bs0Q8//KDixYurRYsWqcottSpUqKBcuXLpq6++0t27d5UnTx79+OOPOnfunJydnZ/qvgAAAAAAAAAAAAAAAAAgK0l3oduIESNS7Nu5c2ey7cHBwRo/fnyqZ3OLjIxU7969zZ+dnZ1VqlQpjR07VoGBganO1dnZWdOnT9fEiRM1Y8YMxcTEqFSpUpo8ebJefPFFi9gaNWpo4sSJmj59uoYNGyZHR0fVqVNHPXr0kJOTkzlu9OjRWrlypdauXav169fr7t27yps3r0JCQtSxY8enXnzm6uqqCRMmaNKkSZo+fbrs7Ozk7++vTz/91KIADwAAAAAAAAAAAAAAAAD+bUyGYRiZncSTiIqKUkqHYGtrm6mzncXHx+vu3bsp9js6OpqXJM3KTGPjHx+E/yzjw+aZnQKyOmNFZmcAAAAAAAAAAAAAAMBz6fTp0ypSpIg2bdqUponB/o3SPaNbVhESEqKLFy8m21elShVNnz49gzP6f3v27FHXrl1T7B88eLCaNGmSgRkBAAAAAAAAAAAAAAAAyCp27dqlcePGKTw8XFevXpWnp6dKliypjh076s0338zs9MwSExM1ZswYTZs2TRcvXlTp0qU1ePBgvfbaaxmWw3Nf6DZ69Gjdu3cv2T5XV9cMzsZS6dKlNXPmzBT7CxUqlIHZAAAAAAAAAAAAAAAAAP8OWWGFQqPvk5VeTZ48Wb169VKDBg00adIkFSlSRJcvX9YPP/yg1q1bq0aNGk8p0yfXv39/TZ48WePHj5evr68WL16sZs2aadWqVWrUqFGG5PDcL12KZ2/69Olq3779c7HMKgAAAAAAAAAAAAAAAP79nvdCt3Xr1qlBgwYaOHCghg8fbtW/Zs0aVa1aVdHR0Zm+dOm5c+dUpEgRDR8+XP379ze3N27cWCdOnNCRI0cyJA+bDNkLAAAAAAAAAAAAAAAAAECSNGDAAFWpUkXDhg1Ltr9hw4bKkydPsn3r169X06ZNlTt3bjk5OalixYpaunSpRczhw4f18ssvy8PDQx4eHgoICNDy5cvN/b///rsCAgLk7u6uF154QQ0bNtSvv/6a7P7CwsIUHx+vdu3aWbS3bdtWR48e1YEDB9Jy6OlGoRsAAAAAAAAAAAAAAAAAZJAzZ85o586d6tSpk0wmU5rHt2zZUiVKlNDSpUv166+/qlq1amrZsqUOHTpkjmnSpIkiIyMVFhamH3/8UXXq1DHPHHf79m01atRIOXPm1Lp16/T999+rdOnSGj16dLL727Nnjzw9PeXl5WXRXqVKFXN/RniyhWIBAAAAAAAAAAAAAAAAAKm2f/9+SVLVqlXTNX7v3r0qUKCA+XP58uU1Z84cbdy4UWXLltXVq1d18uRJde/eXbVr15Yk1axZU71795YkHT16VDdu3FD79u3l5+cnSQoMDNT169eT3d+FCxesitwkmdsuXLiQruNIK2Z0AwAAAAAAAAAAAAAAAIAMEhkZKUlyd3dP1/ikIrdTp05pzZo1mjlzphwcHMyFarly5VLRokX19ddfa/fu3eZxHh4ekqSSJUsqR44c+uyzz3Ty5Emr/ofdvXtXTk5OVu1JbXfu3EnXcaQVhW4AAAAAAAAAAAAAAAAAkEHc3Nwk3V9CND2mTZumIkWKqGjRomrXrp0WL14sk8mkxMRESZLJZNLy5csVHx+vqlWr6rXXXtNff/1lHu/u7q5ly5bp1KlTKlOmjN5++20dP348xf05ODgoLi7Oqj2pzcHBIV3HkVYUugEAAAAAAAAAAAAAAABABildurQk6eDBg2keO2/ePHXt2lWtW7fW5cuXdenSJW3btk0vvPCCRVzFihV16NAhffXVVzpw4ICqVaumzz77zNwfFBSkEydOaNiwYVq3bp3Kly+v+fPnJ7vPPHny6OrVq1btV65cMfdnBArdAAAAAAAAAAAAAAAAACCDlCxZUqVKldJ3332X5rGzZs1SjRo1NHz4cHOBWVxcnCIiIqxi7e3t9e677+rw4cNq06aNBgwYoHPnzpn7XV1d1b9/fx0/flwBAQHq0qVLsjO3lStXTmfPnlVUVJRFe1KhXrly5dJ8HOlBoRsAAAAAAAAAAAAAAAAAZKDBgwdr9erVWrBgQbL9K1eutFhuNElkZKQ8PDws2ubMmaPY2Fjz55s3byohIcH82c7OTq+88ooMw1BERISuXbtmMd7Z2VkNGjTQ3bt3FRkZabXPJk2aKDExUd9//71F+4IFC+Tp6akXX3zx8Qf8FNhlyF4AAAAAAAAAAAAAAAAAAJKkkJAQ7dq1S23bttWGDRvUvHlzeXl56eLFi1q0aJEWLFigrVu3Wo0LCgrSV199pa+++kp+fn5avXq1Zs6cKQcHB3PMihUrNGLECPXo0UPVq1fXtWvXNHToUJUrV06VKlVSaGiowsLC9P7778vHx0dnzpzRxIkT1aBBA+XOndtqn2XKlFG7du30wQcfyMbGRhUqVNDKlSv17bffas6cObK1tX2m5yoJhW4AAAAAAAAAAAAAAAAAnitG3+e/7Onzzz9X3bp1NXnyZLVt21aRkZHKlSuXqlevrl9++UU1atTQ6dOnLcaEhoYqOjpaQ4cO1Z07dxQUFKT169erdu3a5pgmTZro0KFDmjp1qj7++GO5urqqbt26GjVqlOzs7NSxY0dFRETos88+07lz5+Th4aEmTZooNDQ0xVynTZsmLy8vDRo0SJcvX1aJEiX0zTffqHXr1s/q9FgxGYZhZNje8FwyjY3P7BSQxRgfNs/sFJAVGSsyOwMAAAAAAAAAAAAAAPAvZZPZCQAAAAAAAAAAAAAAAAAA8CgUugEAAAAAAAAAAAAAAAAAsjQK3QAAAAAAAAAAAAAAAAAAWRqFbgAAAAAAAAAAAAAAAACALM0urQOmTZumGTNmPDJm586d6ty5s3bt2qWyZcvqm2++STF25cqVGj58uCQpLCxM+fLlkyTz+Ae5ubmpevXq6tWrl/LmzZumvKOiojR58mSFh4crOjpaZcuWVbdu3eTj42MRZxiGfvjhBy1dulRnz56Vu7u7goKC1K1bN7m6ulrEbt++XXPnztXhw4clSYUKFVLLli3VqFEjmUymNOUHAAAAAAAAAAAAAAAAAEhemgvdkgwcOFC5c+d+ZIytra0OHTqkixcvysvLK9mYDRs2yNbWVgkJCVZ92bNn15AhQyRJ8fHxOnPmjBYuXKhWrVppwYIF8vT0TFWu8fHx6t69u/7++2916tRJefLk0eLFi9W1a1fNnDlT5cqVM8eOHz9eixYtUlBQkDp16qSIiAjNmjVLhw8f1syZM2Vnd/+UTZ8+XTNmzFCdOnXUp08f2dnZacOGDRoyZIgiIiLUqVOnVOUGAAAAAAAAAAAAAAAAAHi0dBe6+fr6qmDBgo+MKVq0qE6cOKENGzaodevWVv23bt3Sjh07VKxYMR07dsyq38HBQf7+/hZtVapUUYcOHbR48WL16tUrVbmuWrVK+/fv1+eff66goCBJUp06dfTGG29o3Lhxmj17tiTp5MmTWrRokRo2bKjQ0FDz+BdffFFt2rRRWFiYXn/9dXPuU6dOVdWqVc1xL7/8st566y0tWrRIHTt2ZFY3AAAAAAAAAAAAAAAAAHgKbJ7lxt3c3FSuXDlt2LAh2f5ff/1V8fHxqlmzZqq3WbFiRTk5OenkyZOpHvPzzz/L09PTXOQmSc7OznrllVe0b98+nTt3TpIUHh4uSVZFeSVLllTNmjX1yy+/mNv69u1rUeSWpEqVKoqMjNT169dTnR8AAAAAAAAAAAAAAAAAIGXPtNBNkoKDg3XgwAFFRERY9a1fv17FixeXt7d3qrdnGIYSExPl5uaWqvj4+HgdOHBAlStXtupLKlTbvXu3JJlzLFSokFVs4cKFdeTIkcfuLyoqSiaTSS4uLqnKb9WqVfL19dXx48c1f/58NW3aVDVr1lTLli3122+/WcTevHlTs2fPVkhIiGrVqqV69eqZl2R90JAhQ1SvXj3dvn1bo0aNUv369VWrVi29++67On36dKryAgAAAAAAAAAAAAAAAICsIkMK3SRp48aNFu1RUVHasWOHuT+1jhw5otjY2FTPAhcREaG4uDgVKFDAqi+p7Z9//pF0f6nUpNwedvHiRUVHR+vevXsp7uvOnTv67bffVLZsWTk6OqYqvyRjx47V/PnzFRISooEDB8rFxUUfffSRzp49a44ZM2aMvv/+ewUEBGjo0KF69913dfLkSb333ntWecXHx+vdd9/VoUOH1LNnT/Xu3VunTp1S7969FR8fn6bcAAAAAAAAAAAAAAAAACAz2aV3YExMjO7cuWPVni1bNmXLls382dPTUxUqVND69evVqlUrc3t4eLji4uIUHBys/fv3J7sPwzDM+4iNjdWhQ4c0fvx4VatWTQ0bNkxVnpGRkZKkHDlyWPUltSXFlC5dWpK0detWvfbaa+a469eva+vWrZKkW7duKXfu3Mnua8qUKYqMjNSgQYNSlduDjh8/roULF8rT01OSVKtWLTVo0ECrV69Wly5dJEkvv/yyBg0aJCcnJ/M4Nzc3DRw4UPv27ZOvr6+5PTo6Wg4ODpoyZYq5gC9Xrlz68MMPtWfPHotYAAAAAAAAAAAAAAAAAMjK0l3oFhISkmx7hw4d9N5771m01atXTxMmTNDly5f1wgsvSJI2bNig4sWLq3DhwikWul25ckV16tSxaCtZsqQGDRokO7vUpZ4005m9vb1VX1JbTEyMOc+vv/5aEydOlJubm6pVq6bz588rNDRUxYoV04EDB2Rjk/wkeDt27NB3332n+vXrKzAwMFW5Pah9+/bmIjfpfhGet7e3Tp48aW6rXbu21TgPDw9J98/Vw/r06WMucpOkSpUqSZJOnjxJoRsAAAAAAAAAAAAAAACA50a6C91GjhxpLlp70IPFWkmCg4M1YcIEbdy4UW+++aaioqL0xx9/qEOHDo/cR86cOfX5559LkuLi4nThwgWtWLFCb7zxhoYMGaL69es/Ns+kgriEhASrvqQlPJMK3pycnDR58mR98skn6t+/v6T7y5l26NBB2bJl06FDh+Tm5ma1nevXr2vw4MHKnz+/Bg4c+NicklOjRg2rNnd3d92+fduibevWrdq0aZOOHDmis2fPmme8e3g50uzZs6ts2bJW25NktU0AAAAAAAAAAAAAAAAAWc/p06dVpEgRbdq0KV2Tb/2bpLvQrUyZMipYsGCqYvPmzasKFSpow4YNevPNN7V582bdu3dPwcHBjxyXLVs28yxkSRo3bqz27dsrNDRU1apVS3ZJ0ge5urpKkqKioqz6oqOjLWIkqWjRolq4cKHOnTun27dvq2DBgnJ1ddXw4cOVL18+i2VZpfsFZh999JGioqI0ceJEi22lhbOzs1WbjY2NRYHekCFD9OOPP6pq1aqqW7euChQoIHt7e/Xt2zdV27O1tZWUfNEfAAAAAAAAAAAAAAAA8NwwvZrZGUjGiifexK5duzRu3DiFh4fr6tWr8vT0VMmSJdWxY0e9+eabT57jU5SYmKhu3bpp6tSpmjNnjt5+++0M3X/y63A+A/Xr19fevXt19epVrV+/XsWKFVPhwoXTvB1bW1sFBwcrOjpaBw4ceGy8l5eXTCaTIiIirPouXrwoScqXL59VX4ECBVSmTBlz4dqOHTtUuXJlq7jRo0dr7969Gjx4sEqWLJnWw0m1X3/9VT/++KO6du2qadOmqUOHDnrppZdUtGjRZ7ZPAAAAAAAAAAAAAAAAAM/G5MmTVa1aNd24cUOTJk3S77//rmnTpqlw4cJq3bq1zpw5k9kpmh05ckR16tTRTz/9lGk5ZFihW3BwsAzD0Jo1a7R9+/ZULTuaEsMwJN1fzvRxHB0dVaJECe3fv9+qb9++fZKkChUqPHIbW7Zs0cWLF9WgQQOL9iVLlmj58uXq0KHDY2ene1K7du2SJDVv3tyi/dixY890vwAAAAAAAAAAAAAAAACernXr1qlnz54aMGCAfv75ZzVr1kxVqlRRw4YNNWPGDP3444/JruiYWdauXauqVaua660yQ4YVuuXJk0c+Pj6aP3++YmNj010YFh8fr7Vr18re3l4+Pj6pGvPSSy/p6NGj2rt3r8V2li1bpiJFiqhUqVIpjr1+/brGjBmjSpUqqXr16ub2pGkD69Spo65du6brWNIiacnUyMhIc1t0dLSmT5/+zPcNAAAAAAAAAAAAAAAA4OkZMGCAqlSpomHDhiXb37BhQ+XJkyfZvvXr16tp06bKnTu3nJycVLFiRS1dutQi5vDhw3r55Zfl4eEhDw8PBQQEaPny5eb+33//XQEBAXJ3d9cLL7yghg0b6tdff00x3x49emjixInKkSNH2g/2KbFL78CdO3fq9OnTyfb5+/sn2x4cHKyxY8emetnS2NhYbd68WdL9WdwuXbqksLAwHT16VD179pSHh0eqcm3RooVWrVqlPn36qFOnTsqZM6eWLVumM2fOaNKkSTKZTObYNm3aqEGDBvL09NT58+f13Xffyc7OTqGhoeaYqKgo9evXT/b29nrppZe0ZcsWq31WqFDhqX6x/v7++uabb9SvXz+1bt1aCQkJmj9/vhwdHZ/aPgAAAAAAAAAAAAAAAAA8W2fOnNHOnTv19ddfW9QtpVbLli3Vvn179e7dW87Ozpo2bZpatmyp/fv3q2zZspKkJk2aKG/evAoLC5ONjY1Wr16t4cOH67XXXtPt27fVqFEj1alTR+vWrdPdu3e1YsUKjR49WgEBAU/7cJ+adBe6jRgxIsW+nTt3JtseHBys8ePHp3o2t8jISPXu3dv82dnZWaVKldLYsWMVGBiY6lydnZ01ffp0TZw4UTNmzFBMTIxKlSqlyZMn68UXX7SILVGihBYuXKgbN27Iw8NDgYGB6tSpk0VR3a1bt3Tjxg1J0ieffJLsPqdOnSpfX99U5/g4Pj4+Cg0N1Zw5czRq1CjlypXLfMG1adPmqe0HAAAAAAAAAAAAAAAAwLOzf/9+SVLVqlXTNX7v3r0qUKCA+XP58uU1Z84cbdy4UWXLltXVq1d18uRJde/eXbVr15Yk1axZ01yHdfToUd24cUPt27eXn5+fJCkwMFDXr19/ksN65kyGYRiZncSTiIqKUkqHYGtrm6lr1cbGxurevXsp9ru4uMjGJsNWj00309j4zE4BWYzxYfPMTgFZkbEiszMAAAAAAAAAAAAAAPxXmF7N7AzS/XvyBQsWqHXr1jpy5IhKlSr1yNjTp0+rSJEi2rRpk9XEYKdOndLRo0d17Ngx9evXTx9//LE+/fRTGYah4sWLK1u2bFq0aJEqV65sMe7WrVvy9vZW6dKl9e2336pYsWJpyt9kMmnOnDl6++230zTuSaV7RresIiQkRBcvXky2r0qVKpo+fXoGZ/T/5s6dqxkzZqTYHxYWpnz58mVgRgAAAAAAAAAAAAAAAAAyk5ubmyTp9u3b6Ro/bdo0ffbZZzp9+rReeOEFFStWTCaTSYmJiZLuF6ItX75cr7/+uqpWraqmTZvqk08+Mc8g5+7urmXLlikkJERlypRRq1atNHDgQJUoUeLpHOAz8twXuo0ePTrFWdNcXV0zOBtLTZs2NU/vl5zcuXNnYDYAAAAAAAAAAAAAAAAAMlvp0qUlSQcPHpSvr2+axs6bN09du3bVJ598oh49eihPnjySpMKFC1vEVaxYUYcOHdKsWbM0fvx4VatWTSNHjlT//v0lSUFBQTpx4oS+/PJLTZ48WYsWLdLMmTPVpk2bJz/AZ+S5X7oUz9706dPVvn17ZcuWLbNTAQAAAAAAAAAAAAAAAJ7rpUul+8VuRYsW1c8///zIuIeXLq1Tp47i4+O1bds2c0xcXJzc3NzUv39/DRkyxGob8fHx6tixo+bPn6+zZ8+qQIECFv137tzRq6++qi1btigyMvKxNUKZtXSpTYbuDQAAAAAAAAAAAAAAAAD+4wYPHqzVq1drwYIFyfavXLlSf/31l1V7ZGSkPDw8LNrmzJmj2NhY8+ebN28qISHB/NnOzk6vvPKKDMNQRESErl27ZjHe2dlZDRo00N27dxUZGfkkh/VMPfdLlwIAAAAAAAAAAAAAAADA8yQkJES7du1S27ZttWHDBjVv3lxeXl66ePGiFi1apAULFmjr1q1W44KCgvTVV1/pq6++kp+fn1avXq2ZM2fKwcHBHLNixQqNGDFCPXr0UPXq1XXt2jUNHTpU5cqVU6VKlRQaGqqwsDC9//778vHx0ZkzZzRx4kQ1aNBAuXPnzsjTkCYUugEAAAAAAAAAAAAAAAB4vjzBsqFZxeeff666detq8uTJatu2rSIjI5UrVy5Vr15dv/zyi2rUqKHTp09bjAkNDVV0dLSGDh2qO3fuKCgoSOvXr1ft2rXNMU2aNNGhQ4c0depUffzxx3J1dVXdunU1atQo2dnZqWPHjoqIiNBnn32mc+fOycPDQ02aNFFoaGgGn4G0MRmGYWR2Esjapk+frvbt2z92/V0AAAAAAAAAAAAAAAAAeBZsMjsBAAAAAAAAAAAAAAAAAAAehUI3AAAAAAAAAAAAAAAAAECWRqEbAAAAAAAAAAAAAAAAACBLo9ANAAAAAAAAAAAAAAAAAJClUegGAAAAAAAAAAAAAAAAAMjSKHQDAAAAAAAAAAAAAAAAAGRpFLoBAAAAAAAAAAAAAAAAALI0Ct0AAAAAAAAAAAAAAAAAAFkahW4AAAAAAAAAAAAAAAAAgCyNQjcAAAAAAAAAAAAAAAAAQJZGoRsAAAAAAAAAAAAAAAAAIEuj0A0AAAAAAAAAAAAAAAAAkKVR6AYAAAAAAAAAAAAAAAAAyNIodAMAAAAAAAAAAAAAAAAAZGkUugEAAAAAAAAAAAAAAAAAsjQK3QAAAAAAAAAAAAAAAAAAWRqFbgAAAAAAAAAAAAAAAACALM0usxNA1mYYhu7evatbt24pW7ZsmZ0OAAAAAAAAAAAAAAAAgH8ZNzc3mUymR8aYDMMwMigfPIeuXr2qPHnyZHYaAAAAAAAAAAAAAAAAAP6lIiMj5e7u/sgYZnTDIzk4OKhSpUr66aef5OrqmtnpAACeA1FRUWrUqBHvDgBAmvD+AACkFe8OAEB68P4AAKQV7w4AyBhubm6PjaHQDY9kMplka2srd3d3XtoAgFSxsbHh3QEASDPeHwCAtOLdAQBID94fAIC04t0BAFmHTWYnAAAAAAAAAAAAAAAAAADAo1DoBgAAAAAAAAAAAAAAAADI0ih0wyPZ29vrnXfekb29fWanAgB4TvDuAACkB+8PAEBa8e4AAKQH7w8AQFrx7gCArMNkGIaR2UkAAAAAAAAAAAAAAAAAAJASZnQDAAAAAAAAAAAAAAAAAGRpFLoBAAAAAAAAAAAAAAAAALI0Ct0AAAAAAAAAAAAAAAAAAFkahW7/cZs2bVKLFi1Us2ZNtWjRQhs2bHhk/L179zRu3DgFBwfL399fvXv3VkRERAZlCwDIKtL6/jh69Kh69eqlgIAABQQEqEOHDvrzzz8zKFsAQFaQ1nfHg9auXStfX19NmzbtGWYIAMiK0vP+iIiI0MCBAxUcHKxatWopJCREhw8fzoBsAQBZQVrfHSdPnlTv3r0VEBCgmjVrqm3bttqyZUsGZQsAyErmzp0rX19frVq16rGxt2/f1uDBgxUYGKjAwEB9+umnunXrVgZkCQD/bRS6/Yft2rVL/fv3V4sWLfTTTz8pJCREAwYM0M6dO1McM2bMGP3222+aNGmSlixZIpPJpG7duikuLi4DMwcAZKa0vj+uXbum9957T5UqVdLChQu1ZMkSlStXTr1799b58+czOHsAQGZIz589kkRGRmrSpEkqVKhQBmQKAMhK0vP+iIiIULt27eTi4qJZs2Zp5cqV6tSpk0wmUwZmDgDILGl9d1y5ckWdO3eWJM2ePVvLli2Tn5+fevfure3bt2dk6gCATBQdHa0ePXpo5cqVcnV1TdWYfv366dSpU5ozZ47mzp2rf/75Rx999NEzzhQAQKHbf9isWbNUp04dNW/eXDlz5tRrr72moKAgzZo1K9n4iIgIhYWF6YMPPlDZsmXl6empwYMHKyIiQmvXrs3g7AEAmSWt7w9nZ2d9++23evvtt5U/f3698MIL6tmzpwzD0I4dOzI4ewBAZkjru+NB48ePV5MmTZQ7d+4MyBQAkJWk5/0xcuRI+fv7a8CAAfL29lbu3LlVr149lS5dOgMzBwBklrS+OzZt2qS7d+9q1KhRKlasmDw9PdWtWzdVqFBBK1euzODsAQCZJSYmRlWrVtXChQvl5ub22Pi9e/dqx44dGjRokIoUKaLChQtr0KBB2rlzp3bv3p0BGQPAfxeFbv9R9+7d019//aU6depYtNepU0e7du1STEyM1ZgdO3bI1tZWNWvWNLdlz55dPj4++v3335951bEIsQAAL+1JREFUzgCAzJee94eTk5O8vLws2mxsbGRjY5NsPADg3yU9744k27dv14EDB9S+fftnnSYAIItJz/vj3Llz2rFjhzp16pRRaQIAspD0vDsSEhJka2urbNmyWbQ7OTmxkg0A/IfkypVL7dq1k5OTU6rit2/fLi8vL5UoUcLcVrRoURUoUEDbtm17VmkCAESh23/W+fPnFR8fL29vb4v2QoUKKSEhIdml5E6fPi0vLy+rP/AVKlRIZ8+efab5AgCyhvS8P5Kzfv16xcTEyM/P71mkCQDIQtL77rh7965Gjhypjz/+WPb29hmRKgAgC0nP++OPP/5Q+fLldeHCBXXt2lUvv/yyunTpooMHD2ZU2gCATJSed8f//vc/ubu7a+zYsbp7964SExMVFhamPXv2KCQkJKNSBwA8Z06fPm31vpH4vTkAZAS7zE4AmePWrVuSJHd3d4v2pKlYb9++bTXm9u3bVvFJY5K2BwD4d0vP++NhV69e1eeff66mTZuqaNGiTz9JAECWkt53x5QpU1SlShX5+vo+2wQBAFlSet4fZ86cUXR0tD7//HP17NlTHh4eWrJkiTp27KgFCxaoWLFizz5xAECmSc+7I0eOHJo3b54++OADBQYGytbWVi4uLvr6669VsWLFZ580AOC5dOvWrRR/b37jxo1MyAgA/juY0e0/yjAMSZLJZLJot7W1lXR/uu6HJSYmysbG+pKxsbFRYmLiM8gSAJDVpOf98aD4+HgNHDhQHh4e6tOnz7NJEgCQpaTn3bF//36tWbNGvXr1eub5AQCypvS8P6Kjo3Xy5EmNGjVK1atXV8mSJTVw4EB5e3tr/vz5zz5pAECmSs+7IyoqSh9++KGcnZ01bdo0zZ07Vw0aNFCfPn10/PjxZ580AOC5ZBhGir83f9zvSQAAT4YZ3f6jnJ2dJd1fDuhB0dHRkiQXF5dkx9y5c8eq/c6dO8nGAwD+fdLz/njQ6NGj9ffff2vOnDlycnJ6NkkCALKUtL47EhISFBoaqu7duytHjhwZkiMAIOtJ799dFS5cWIULF7Zor1q1qvbu3ftsEgUAZBnpeXcsXrxYly5d0rJly+To6ChJ6tu3ry5duqSxY8dq2rRpzzhrAMDziN+bA0DmYUa3/6h8+fLJZDLp/PnzFu0XLlyQyWRSgQIFrMYUKFBAFy9eNP9fUUnOnz+fbDwA4N8nPe+PJPPmzdPPP/+scePG8d4AgP+QtL477ty5o5MnT2r8+PEKDAw0/+zdu1fz5s1TYGCg9uzZk4FHAADIDOn5s0fu3Lnl4OBg1W4YhuLj459ZrgCArCE9744DBw6oTJky5iK3JFWrVtWBAweeab4AgOdXgQIFdOHCBav2Cxcu8PsPAHjGKHT7j3J1dVXZsmW1fft2i/bff/9dZcqUkaurqyRZLEn64osvKjo6Wvv37ze3xcTEaPfu3apWrVrGJA4AyFTpeX9I0oYNGzRlyhQNGTJEFStWzLB8AQCZL63vDhcXF4WFhWnhwoUWP2XKlFGzZs3M/w4A+HdLz589fH19dfLkSV29etXcZhiGduzYoQoVKmRM4gCATJOed0fOnDl16tQpq2XmTpw4YZ4hDgCAh3/nUa1aNZ06dUqXL182t12+fFknT57Uiy++mNHpAcB/CoVu/2EdOnTQqlWrtHr1at28eVOrV69WWFiYOnbsKElq3bq1mjRpopiYGElSkSJFFBQUpM8++0wnTpzQpUuXFBoaKhcXFzVp0iQzDwUAkIHS+v44ePCgPv30U7399tuqVauW7ty5Y/65d+9eZh4KACCDpOXdYWNjo3z58ln92Nvby9XVVfny5Ut2th4AwL9PWv/sUa5cOfn6+uqjjz7SkSNHdPnyZYWGhury5cvq1KlTZh4KACCDpPXd8frrr+v8+fP69NNPderUKUVERGjevHkKCwtT8+bNM/NQAABZRExMjJo0aaLWrVub22rUqKHSpUtryJAh+ueff3Tu3DkNGTJEpUuXVs2aNTMxWwD49zMZD69Dif+UH3/8UXPmzNGFCxeUL18+dezYUS+//LIkqVOnTrp+/boWLVpk/kVSdHS0JkyYoI0bNyo2NlZVqlTRhx9+qIIFC2bmYQAAMlha3h/Tpk3TjBkzkt1O48aNNWTIkAzMHACQWdL6Z4+Hde7cWVWrVlWXLl0yMm0AQCZLz99dffHFF9q0aZNiYmJUsWJFffjhhypatGhmHgYAIAOl9d3x119/adq0aTp8+LDi4+NVoEABvfrqqwoJCZGNDfNFAMB/TZMmTdS5c2fzRC+xsbEKCQmRh4eHZs6caY67evWqxo0bp23btkmSatasqb59+ypXrlyZkjcA/FdQ6AYAAAAAAAAAAAAAAAAAyNL4X1EAAAAAAAAAAAAAAAAAAFkahW4AAAAAAAAAAAAAAAAAgCyNQjcAAAAAAAAAAAAAAAAAQJZGoRsAAAAAAAAAAAAAAAAAIEuj0A0AAAAAAAAAAAAAAAAAkKVR6AYAAAAAAAAAAAAAAAAAyNIodAMAAAAAAAAAAAAAAAAAZGkUugEAAAAAAAAAAAAAAAAAsjQK3QAAAAAAwL/K5s2blStXLl26dMmiPTAwUG+//XbmJPWcOX36tEwmk+bOnZth+woPD7donzFjhsqXL6+7d++me9sjR45UjRo1ZBjGE2b57xYbG6sKFSrIx8dH9+7dy+x08BQNGTJEJpNJJ06cSNO4jHwG4Nk6cOCAcufOrS5dumTI/sLDw2UymTRz5sw0jVu3bp28vLx0+fLlZ5QZAAAAAODfgEI3AAAAAADwr3H9+nW1atVKn3zyifLmzZvqccePH1eHDh1UvHhxOTk5ydvbW//73/+0bNkyi7jAwEAVKFAgxe08rqjkzTfflMlkUkhISLL9ScUlST/ZsmWTt7e32rVrp71796b6eP4NOnbsKEdHR/Xq1Std47dt26ahQ4fqyy+/lMlkerrJ/cvExcXp/PnzOnfuHIVuwL/MjRs3dOPGDZ0+fTqzU3mk+vXrq0aNGmrbti3FyQAAAACAFFHoBgAAAAAA/jWGDx8uGxsbde/ePdVjdu/ercqVK2vnzp3q2bOnFixYoAEDBki6X2z1tNy7d0+rV6+WJK1evVpxcXEpxrZs2VKrVq3SwoUL1bVrV23fvl2+vr5avnz5U8snq7OxsdGYMWM0ffp07dq1K83j33//fTVr1kxVq1Z9Btn9u7i6uurUqVM6deqUXF1dMzudJ3L69GkNGTIkyxf1IHkZ/f2Fh4dryJAhGbKvzOLv768LFy5o1apV5rabN29qyJAh2rNnT+YllowxY8Zo7dq1/6l3HQAAAAAgbSh0AwAAAAAA/woXL17U1KlT1bVrV9nZ2aV63PDhw5UtWzZt2bJF3bt31+uvv64uXbpo9erVCgsLe2r5hYeH69atW6pUqZIiIyO1adOmFGOLFy+uxo0b64033tDHH3+svXv3qmLFiurUqdMTLeX5vKlbt67Kli2b5kKU5cuXa/fu3WkqePyvy549u9zd3TM7jSd2+vRpDR06lEK351RGf3/h4eEaOnRohuwrM+XNm1f29vbmzzdv3tTQoUOzXKFb8eLF1bBhQw0bNoxZ3QAAAAAAyaLQDQAAAAAA/CvMmDFDcXFx6tSpU5rGHTp0SN7e3skW+fj7+z+t9LRy5UrZ2Nho8uTJ5s+p5ejoqHfffVfXr1/Xli1bnlpOz4POnTtr1apVOnPmTKrHTJ48WRUqVFCNGjWeYWYAgKetS5cu2rt3rzZv3pzZqQAAAAAAsiAK3QAAAAAAgN5++215enrq0qVLatGihdzd3eXp6WmeSevmzZtq166dcuTIoVy5cqlXr15KSEiw2EZkZKQ+/PBDeXt7y8HBQSVKlNDo0aOVmJhojrl3757mz5+vgIAAZc+eXa6urvL19bVYUk2S5s6dK5PJpH379mns2LEqVqyYHB0dVaFCBavYJCtXrlSVKlWUJ0+eNB27l5eXjhw58sxnEAoLC5Ofn59q166tEiVKKCwsLE0z1uTPn1+SdPXqVau++vXrq3DhwsmO69y5s5ydnRUVFSVJ2rRpk5o3b668efPK0dFRpUqV0ujRo62+z4eZTKZkZ1b79ttvZTKZrM7fokWL5OfnJ2dnZ+XKlUshISH6559/Hn+gD2nYsKGk1BcG3rhxQ5s3bzaPe9DRo0fVrVs3FS1aVA4ODipYsKDeeecd3bhxwxxTokQJBQQEJLvtl156Sd7e3ubvLTY2VqGhoSpZsqQcHBxUqFAh9evXTzExMeYxSdfyr7/+qrfeektubm4ymUy6efNmqu8HSVqzZo2qV68uJycn5c2bV926ddOff/4pk8mkuXPnWsTu3LlTjRs3Vo4cOeTi4qKAgABt3br1keetQIECCgwMtMp7//79GjBggPLmzascOXKoZcuWun37tgzD0IgRI5Q/f345OzvrlVde0aVLl8zjw8PDZTKZNGvWLC1cuFClS5c2PxfGjBljce1HR0frq6++kp+fn1xdXZUjRw75+/tr27ZtVnneuXNHw4cPV5kyZeTk5KSCBQvqjTfe0NGjR2UymRQUFCRJCgoKkslkSvG+SPr+hg4dquLFi5uvh169eunmzZsWcYULF1bz5s116NAhNW7cWG5ubvLw8NDbb7+t27dvp7j9pHOwZs0ajR8/XoUKFZKbm5saNmyoCxcuSJKmT5+uYsWKycnJSQEBATp27JjFNhITEzVp0iSVK1dOjo6Oyps3rzp06KDz589b7W/x4sXy8fGRo6OjihYtqvHjx6f4jHla9+eOHTvUsGFD8/Xr7++vn3/+2SLm7bffTvF7sLOzMz9XHvX9peV6Kly4sN5++22rfW3ZskUmk0nh4eHm7SXN5mYymWQymZIdJ0lDhgyRyWTS1atX1aVLF+XMmVO5c+fWu+++q/j4eMXGxqp3797KkyeP3N3d1bZtW0VHR5vHJyYmauXKlfrf//6nXLlyydnZWeXLl9fs2bOt9vXnn3+qbt26cnV1lYeHh9566y2dOXPG6hkcGBgoX19fnT9/Xq1atVKOHDnk7u6u1157TRcvXjTHnThxwmJs4cKFVaRIEUlS+/btzccu/f99n9z7MDg42OIZIUm7du1ScHCwXF1dlTt3br333nu6detWsucwtc+levXqyc7OTitWrEh2OwAAAACA/7bUr+MBAAAAAAD+1eLj49WgQQMVLlxYM2bMUFhYmIYOHaoyZcpo4sSJypEjh6ZNm6bffvtNEydOVKFChfTBBx9Ikm7fvq3atWvryJEj+vDDD1WxYkWtXbtW/fv31/nz5zVp0iRJ0pw5c9S7d2+1adNGXbt2VWJioqZMmaJXX31Vf/31lypVqmSRU8+ePXXo0CENHDhQOXPm1Ndff61mzZrp4MGDKlGihDnu0qVL2rVrlz788MM0H3fz5s0VHh6ugIAATZo0Sa+88or5l/7JMQzDXDT2sHv37iXbvnPnTp07d05dunSRJDVt2lRjx47VX3/9JV9f31TlmVQYkzdvXqu+kJAQdezYUTt37rTYXkJCgpYvX64mTZrI1dVVR48eVb169dS4cWONHDlS7u7u+vnnn9W/f385OTmpR48eqcrlcQYPHqxhw4apXr16mj17tv755x+NGjVK27Zt0969e5UjR45Ub6tUqVLy8vLSTz/9lKr81q1bp/j4eKuCDOl+YYiXl5e6d+8ub29vHThwQCNGjNC1a9e0bNkySffP5YgRI3Tp0iWLc33t2jVt2rRJH3zwgUwmkxISEtSoUSNt2LBBXbt2VVBQkLZv366xY8fqyJEjVoV57dq1U5EiRTRr1iz9888/cnBwSPX9sG7dOjVu3FgVK1bU1KlTlZiYqLFjx2rjxo1Wx/jLL7/olVdeUcGCBTVu3DglJCToyy+/VL169bRp06Y0z3LXs2dPXblyRWPHjtXff/+tYcOGKXfu3HJzc9OKFSs0cuRIXblyRYMGDdI777xjtdzv3LlzdfjwYX3yySfy8vLS7Nmz1a9fP0VGRmrEiBGSpNDQUE2dOlUdOnRQnz59FB0drc8//1z169fXyZMn5enpKel+QVxAQIB27dqlbt26qU6dOrp8+bLWrl2rqVOnatWqVebCvJEjR6pChQpydnZO9rgSExPVpEkTbdu2TX369JGPj4/+/vtvjRkzRhs2bNDvv/8uV1dXc/zff/+tmjVrqkGDBpo1a5YOHz6s0NBQmUwmzZkz55HncMyYMTpx4oQ+/fRTRUVFacCAAXrnnXfUpEkTDRs2TIMHD5ZhGBo4cKCaN2+uffv2mcd27txZ8+bN0/vvv6/atWsrIiJC48aNk5+fn/744w9zAeyMGTPUuXNnNWzYUB9//LGuXLmioUOHysbG+v9zflr354YNG/Tyyy+revXqmjx5suzt7fX999+rcePGmj17dopFYylJzfeXmuspNZKKphcuXKhFixaZi0sLFiz4yHEtWrRQfHy8pkyZou3bt2vSpEkqWrSo/vrrL50+fVoTJ07U4cOHNWLECOXIkcP87lu3bp2aN2+uFi1a6IsvvpCDg4O+/fZbdezYUblz59Yrr7wiSdq/f78CAwPl6emp8ePHy8XFRVOmTEm2cFe6X9hbs2ZNlSxZUl9//bUuXLigoUOHqlWrVikujz179mydPn1aHTt2VK9evVSvXr1Un7cH7d27V/7+/ipUqJAmTJggBwcHjRs3TkuXLrWKTctzydXVVdWqVdNPP/2k8ePHpys3AAAAAMC/mAEAAAAAAP7z2rVrZ0gyXn31VSMxMdEwDMNITEw0vLy8jOzZsxtNmjQxEhISzPG+vr6Gr6+v+fMHH3xgSDIWLVpksd0uXboYJpPJOHHihGEYhrFjxw7j1KlTFjHnz583TCaT8emnn5rb5syZY0gycubMaZw5c8bcfuXKFcPW1tYYPHiwxTbWr19vSDKmT5+e4jEGBAQY7dq1s2qPi4szWrdubUgyJBkVKlQwli1bluI2kuIe9XP8+HGLcZ988okhydi1a5dhGIaxefNmQ5IxcOBAi7hTp04l256QkGDUrl3b8PDwMO7evWuV140bNwx7e3ujX79+yZ6X5cuXG4ZhGJcvXzY2bdpkEZOYmGiULl3aqFOnjlUec+bMMbdJsjrvhmEY8+fPNySZv9c9e/YYNjY2RqNGjYz4+Hhz3O+//25IsvieH9zXw3k9yN/f3/D09Eyx/0FJ5/rYsWNWfUuWLDFf30m6dOliZMuWzYiNjTUMwzAOHTpkSDK+/vpri7iZM2cakozdu3cbhmEYkyZNMiQZo0aNsogbNWqUIcnYuHGjYRj/fy1XrVrVuHfvnkVsau+HihUrGl5eXkZkZKS57datW0bx4sUtvqeYmBgjX758Rv78+Y2rV69axObJk8fiO35Y/vz5jYCAAPPnpLy9vb2NGzdumNtbtmxpuLi4GEWKFDGuXbtmbu/bt69ha2trznHTpk2GJCNbtmzGvn37zHFxcXFG5cqVDXt7e+PKlSuGYRjGxo0bzf+eJOl6mT17trmtT58+hiRj3rx5VvmfO3fOYr+Pup4MwzBmzZplSDLCwsIs2nfv3m3Y2tpa3IPe3t6GJKNPnz4WsT169DAcHR2TvScfzMXNzc3iOdavXz/DxsbG8PT0NP7++29z+5dffmlIMg4cOGAYhmFs2LDBkGRMmjTJYrvnz5833N3djbfeesswDMOIjIw0smfPbgQEBFg8p/fv32/Y2dlZPJNSe38m9wx4UHx8vFG4cGHDz8/PiIuLs+hr1qyZ4erqar5u2rVrZ3h7eye7nYef5yl9f2m5nry9vZN91ic9dx/c9uDBg43U/BV5Uly1atWMmJgYc7ufn5+RPXt2o2rVqhbXQfPmzY08efKYPx89etTYs2ePxTbv3Llj5MiRw2jbtq257ZVXXjGcnJwsrpfY2Fijdu3aVs/gpPfRG2+8YfG9jx8/3uKZfPz4cauxKX2/Sff9w88lwzCMevXqWTwjAgMDjVy5chmXL182t0VFRRnFihUzJBkzZswwDCN9z6U2bdoYNjY2xp07d6z6AAAAAAD/bSxdCgAAAAAAzIYPH26ezcxkMqlq1armmXIenBnIz89Px48fl3R/1rDZs2fLz89Pb775psX2unbtKsMw9NNPP0mSqlWrZrWEnZOTk9zc3Mwzlj1owIABKlSokPlz7ty5VapUKR04cMAi7vLly5KkF154Ic3HbGdnp/nz52vJkiUqVaqU9u/fr9dff13169fX9evXreLz5MmjzZs3J/vTvn37ZPexcuVKeXl5qXLlypKkmjVrKnfu3CkuxxkXF6eoqCidP39e4eHhatKkibZs2aIvvvhCjo6OVvE5cuTQ//73P/3www8W7UuXLlWOHDn08ssvm3N/eKazmJgYFSpUKNnznx6zZs0yzzhma2trbq9evbp8fHxSXHr2UfLmzasrV66kKvZR10Lz5s0tZutLSEiQl5eX4uLizNsvU6aMfHx8kj2XZcqUMc+yNmPGDBUoUEB9+/a1iOvataskWR1nnz59lC1bNou21NwPp06d0r59+9S2bVu5u7ub49zc3DRgwACLsT/99JMuXLigTz75RLly5bKIbdWqlTZv3qzIyEir8/Ioffv2tZjhq1q1aoqOjlbv3r3l4eFhbvfz81NCQoL+/vtvi/EhISGqUKGC+bOdnZ3ef/993bt3zzwjXVBQkHLnzm0xLmkJ4qTzkJCQoJkzZ6pKlSpq27atVZ5JM5ul1vz58+Xt7a0mTZpYtFeqVEl16tTRggULrPIZNmyYRVvt2rUVExOjEydOPHJfnTt3tniOVatWTYmJiWrVqpV5CUnp/jmUZH62zp8/X46OjurUqZPF9vLly6dmzZpp6dKlio2N1erVqxUZGakPPvjA4jldvnx5NW7c2GLs07o/t2zZotOnT6tr166ys7NcNKRHjx6KiopK9XLDaZGa6+lZ+vTTT+Xg4GD+XK1aNUVGRmrw4MEWz2Y/Pz9duXLFvAxuyZIl5ePjY7Gt+Ph4eXl5ma/x2NhYrVmzRq+88orF9WJvb2917SXJli2bJk6caPG9165dW5Ks3pNPU0REhH799Ve1bdvWYrlwFxcXde/e3SI2Pc+lvHnzKjExMdmlugEAAAAA/20UugEAAAAAAEn3f0Fdrlw5i7acOXPKxcVF5cuXt2q/ffu2JOnYsWO6efOmgoKCFBUVZfGTVGz0YCHI3r171a9fP/n7++uFF16Qh4eHbt26pbi4OKucGjRoYNXm4eGhGzduPPHxPqx58+Y6dOiQFi9erEKFCmn9+vWqW7euVV729vaqXbt2sj8PFickOXXqlPbv328uNpMkGxsbNW7cWAcOHNDJkyetxowZM0Zubm4qUKCAgoKCdPr0aa1YsSLZAp8kISEhOnHihHnZw8TERC1btkyvv/667O3tzXEXL17U559/roYNG6pw4cJycXHR2rVrkz3/6fHHH3+oQIECKlCggNX1ULBgwccWBT1LMTExmjNnjt544w2VKVNGzs7OGjJkiCRZHH9ISIjCw8PNhY43btzQhg0bFBISIkm6c+eODhw4IH9/f8XExFgco52dnbJnz251nA8W5zzocffDwYMHJclcJPmgqlWrWnz+448/JN0vdHn43OfLl0+GYSR7vT1KUvFVkpw5c0q6XxiVXHvScyHJw3GSzM+TB4vitm7dqh49eqh69erKmTOnihcvLun/v5djx44pMjJSL730UpryT8nevXtVsWLFZPt8fHx0+vRpc5GSJPn7+1sto5lU6Pe451F6z+HevXtVokQJOTk5JZtjbGysDh8+rN27d0u6X3T1sLJly1p8flr35969eyUp2XOYVNCVlNfTlNrr6Vl5kvvh5MmTGjp0qOrWrav8+fPL3d1dhw8fNl/jx48f171791J1ryepUKGCvLy8LNpSe10+iT179sgwjFRfc9LTfS4BAAAAAP67KHQDAAAAAACS7s+W9uBsV9L9gqzk2m1tbZWYmChJ5mKgzz77TG5ubhY/SbMsRUVFSbo/C5avr6/CwsLk7++vMWPG6Ndff1W+fPmSzcnNzc2qzdbWVgkJCRZtSQV1SbN5pZeNjY1atmypPXv2qGrVqtq7d6++/fbbJ9pm0qxGfn5+ioiIMP/UqlXLov9Bbdu21ebNm/XHH3/o3LlzOnjwoJo2bfrI/TRp0kQuLi5aunSpJOm3337T5cuX1apVK3PMn3/+qYoVK2rMmDEqVqyY+vfvb55B6Gm5fv26zp07Z3UtuLm56ccffzRfC2lx6dIli1mDHiWla+H69euqUaOGunTpIjs7O3Xp0kXff/+9udDtQW+++aYSEhK0YsUKSfe/o7i4OHOh240bN2QYhhYtWpTscUZGRlodp6urq9V+UnM/JM109OCsakkenOEt6Ril+8UvD+fUr18/SUrz+X/4vCfNHPVwe9LsYA/fm8l9b9mzZ5d0fwYrSRo8eLBq166tbdu2qWHDhvryyy+1bdu2ZI8trTO3peTWrVvJfifS/39XD84yldKzSLI+5oel9xymNsdr165JksUMe0kenkXwad2ft27dssgjpdyettRcT89Ser/Ln376SRUrVtSsWbNUuXJlDR06VBs2bJCvr695TFru9SRPcl0+ibRec1LankuXLl0y//cHAAAAAAAPsnt8CAAAAAAAQMqSfgHfq1cvNWvWLNmYvHnz6tKlS+rZs6eCg4MVFhZm9cvwJ5E0o0/Skn9PKmfOnBo9erSCg4O1e/fuFJckTY2kYqnOnTun2P/BBx9YtBUsWNC8/FxqOTs7q2nTpvrhhx80bNgwLV26VJ6engoKCjLHdOnSRfb29tq9e7fF0p7z5s177Pbt7OySLZy4c+eOxWd3d3eVLFlSs2bNSlP+j3LixIkUZ996WNLMacePH1eJEiXM7aGhodq3b582btyogIAAc3tyxZHe3t6qUaOGfvjhB3Xo0EFLly5VtWrVzLOMJRWXtGzZUu+//36yeSQV36QktfdD0jKJyS2jGx0dbfE56V785ZdfrGYfS5LSzHLPimEYVm0RERGS7hfJ7N27V8OHD1enTp00ffp0c1HtwzMMJhVPXbp06anklT17dqvZ55IktT/uO3zWUptjUmHU9evXrWb3eriA6Gndn0nnJrn8Hj5/2bJlS/bZce/evTQXYz3uenrU/h5+VmWUe/fuqVOnTipRooS2bt0qFxcXc9+Ds22m5V5/VpKeQymdv6R8H7zmHpbcNSel7bl04sQJFS9ePNnZDAEAAAAA/20UugEAAAAAgCdSqlQpOTk5KSEh4ZHFWatWrdLdu3fVsWNHi6Key5cv68KFC0+UQ968eVWlShWFh4eneWxCQoJ5BpwHJc0kY2eX/r8+uXbtmrZs2aIGDRqoV69eVv1fffWVVq9eratXrz6VmWtCQkK0cOFCHTp0SMuWLVPLli3NMw7dvn1bu3fvVrdu3SyK3KT7y9A9jpeXl65evWrVvnXrVovPlSpV0g8//KBatWpZzQSYHkePHtXFixfVv3//VMXXr19fdnZ2Cg8Pt1gu9tdff1WZMmUsityklI89JCREffr00dmzZ7Vu3TqNHj3a3Ofu7q6iRYvqzp07aS5ITLJjx45U3Q9JxXWHDh2y2sbhw4ctPleqVEnS/Wu3SpUq6crraTt16pRV286dOyXdX/byt99+k2EY6tKli8X18vD3UqpUKTk4OGjLli1PJS8fHx/z8psP2717twoXLpzszFoZycfHRwsXLtSdO3esCoR2794tR0dHlSlTRgULFpR0/xp5uNBt//79Fp+f1v2ZtDzp3r17rZbVTFqyNGkJTi8vL/MMYA96+NmRGo+7npL2l5pnVUY5evSoIiIi1KdPH4sit7i4OB08eNB83xYrVkxS6u71ZyXp+rl69ao5H+n++2P//v3m7/rBa+5hyV1zUuqfS1FRUfrzzz/VvXv3dB0DAAAAAODfjaVLAQAAAADAE7G3t1erVq00b948nT171qr/yJEjioyMNM8E8+AMMIZh6MMPP3wqeTRt2lS7du3SlStX0jSuUaNGyc5K9NNPP0mSVWFUWvz0009KSEjQO++8o4YNG1r9dOnSRQkJCVq1alW69/GgBg0ayMPDQ71799bFixfNS21K95ezs7GxsZqBZ9asWckWKzysdOnS2rRpk8WMSjt37tSCBQss4tq3b6/IyEhNmjTJahsxMTHat29fmo5pzZo1kvTYpVuT5MyZU/7+/uZxSezt7RUZGWkxU9HBgwc1e/bsZLfzxhtvmL+7+Ph4tWzZ0qK/ffv2Wr16tf766y+rsefPn9e5c+cemWdq74eKFSuqUKFCmjVrlsWsTnFxcRo3bpxF7KuvvqqcOXMqNDQ02Zmv/vjjj0fm9CzMnDlTMTEx5s8xMTGaOnWq8uXLp1q1aiV7HuLi4qwKGx0dHdWiRQuFh4crLCzMaj/Hjh0zx0n3Z9F6lDZt2uiff/7RsmXLLNp37typLVu26K233krDUT4bbdq0UWxsrKZOnWrRnpR3s2bN5ODgoHr16kmS1T33559/at26dRZtT+v+rFWrlooUKaIpU6ZYzb43YcIEubq6mpdELl26tO7evavff//dYl8ff/yx1XYf9/097npK2t/vv/9uEXfmzBlNmDAhzft7GpK7xiVp2LBhFsu75syZU7Vr19bSpUstil0Nw7AotH0aUjruUqVKSZI2btxo0T5o0CCLmdrKlSsnT09PzZ492+K5dPPmTU2ZMsVibFqfSxs2bFB8fLxeffXVtB8YAAAAAOBfjxndAAAAAADAE/vss88UHh6uypUrq1u3bipfvrzu3r2rDRs26LvvvtOhQ4dUvXp15c2bVx9//LGio6OVL18+zZs3TydOnDAvS/gk3nnnHY0aNUozZ85MtoAiJWvXrlXZsmXVsWNHlStXTnfv3tWWLVs0Z84cNWjQwFyskR4rV66Ui4uLxcxiD3rppZeUPXt2rVy58omWR02SLVs2NWvWTDNmzFCxYsXk5+dn7nN2dlb9+vX13XffqUCBAnrxxRf122+/afbs2SpTpsxjl/V777339Nprr+mNN95Q8+bN9ffff2vkyJGqVq2atm/fbo6rXbu2evbsqd69e2v79u0KDg5W9uzZtW/fPs2bN09t27ZN9TKkkjR9+nQ1adJE3t7eqR7TvXt3vf766/r9999Vo0YNSfcL5T7++GM1b95cISEhOnPmjEaMGKGyZctq165dVtvImzev6tatq7Vr16pu3bpWM2X17dtXP/74o+rUqaOuXbuqatWqMgxDW7du1fz587VixQoVKFAgxRxTez/Y2Njos88+U6tWrRQYGKgePXro7t27mjt3rm7evClJ5hkJ3dzcNH36dL355puqWbOm3nrrLeXLl09nzpzR4sWLZW9vr82bN6f6PD4N58+fV/Xq1fXuu+/KxcVFU6ZM0cmTJ7VkyRLZ2dmpfv36cnJy0jvvvKMBAwbIxcVFX331VbKFR2PHjtXWrVv1+uuv67333pO/v7/u3r2rpUuXysPDQ3PnzlWxYsVkZ2en0NBQ7d+/X7t379agQYPMBTxJ2rVrpwULFqhNmzbas2ePKlWqpJMnT2rMmDEqXbq0+vXrl1GnKEVBQUF6++231a9fP509e1b+/v66ePGixo4dKzc3N40aNUrS/aKjNm3aaP78+WrVqpWaNWumU6dOafjw4XrhhRcslnt9Wvenra2tpk2bpkaNGqlu3bp65513ZG9vr0WLFiksLEwzZ85Uzpw5Jd2/9/Lly6eQkBB9/PHHcnJy0oQJE8xLdT4ope8vyeOuJ+n+MtGzZ8/Wyy+/rA4dOuj69esKDQ1V2bJlLZ5V0v2iuKQxZcuW1Z9//qklS5ak8Zt6tOLFi6ts2bKaMGGCnJycVKZMGa1YsUJr1641z4yWZOTIkapXr54CAwP10Ucfyc7OTkuWLNHff/8tScnOPpoeefPmVc6cOfXll1/q1q1bOnbsmEJCQhQUFKSmTZtq2LBhSkxMVIkSJbRkyRJt3rxZhQsXNo+3tbXViBEj1LFjR9WtW1c9evRQbGysPv/8c6vCx7Q+l6ZNmyYfHx/5+/s/lWMFAAAAAPzLGAAAAAAA4D+vXbt2hre3d6rbBw8ebDz81wpXr141evToYRQqVMjIli2b4enpadSqVcuYM2eOkZiYaBiGYezcudOoW7eu4e7ubuTKlcto166dceHCBfO/J5kzZ44hyTh16pTVvgMCAoyAgIBkj6NXr15GoUKFjHv37iU77sF9JNmyZYvRvHlzI3/+/IadnZ3h7OxsVK5c2Rg7dqzVdgICAoz8+fMnu+8Hz8vx48eNu3fvGi4uLsYbb7yRYrxhGEabNm0MJycnIzo62jh16pQhyRg4cOAjxzzKxo0bU9xGRESE0aZNG+OFF14wXFxcjODgYOOPP/4wmjVrZvE9J+UxZ84ci/FTp041ihUrZtjb2xtly5Y15s+fb0yePDnZ72rOnDmGr6+v4eTkZLi7uxtly5Y1evXqZVy9etUiLmlfmzZtssp3/fr1hiTjr7/+StM5SExMNCpXrmyEhISY2+Lj441PP/3UKFy4sOHo6GhUqlTpkfkbhmHMnj3bkGTMmDEj2f1ER0cbgwYNMkqUKGHY29sbefLkMapVq2Z88cUXRlxcnPk8pLT91N4PhmEYS5YsMapWrWo4ODgYHh4eRtu2bY3Nmzcbkozly5dbxG7ZssVo2LChkT17dsPJyckoWrSo0bJlS2P//v0pnrP8+fNb3Fcp5Z1S+6ZNmyy+x6TP48ePNzp16mR4eHgYjo6ORrVq1YywsDCLsevWrTP8/PwMFxcXw8vLy+jRo4dx/vx5Q5IxePBgi9hr164Zffv2NYoXL27Y29sb+fLlM15//XXj2LFj5pgvv/zSKFCggOHo6Gj4+fkZFy9eTPaY7969awwaNMgoWrSoYW9vb+TPn9/o1q2bce3aNYs4b2/vZJ8dDx9zavtTak/uvouPjzfGjx9vlClTxnyNtWnTxjh79qzF2NjYWOOTTz4xP38LFy5sjBkzxujdu7f5mfSgx92fKT0DHrZt2zbjpZdeMtzc3AxnZ2ejZs2aVt+vYRjGgQMHjKCgIMPJycnInTu38d577xkXLlxI9jtO7vtLy/VkGIaxbNkyo1y5coa9vb1RtGhR44svvjDCwsKsznt8fLzRoUMHw8PDw3B1dbV4ZjwouXfeo9ofvk9OnDhhNG3a1MiZM6eRPXt247XXXjOOHj1qVK1a1ep9tmnTJqN27dqGs7Oz4e7ubjRt2tQ4fPiwIcn44osvzHEpvQsf/u6OHz+e7HlesmSJUaxYMcPBwcEoX768ceDAAcMwDOPGjRtG27ZtjRw5chguLi5G48aNjWPHjiWb6/z5840KFSoY9vb2Ru7cuY1OnToZCxYsSPa5mZrn0rFjxwyTyWT88MMPVscFAAAAAIBhGIbJMJKZLxwAAAAAAOA5dP36dfn4+Kh379764IMPLPoCAwNVuHBhzZ07N3OSQ7JOnz6tIkWKaNOmTQoMDDS3JyYm6sUXX1TVqlU1bdq0NG9327ZtCgoK0tatW+Xr6/sUM846fv75ZzVq1Ej79+9X+fLlMzsdC+Hh4QoKCtKMGTPUqVOnzE4Hz7n/+vV06NAhlStXTqtWrVLjxo0zO51n5rXXXtPdu3e1evVqmUymzE4HAAAAAJAF2WR2AgAAAAAAAE+Lh4eHFixYoBEjRlgs14fnz6xZsxQTE6MJEyaka3zNmjU1ePBg9ejRQ8/7/+cZERGRbPuUKVNUqFAhlStXLoMzAvAsXL58WYmJiVbtU6ZMkbOzswICAjIhq4yxfv16bd++Xd988w1FbgAAAACAFNlldgIAAAAAAABPU506dXTt2jWr9vDw8IxPBo9VuHDhZAvR3nnnHb3zzjtPtO0BAwZowIABT7SNrGDs2LHasmWLmjVrpqJFiyoyMlJLly7VmjVr9P3331MUAvxLfP/99/r6668VEhKiUqVKKSYmRqtXr9aiRYs0btw4ubm5ZXaKz0xwcLAuXryY2WkAAAAAALI4Ct0AAAAAAACALKxFixb6+++/NWHCBF25ckWurq7y9fXVL7/8ovr162d2egCekvr16+uPP/7QjBkzFBERIUdHR1WsWFGLFy9Wy5YtMzs9AAAAAAAyncl43tduAAAAAAAAAAAAAAAAAAD8q9lkdgIAAAAAAAAAAAAAAAAAADwKhW4AAAAAAAAAAAAAAAAAgCyNQjcAAAAAAAAAAAAAAAAAQJZGoRsAAAAAAAAAAAAAAAAAIEuj0A0AAAAAAAAAAAAAAAAAkKVR6AYAAAAAAAAAAAAAAAAAyNIodAMAAAAAAAAAAAAAAAAAZGkUugEAAAAAAAAAAAAAAAAAsjQK3QAAAAAAAAAAAAAAAAAAWdr/AYJROTL4NLo4AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 2500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# asset 실행\n",
    "train2_asset_structure=run(step, pipeline, alo.asset_structure)\n",
    "\n",
    "# train2 asset의 결과 dataframe은 train2_asset_structure.data['dataframe']으로 확인할 수 있습니다.\n",
    "train2_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0158d579-4e19-401f-8a1b-ac06a92f0e9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [5] Output asset  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd463df-8ac2-4eeb-a020-49f89a05c5a1",
   "metadata": {},
   "source": [
    "AI content asset sequence 표준화에 따라 GCR 2.0.0부터는 train 및 output pipeline 모두 input asset으로 시작해 output asset으로 종료하도록 변경되었습니다.   \n",
    "Output asset은 pipeline의 산출물들을 올바른 위치에 옮겨주는 asset입니다. AI advisor 규약에 따라 내부적으로 미리 지정된 위치를 이용하므로 사용자가 별도로 입력해 줘야 할 parameter는 없습니다.   \n",
    "<br />\n",
    "\n",
    "#### 주요 Parameter\n",
    "- 사용자가 설정해 줘야 할 parameter 없음\n",
    "\n",
    "#### Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a20b2f60-8fb3-4115-9006-8bee0b3c5ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - readiness(1) - train1(2) - preprocess(3) - train2(4) - output(5))\n",
    "step = 5\n",
    "alo.asset_structure = copy.deepcopy(train2_asset_structure)\n",
    "alo.asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "alo.asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4230191c-ec22-4e85-9a18-82817a90e2fb",
   "metadata": {},
   "source": [
    "#### Output asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "025f4f8c-f430-4c28-b8e0-e27e7fe1a0db",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2024-02-16 06:23:56,012][ASSET][INFO][train_pipeline][output]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2024-02-16 06:23:56\n",
      "- current step      : output\n",
      "- asset branch.     : output_dev\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : 2.1\n",
      "- load envs. keys   : dict_keys(['project_home', 'solution_metadata_version', 'artifacts', 'alo_version', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'pipeline', 'step', 'num_step', 'asset_branch', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys([])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'center_node_column', 'columns_map', 'preprocess'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2024-02-16 06:23:56,014][ASSET][INFO][train_pipeline][output]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2024-02-16 06:23:56\n",
      "- current step      : output\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'center_node_column', 'columns_map', 'preprocess'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2024-02-16 06:23:56,016][PROCESS][INFO]: ==================== Finish pipeline: train_pipeline / step: output\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_000_nan</th>\n",
       "      <th>EMB_001_nan</th>\n",
       "      <th>EMB_002_nan</th>\n",
       "      <th>EMB_003_nan</th>\n",
       "      <th>EMB_004_nan</th>\n",
       "      <th>EMB_005_nan</th>\n",
       "      <th>EMB_006_nan</th>\n",
       "      <th>EMB_007_nan</th>\n",
       "      <th>EMB_008_nan</th>\n",
       "      <th>EMB_009_nan</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_126_nan_shapley</th>\n",
       "      <th>EMB_127_nan_shapley</th>\n",
       "      <th>target_encoded_nan</th>\n",
       "      <th>pred_target_encoded_nan</th>\n",
       "      <th>pred_target_encoded_nan_best0</th>\n",
       "      <th>pred_target_encoded_nan_best1</th>\n",
       "      <th>pred_target_encoded_nan_best2</th>\n",
       "      <th>prob_0</th>\n",
       "      <th>prob_1</th>\n",
       "      <th>train_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.074153</td>\n",
       "      <td>-0.055988</td>\n",
       "      <td>-0.052818</td>\n",
       "      <td>0.005892</td>\n",
       "      <td>0.015515</td>\n",
       "      <td>-0.021260</td>\n",
       "      <td>0.026566</td>\n",
       "      <td>0.017620</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>-0.001954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034957</td>\n",
       "      <td>0.122719</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.999946</td>\n",
       "      <td>1th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.068774</td>\n",
       "      <td>-0.049618</td>\n",
       "      <td>-0.058634</td>\n",
       "      <td>0.009538</td>\n",
       "      <td>0.005547</td>\n",
       "      <td>-0.021479</td>\n",
       "      <td>0.025170</td>\n",
       "      <td>-0.010637</td>\n",
       "      <td>0.005298</td>\n",
       "      <td>0.009066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006115</td>\n",
       "      <td>-0.162118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011164</td>\n",
       "      <td>0.988836</td>\n",
       "      <td>0th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.033640</td>\n",
       "      <td>-0.032654</td>\n",
       "      <td>-0.017567</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>-0.008067</td>\n",
       "      <td>0.017446</td>\n",
       "      <td>0.049978</td>\n",
       "      <td>-0.011376</td>\n",
       "      <td>-0.024331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017734</td>\n",
       "      <td>0.073409</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.999555</td>\n",
       "      <td>2th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.055558</td>\n",
       "      <td>-0.024381</td>\n",
       "      <td>-0.035403</td>\n",
       "      <td>0.003343</td>\n",
       "      <td>0.007629</td>\n",
       "      <td>-0.023969</td>\n",
       "      <td>0.032309</td>\n",
       "      <td>0.009788</td>\n",
       "      <td>-0.018074</td>\n",
       "      <td>-0.006553</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000854</td>\n",
       "      <td>0.174334</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.521614</td>\n",
       "      <td>0.478386</td>\n",
       "      <td>1th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.019305</td>\n",
       "      <td>-0.013052</td>\n",
       "      <td>-0.031120</td>\n",
       "      <td>0.008044</td>\n",
       "      <td>-0.031994</td>\n",
       "      <td>-0.012280</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>-0.081808</td>\n",
       "      <td>0.015399</td>\n",
       "      <td>0.039381</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009817</td>\n",
       "      <td>-0.097222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.606196</td>\n",
       "      <td>0.393804</td>\n",
       "      <td>3th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001473</td>\n",
       "      <td>-0.021499</td>\n",
       "      <td>-0.013175</td>\n",
       "      <td>0.013053</td>\n",
       "      <td>-0.029639</td>\n",
       "      <td>-0.006258</td>\n",
       "      <td>-0.016850</td>\n",
       "      <td>-0.078868</td>\n",
       "      <td>0.014410</td>\n",
       "      <td>0.029548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005287</td>\n",
       "      <td>0.088159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.717698</td>\n",
       "      <td>0.282302</td>\n",
       "      <td>3th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.045707</td>\n",
       "      <td>-0.011426</td>\n",
       "      <td>-0.021226</td>\n",
       "      <td>-0.004355</td>\n",
       "      <td>0.013403</td>\n",
       "      <td>-0.017541</td>\n",
       "      <td>0.031196</td>\n",
       "      <td>0.036596</td>\n",
       "      <td>-0.013267</td>\n",
       "      <td>-0.011521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050397</td>\n",
       "      <td>-0.216051</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014050</td>\n",
       "      <td>0.985950</td>\n",
       "      <td>0th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.060994</td>\n",
       "      <td>-0.010418</td>\n",
       "      <td>-0.041536</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.003026</td>\n",
       "      <td>-0.027531</td>\n",
       "      <td>0.046743</td>\n",
       "      <td>0.006817</td>\n",
       "      <td>-0.020207</td>\n",
       "      <td>-0.004573</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037533</td>\n",
       "      <td>-0.238891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034753</td>\n",
       "      <td>0.965247</td>\n",
       "      <td>2th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.063151</td>\n",
       "      <td>-0.013606</td>\n",
       "      <td>-0.026188</td>\n",
       "      <td>-0.015054</td>\n",
       "      <td>0.031623</td>\n",
       "      <td>-0.023023</td>\n",
       "      <td>0.047041</td>\n",
       "      <td>0.071892</td>\n",
       "      <td>-0.024079</td>\n",
       "      <td>-0.023515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003898</td>\n",
       "      <td>-0.198318</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006411</td>\n",
       "      <td>0.993589</td>\n",
       "      <td>0th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.010719</td>\n",
       "      <td>0.022915</td>\n",
       "      <td>-0.000830</td>\n",
       "      <td>0.012984</td>\n",
       "      <td>-0.020518</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>-0.043937</td>\n",
       "      <td>-0.001184</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074513</td>\n",
       "      <td>0.022036</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.931072</td>\n",
       "      <td>0.068928</td>\n",
       "      <td>1th_test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EMB_000_nan  EMB_001_nan  EMB_002_nan  EMB_003_nan  EMB_004_nan  \\\n",
       "0    -0.074153    -0.055988    -0.052818     0.005892     0.015515   \n",
       "1    -0.068774    -0.049618    -0.058634     0.009538     0.005547   \n",
       "2    -0.033640    -0.032654    -0.017567     0.003200     0.020300   \n",
       "3    -0.055558    -0.024381    -0.035403     0.003343     0.007629   \n",
       "4    -0.019305    -0.013052    -0.031120     0.008044    -0.031994   \n",
       "5     0.001473    -0.021499    -0.013175     0.013053    -0.029639   \n",
       "6    -0.045707    -0.011426    -0.021226    -0.004355     0.013403   \n",
       "7    -0.060994    -0.010418    -0.041536     0.001866     0.003026   \n",
       "8    -0.063151    -0.013606    -0.026188    -0.015054     0.031623   \n",
       "9     0.010719     0.022915    -0.000830     0.012984    -0.020518   \n",
       "\n",
       "   EMB_005_nan  EMB_006_nan  EMB_007_nan  EMB_008_nan  EMB_009_nan  ...  \\\n",
       "0    -0.021260     0.026566     0.017620     0.001553    -0.001954  ...   \n",
       "1    -0.021479     0.025170    -0.010637     0.005298     0.009066  ...   \n",
       "2    -0.008067     0.017446     0.049978    -0.011376    -0.024331  ...   \n",
       "3    -0.023969     0.032309     0.009788    -0.018074    -0.006553  ...   \n",
       "4    -0.012280     0.001667    -0.081808     0.015399     0.039381  ...   \n",
       "5    -0.006258    -0.016850    -0.078868     0.014410     0.029548  ...   \n",
       "6    -0.017541     0.031196     0.036596    -0.013267    -0.011521  ...   \n",
       "7    -0.027531     0.046743     0.006817    -0.020207    -0.004573  ...   \n",
       "8    -0.023023     0.047041     0.071892    -0.024079    -0.023515  ...   \n",
       "9     0.000423     0.004345    -0.043937    -0.001184     0.011903  ...   \n",
       "\n",
       "   EMB_126_nan_shapley  EMB_127_nan_shapley  target_encoded_nan  \\\n",
       "0             0.034957             0.122719                   1   \n",
       "1             0.006115            -0.162118                   1   \n",
       "2             0.017734             0.073409                   1   \n",
       "3            -0.000854             0.174334                   1   \n",
       "4            -0.009817            -0.097222                   0   \n",
       "5             0.005287             0.088159                   0   \n",
       "6             0.050397            -0.216051                   1   \n",
       "7            -0.037533            -0.238891                   0   \n",
       "8             0.003898            -0.198318                   1   \n",
       "9             0.074513             0.022036                   0   \n",
       "\n",
       "   pred_target_encoded_nan  pred_target_encoded_nan_best0  \\\n",
       "0                        1                              1   \n",
       "1                        1                              1   \n",
       "2                        1                              1   \n",
       "3                        0                              0   \n",
       "4                        0                              0   \n",
       "5                        0                              0   \n",
       "6                        1                              1   \n",
       "7                        1                              1   \n",
       "8                        1                              1   \n",
       "9                        0                              0   \n",
       "\n",
       "   pred_target_encoded_nan_best1  pred_target_encoded_nan_best2    prob_0  \\\n",
       "0                              1                              1  0.000054   \n",
       "1                              1                              1  0.011164   \n",
       "2                              1                              1  0.000445   \n",
       "3                              0                              0  0.521614   \n",
       "4                              0                              0  0.606196   \n",
       "5                              0                              0  0.717698   \n",
       "6                              1                              1  0.014050   \n",
       "7                              1                              1  0.034753   \n",
       "8                              1                              1  0.006411   \n",
       "9                              0                              0  0.931072   \n",
       "\n",
       "     prob_1  train_test  \n",
       "0  0.999946    1th_test  \n",
       "1  0.988836    0th_test  \n",
       "2  0.999555    2th_test  \n",
       "3  0.478386    1th_test  \n",
       "4  0.393804    3th_test  \n",
       "5  0.282302    3th_test  \n",
       "6  0.985950    0th_test  \n",
       "7  0.965247    2th_test  \n",
       "8  0.993589    0th_test  \n",
       "9  0.068928    1th_test  \n",
       "\n",
       "[10 rows x 264 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asset 실행\n",
    "output_asset_structure=run(step, pipeline, alo.asset_structure)\n",
    "\n",
    "# output asset의 결과 dataframe은 output_asset_structure.data['dataframe']으로 확인할 수 있습니다.\n",
    "output_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e992948d-2fc0-4d6b-8107-833b7fd67eb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **3. Inference Workflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f82f0-1e6c-4af5-b842-78d428c1e6f3",
   "metadata": {},
   "source": [
    "#### GCR의 Inference Workflow 구성은 다음과 같습니다.\n",
    "> **[0]** Input asset : *사용자가 지정한 경로로부터 데이터를 Import*   \n",
    "> **[1]** Readiness asset : *Inference 실시 전 입력 데이터의 오류를 검사하여 다음 step을 진행할 지 결정*   \n",
    "> **[2]** Inference1 asset : *Inference set 데이터를 토대로 그래프를 구성하고 필요한 임베딩 추출*   \n",
    "> **[3]** Preprocess asset : *(필요시) 결측치 처리 및 라벨 인코딩*   \n",
    "> **[4]** Inference2 asset : *Train Workflow에서 선택된 베스트 모델을 활용해 라벨 추론*   \n",
    "> **[5]** Output asset : *추론 결과 생성된 산출물을 올바른 경로에 저장*  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6707f7c-816f-4c29-86af-be1f2d990f4a",
   "metadata": {},
   "source": [
    "#### Inference Workflow Setup\n",
    "아래 코드를 실행하여 Inference Workflow에 필요한 라이브러리를 먼저 설치 해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e052e1b-3478-47f8-8c6d-62aa4474b591",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m[2024-02-18 11:46:51,238][PROCESS][WARNING]: You did not write any << s3_private_key_file >> in the config yaml file. When you wanna get data from s3 storage, \n",
      "                                 you have to write the s3_private_key_file path or set << AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY >> in your os environment. \n",
      "\u001b[0m\n",
      "\u001b[94m[2024-02-18 11:46:51,359][PROCESS][INFO]: Successfuly removed << /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/input/inference/ >> before loading external data.\u001b[0m\n",
      "\u001b[94m[2024-02-18 11:46:51,378][PROCESS][INFO]: << ../../sample_data/test >> may be relative path. The reference folder of relative path is << config/ >>. \n",
      " If this is not appropriate relative path, Loading external data process would raise error.\u001b[0m\n",
      "\u001b[92m[2024-02-18 11:46:51,447][PROCESS][INFO]: ==================== Successfully done loading external data: \n",
      " ../../sample_data/test --> /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/input/inference/\u001b[0m\n",
      "\u001b[92m[2024-02-18 11:46:51,452][PROCESS][INFO]: Successfuly finish loading << ../../sample_data/test >> into << /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/input/ >>\u001b[0m\n",
      "\u001b[94m[2024-02-18 11:46:51,456][PROCESS][INFO]: Start setting-up << input >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[92m[2024-02-18 11:46:51,500][PROCESS][INFO]: Now << local >> asset_source_code mode: <input> asset exists.\u001b[0m\n",
      "\u001b[94m[2024-02-18 11:46:51,505][PROCESS][INFO]: Start setting-up << readiness >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2024-02-18 11:46:51,509][PROCESS][INFO]: << readiness >> asset had already been created at 2024-02-16 05:58:38.823572\u001b[0m\n",
      "\u001b[94m[2024-02-18 11:46:51,514][PROCESS][INFO]: Start setting-up << inference1 >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2024-02-18 11:46:51,518][PROCESS][INFO]: Start renewing asset : /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/assets/inference1\u001b[0m\n",
      "\u001b[92m[2024-02-18 11:46:57,093][PROCESS][INFO]: /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/assets/inference1 successfully pulled.\u001b[0m\n",
      "\u001b[94m[2024-02-18 11:46:57,097][PROCESS][INFO]: Start setting-up << preprocess >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[92m[2024-02-18 11:46:57,102][PROCESS][INFO]: Now << local >> asset_source_code mode: <preprocess> asset exists.\u001b[0m\n",
      "\u001b[94m[2024-02-18 11:46:57,106][PROCESS][INFO]: Start setting-up << inference2 >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2024-02-18 11:46:57,111][PROCESS][INFO]: Start renewing asset : /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/assets/inference2\u001b[0m\n",
      "\u001b[92m[2024-02-18 11:46:57,868][PROCESS][INFO]: /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/assets/inference2 successfully pulled.\u001b[0m\n",
      "\u001b[94m[2024-02-18 11:46:57,872][PROCESS][INFO]: Start setting-up << output >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2024-02-18 11:46:57,876][PROCESS][INFO]: << output >> asset had already been created at 2024-02-16 06:23:29.030998\u001b[0m\n",
      "\u001b[94m[2024-02-18 11:46:57,895][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2024-02-18 11:46:57,899][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2024-02-18 11:46:57,903][PROCESS][INFO]: >>> Ignored installing << torch==2.0.0 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2024-02-18 11:46:57,907][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2024-02-18 11:46:57,910][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2024-02-18 11:46:57,913][PROCESS][INFO]: ======================================== Start dependency installation : << input >> \u001b[0m\n",
      "\u001b[94m[2024-02-18 11:46:57,916][PROCESS][INFO]: Start checking existence & installing package - pandas==1.5.3 | Progress: ( 1 / 3 total packages ) \u001b[0m\n",
      "\u001b[92m[2024-02-18 11:46:59,268][PROCESS][INFO]: [OK] << pandas==1.5.3 >> already exists\u001b[0m\n",
      "\u001b[94m[2024-02-18 11:46:59,271][PROCESS][INFO]: ======================================== Start dependency installation : << inference1 >> \u001b[0m\n",
      "\u001b[94m[2024-02-18 11:46:59,274][PROCESS][INFO]: Start checking existence & installing package - torch==2.0.0 | Progress: ( 2 / 3 total packages ) \u001b[0m\n",
      "\u001b[92m[2024-02-18 11:47:00,471][PROCESS][INFO]: [OK] << torch==2.0.0 >> already exists\u001b[0m\n",
      "\u001b[94m[2024-02-18 11:47:00,475][PROCESS][INFO]: Start checking existence & installing package - torchbiggraph@git+https://github.com/seongwooxp/PyTorch-BigGraph | Progress: ( 3 / 3 total packages ) \u001b[0m\n",
      "\u001b[92m[2024-02-18 11:47:00,700][PROCESS][INFO]: [OK] << torchbiggraph@git+https://github.com/seongwooxp/PyTorch-BigGraph >> already exists\u001b[0m\n",
      "\u001b[94m[2024-02-18 11:47:00,704][PROCESS][INFO]: ======================================== Start dependency installation : << force-reinstall >> \u001b[0m\n",
      "\u001b[94m[2024-02-18 11:47:00,707][PROCESS][INFO]: ======================================== Finish dependency installation \n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 아래는 Inference 시 필요한 라이브러리를 설치하는 코드입니다. library 설치 에러가 발생하면 아래 셀을 재실행 해주세요\n",
    "alo.external_load_data(pipelines[1]) # external load data for train_pipeline\n",
    "# 사용하는 pipeline의 package를 설치\n",
    "# train = 0, infernence = 1을 선택해야 하고 둘다 설치 해야함\n",
    "pipeline = pipelines[1]\n",
    "alo.install_steps(pipeline, alo.control[\"get_asset_source\"])\n",
    " # 초기 data structure 구성\n",
    "alo.set_asset_structure()\n",
    "init_asset_structure = copy.deepcopy(alo.asset_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d76d2-b3a0-429b-a5de-b74ae6c04176",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [0] Input asset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f681c8a8-ffde-4f59-a694-1b3849b90dd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 주요 Parameter\n",
    "- input_path : GCR에서는 추론데이터가 'inference' 위치에 자동 저장됩니다. 따로 설정할 필요 없이 주어진 'inference'로 놓고 사용합니다.\n",
    "- x_columns : Inference workflow에서는 x_column을 따로 지정하지 않습니다. None으로 설정합니다.\n",
    "- use_all_x : Inference workflow에서는 use_all_x를 True로 놓습니다.\n",
    "- y_column : 추론데이터는 y_column이 없습니다. None으로 설정합니다.\n",
    "- groupkey_columns : 특정 컬럼 명을 기준으로 데이터를 그룹으로 나누어 모델링을 하고 싶을 경우에 사용합니다.\n",
    "- drop_columns : use_all_x가 True일 때 삭제하고 싶은 컬럼을 입력합니다.\n",
    "- time_column : 데이터에 시간 컬럼이 있을 경우 입력합니다.\n",
    "- concat_dataframes : 같은 형태 csv 파일 여러 개를 input data로 불러올 시, concat 여부를 선택합니다. [*True / False*]\n",
    "- encoding : pd.read_csv() 시에 사용할 encoding 방법을 설정합니다.\n",
    "\n",
    "#### Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2f43e8e-5a9a-4cbc-b2b5-f548d92b3feb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_path': 'test',\n",
       " 'x_columns': None,\n",
       " 'use_all_x': True,\n",
       " 'y_column': None,\n",
       " 'groupkey_columns': None,\n",
       " 'drop_columns': None,\n",
       " 'time_column': None,\n",
       " 'concat_dataframes': None,\n",
       " 'encoding': None}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - readiness (1) - inference1 (2) - preprocess(3) - inference2(4) - output(5))\n",
    "step = 0 \n",
    "alo.asset_structure = copy.deepcopy(init_asset_structure)\n",
    "alo.asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 input asset argument를 원하는 값으로 수정합니다.\n",
    "# asset_structure.args['x_columns'] = ['']\n",
    "alo.asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705ac75-11a8-4093-9531-3b09de8fc38e",
   "metadata": {},
   "source": [
    "##### Input asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "026f2355-99c7-437f-8274-dbed7aa968d8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-18 12:37:07,720][USER][INFO][inference_pipeline][input]: >> Load path : ['/home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/input/inference/test/']\n",
      "[2024-02-18 12:37:07,766][USER][INFO][inference_pipeline][input]: >> The file for batch data has been loaded. (File name: /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/input/inference/test/inference.csv)\n",
      "[2024-02-18 12:37:07,771][USER][INFO][inference_pipeline][input]: You set the << use_all_x >> as << True >> in the yaml file. So skip checking dataframe columns existence.\n",
      "[2024-02-18 12:37:07,776][USER][INFO][inference_pipeline][input]: ==================== Success loading dataframe ====================\n",
      "[2024-02-18 12:37:07,780][USER][INFO][inference_pipeline][input]: >> Start processing ignore columns & drop columns: ['/home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/input/inference/test/inference.csv']\n",
      "[2024-02-18 12:37:07,787][USER][INFO][inference_pipeline][input]: >> You set the << use_all_x >> parameter as << True >> in your config yaml. (So, these x_columns are used: ['workclass', 'sex', 'education-num', 'occupation', 'age', 'marital-status', 'capital-loss', 'race', 'fnlwgt', 'capital-gain', 'hours-per-week', 'native-country', 'ID', 'relationship', 'education'] )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2024-02-18 12:37:07,694][ASSET][INFO][inference_pipeline][input]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2024-02-18 12:37:07\n",
      "- current step      : input\n",
      "- asset branch.     : tabular_2.0\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : 2.1\n",
      "- load envs. keys   : dict_keys(['project_home', 'solution_metadata_version', 'artifacts', 'alo_version', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'pipeline', 'step', 'num_step', 'asset_branch', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path'])\n",
      "- load args. keys   : dict_keys(['input_path', 'x_columns', 'use_all_x', 'y_column', 'groupkey_columns', 'drop_columns', 'time_column', 'concat_dataframes', 'encoding'])\n",
      "- load config. keys : dict_keys(['meta'])\n",
      "- load data keys    : dict_keys([])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2024-02-18 12:37:07,788][ASSET][INFO][inference_pipeline][input]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2024-02-18 12:37:07\n",
      "- current step      : input\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2024-02-18 12:37:07,792][PROCESS][INFO]: ==================== Finish pipeline: inference_pipeline / step: input\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>209642</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>CX7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>Private</td>\n",
       "      <td>45781</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>14084</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>CX8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>Private</td>\n",
       "      <td>205019</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>CX13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>Private</td>\n",
       "      <td>186824</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>CX17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>292175</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>CX19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>193524</td>\n",
       "      <td>Doctorate</td>\n",
       "      <td>16</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>United-States</td>\n",
       "      <td>CX20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>54</td>\n",
       "      <td>?</td>\n",
       "      <td>180211</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>?</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>South</td>\n",
       "      <td>CX27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31</td>\n",
       "      <td>Private</td>\n",
       "      <td>84154</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>?</td>\n",
       "      <td>CX38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>47</td>\n",
       "      <td>Private</td>\n",
       "      <td>51835</td>\n",
       "      <td>Prof-school</td>\n",
       "      <td>15</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>1902</td>\n",
       "      <td>60</td>\n",
       "      <td>Honduras</td>\n",
       "      <td>CX52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>183175</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>CX66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt     education  education-num  \\\n",
       "0   52  Self-emp-not-inc  209642       HS-grad              9   \n",
       "1   31           Private   45781       Masters             14   \n",
       "2   32           Private  205019    Assoc-acdm             12   \n",
       "3   32           Private  186824       HS-grad              9   \n",
       "4   43  Self-emp-not-inc  292175       Masters             14   \n",
       "5   40           Private  193524     Doctorate             16   \n",
       "6   54                 ?  180211  Some-college             10   \n",
       "7   31           Private   84154  Some-college             10   \n",
       "8   47           Private   51835   Prof-school             15   \n",
       "9   28           Private  183175  Some-college             10   \n",
       "\n",
       "       marital-status         occupation   relationship                race  \\\n",
       "0  Married-civ-spouse    Exec-managerial        Husband               White   \n",
       "1       Never-married     Prof-specialty  Not-in-family               White   \n",
       "2       Never-married              Sales  Not-in-family               Black   \n",
       "3       Never-married  Machine-op-inspct      Unmarried               White   \n",
       "4            Divorced    Exec-managerial      Unmarried               White   \n",
       "5  Married-civ-spouse     Prof-specialty        Husband               White   \n",
       "6  Married-civ-spouse                  ?        Husband  Asian-Pac-Islander   \n",
       "7  Married-civ-spouse              Sales        Husband               White   \n",
       "8  Married-civ-spouse     Prof-specialty           Wife               White   \n",
       "9            Divorced       Adm-clerical  Not-in-family               White   \n",
       "\n",
       "      sex  capital-gain  capital-loss  hours-per-week native-country    ID  \n",
       "0    Male             0             0              45  United-States   CX7  \n",
       "1  Female         14084             0              50  United-States   CX8  \n",
       "2    Male             0             0              50  United-States  CX13  \n",
       "3    Male             0             0              40  United-States  CX17  \n",
       "4  Female             0             0              45  United-States  CX19  \n",
       "5    Male             0             0              60  United-States  CX20  \n",
       "6    Male             0             0              60          South  CX27  \n",
       "7    Male             0             0              38              ?  CX38  \n",
       "8  Female             0          1902              60       Honduras  CX52  \n",
       "9  Female             0             0              40  United-States  CX66  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asset 실행\n",
    "input_asset_structure=run(step, pipeline, alo.asset_structure)\n",
    "\n",
    "# input asset의 결과 dataframe은 input_asset_structure.data['dataframe']으로 확인할 수 있습니다.\n",
    "input_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2436cd32",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [1] Readiness asset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac05657",
   "metadata": {},
   "source": [
    "#### 주요 Parameter   \n",
    "<br />\n",
    "GCR 2.0.0부터는 data readiness check을 수행하기 위한 readiness asset이 추가되었습니다.   \n",
    "<br />\n",
    "Readiness asset은 AI content 마다 수행하는 기능과 parameter들이 상이한데, 이는 각 AI content의 동작에 맞도록 input data의 오류를 검사하기 때문입니다.   \n",
    "Input data에서 오류가 발견된 경우, readiness asset은 error를 발생시켜 전체 pipeline을 멈추게 됩니다. 이 때, 발생된 error는 log file에서 확인하실 수 있습니다.   \n",
    "<br />\n",
    "GCR의 readiness asset은 label에 해당하는 column에 결측치가 있는지, graph의 center node에 해당하는 column에 결측치가 있는지를 확인합니다. \n",
    "<br />  \n",
    "이를 위해 필요한 parameter들이 아래와 같습니다.   \n",
    "<br />\n",
    "현재 readiness asset이 alpha version인 관계로, 다른 asset과 parameter 중복이 존재하는데, 이 문제는 readiness asset 정식 version이 release되는 시점 (24년 3월)에 정리될 예정이오니 양해 부탁 드립니다.   \n",
    "<br />\n",
    "\n",
    "- ***x_columns*** : 데이터의 모든 컬럼을 활용하지 않는 경우엔 직접 선택해서 사용할 수 있습니다.\n",
    "- ***y_column*** : Classification, Regression을 위해서는 Label이 있어야 합니다. Label에 해당하는 컬럼을 작성합니다.\n",
    "- ***groupkey_columns*** : 특정 컬럼 명을 기준으로 데이터를 그룹으로 나누어 모델링을 하고 싶을 경우에 사용합니다.\n",
    "- ***center_node_column*** : 특정 컬럼 명을 기준으로 데이터를 그룹으로 나누어 모델링을 하고 싶을 경우에 사용합니다.\n",
    "\n",
    "\n",
    "#### Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5440aa25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_columns': None,\n",
       " 'y_column': 'target',\n",
       " 'groupkey_columns': None,\n",
       " 'center_node_column': 'ID'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - readiness(1) - train1(2) - preprocess(3) - train2(4) - output(5))\n",
    "step = 1\n",
    "alo.asset_structure= copy.deepcopy(input_asset_structure)\n",
    "alo.asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 input asset argument를 원하는 값으로 수정합니다. \n",
    "#asset_structure.args['x_columns'] = ['']\n",
    "alo.asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bf4ef9",
   "metadata": {},
   "source": [
    "#### Readiness asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4cbfc4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2024-02-18 12:37:27,456][ASSET][INFO][inference_pipeline][readiness]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2024-02-18 12:37:27\n",
      "- current step      : readiness\n",
      "- asset branch.     : gcr-0.9.0\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : 2.1\n",
      "- load envs. keys   : dict_keys(['project_home', 'solution_metadata_version', 'artifacts', 'alo_version', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'pipeline', 'step', 'num_step', 'asset_branch', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['x_columns', 'y_column', 'groupkey_columns', 'center_node_column'])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "In __init__: readiness_check initialized\n",
      "In check_data: Data checking begins at 2024-02-18 12:37:27.459203\n",
      "In __del__: pbg deleted\n",
      "In check_data: Data checking ends at 2024-02-18 12:37:27.652099. Memory usage = 0.0\n",
      "In __del__: readiness_check deleted\n",
      "\u001b[94m[2024-02-18 12:37:27,652][ASSET][INFO][inference_pipeline][readiness]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2024-02-18 12:37:27\n",
      "- current step      : readiness\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'center_node_column'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2024-02-18 12:37:27,654][PROCESS][INFO]: ==================== Finish pipeline: inference_pipeline / step: readiness\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>209642</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>CX7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>Private</td>\n",
       "      <td>45781</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>14084</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>CX8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>Private</td>\n",
       "      <td>205019</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>CX13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>Private</td>\n",
       "      <td>186824</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>CX17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>292175</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>CX19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>193524</td>\n",
       "      <td>Doctorate</td>\n",
       "      <td>16</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>United-States</td>\n",
       "      <td>CX20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>54</td>\n",
       "      <td>?</td>\n",
       "      <td>180211</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>?</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>South</td>\n",
       "      <td>CX27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31</td>\n",
       "      <td>Private</td>\n",
       "      <td>84154</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>?</td>\n",
       "      <td>CX38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>47</td>\n",
       "      <td>Private</td>\n",
       "      <td>51835</td>\n",
       "      <td>Prof-school</td>\n",
       "      <td>15</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>1902</td>\n",
       "      <td>60</td>\n",
       "      <td>Honduras</td>\n",
       "      <td>CX52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>183175</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>CX66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt     education  education-num  \\\n",
       "0   52  Self-emp-not-inc  209642       HS-grad              9   \n",
       "1   31           Private   45781       Masters             14   \n",
       "2   32           Private  205019    Assoc-acdm             12   \n",
       "3   32           Private  186824       HS-grad              9   \n",
       "4   43  Self-emp-not-inc  292175       Masters             14   \n",
       "5   40           Private  193524     Doctorate             16   \n",
       "6   54                 ?  180211  Some-college             10   \n",
       "7   31           Private   84154  Some-college             10   \n",
       "8   47           Private   51835   Prof-school             15   \n",
       "9   28           Private  183175  Some-college             10   \n",
       "\n",
       "       marital-status         occupation   relationship                race  \\\n",
       "0  Married-civ-spouse    Exec-managerial        Husband               White   \n",
       "1       Never-married     Prof-specialty  Not-in-family               White   \n",
       "2       Never-married              Sales  Not-in-family               Black   \n",
       "3       Never-married  Machine-op-inspct      Unmarried               White   \n",
       "4            Divorced    Exec-managerial      Unmarried               White   \n",
       "5  Married-civ-spouse     Prof-specialty        Husband               White   \n",
       "6  Married-civ-spouse                  ?        Husband  Asian-Pac-Islander   \n",
       "7  Married-civ-spouse              Sales        Husband               White   \n",
       "8  Married-civ-spouse     Prof-specialty           Wife               White   \n",
       "9            Divorced       Adm-clerical  Not-in-family               White   \n",
       "\n",
       "      sex  capital-gain  capital-loss  hours-per-week native-country    ID  \n",
       "0    Male             0             0              45  United-States   CX7  \n",
       "1  Female         14084             0              50  United-States   CX8  \n",
       "2    Male             0             0              50  United-States  CX13  \n",
       "3    Male             0             0              40  United-States  CX17  \n",
       "4  Female             0             0              45  United-States  CX19  \n",
       "5    Male             0             0              60  United-States  CX20  \n",
       "6    Male             0             0              60          South  CX27  \n",
       "7    Male             0             0              38              ?  CX38  \n",
       "8  Female             0          1902              60       Honduras  CX52  \n",
       "9  Female             0             0              40  United-States  CX66  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readiness_asset_structure = run(step, pipeline, alo.asset_structure)\n",
    "\n",
    "# Readiness asset의 결과 dataframe은 readiness_asset_structure.data['dataframe']으로 확인할 수 있습니다.\n",
    "readiness_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96064143",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [2] Inference1 asset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f56140",
   "metadata": {},
   "source": [
    "GCR 2.0.0부터 inductive graph embedding을 지원함에 따라, inference pipeline에 inference1 asset이 추가되었습니다.   \n",
    "즉, 기존에는 train set과 inference set을 합쳐 train set에서 graph embedding이 이루어졌고, 그로 인해 inference set이 바뀔 때 마다 train pipeline의 재수행이 필요했으나, GCR 2.0.0부터는 inference set이 바뀌어도 해당 inference set에 대해서만 새로운 graph embedding이 이루어지므로 더 이상 train pipeline의 재수행이 필요하지 않습니다.   \n",
    "<br />\n",
    "Graph embedding parameter 설정은 train 시의 설정을 동일하게 유지하므로, inference pipeline에서 고객이 추가로 입력해야 할 parameters는 없습니다.   \n",
    "\n",
    "#### 주요 Parameter\n",
    "- 없음\n",
    "\n",
    "#### Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "22d8ce08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - readiness (1) - inference1 (2) - preprocess(3) - inference2(4) - output(5))\n",
    "step = 2 \n",
    "alo.asset_structure = copy.deepcopy(readiness_asset_structure)\n",
    "alo.asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 inference1 asset argument를 원하는 값으로 수정합니다.\n",
    "# asset_structure.args['x_columns'] = ['']\n",
    "alo.asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d430c960",
   "metadata": {},
   "source": [
    "#### Inference1 asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4f2b15f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[2024-02-18 12:38:21,558][ASSET][INFO][inference_pipeline][inference1]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/\u001b[0m\n",
      "\u001b[94m[2024-02-18 12:38:21,561][ASSET][INFO][inference_pipeline][inference1]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2024-02-18 12:38:21\n",
      "- current step      : inference1\n",
      "- asset branch.     : develop\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : 2.1\n",
      "- load envs. keys   : dict_keys(['project_home', 'solution_metadata_version', 'artifacts', 'alo_version', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'pipeline', 'step', 'num_step', 'asset_branch', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys([])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'center_node_column'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "Embedding Column not selected\n",
      "Assigning ID as Embedding Column\n",
      "preprocessing blank space...\n",
      "In __init__: pbg ready\n",
      "[Embedding Complete]\n",
      "In __del__: pbg deleted\n",
      "\u001b[94m[2024-02-18 12:38:22,092][ASSET][INFO][inference_pipeline][inference1]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2024-02-18 12:38:22\n",
      "- current step      : inference1\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'center_node_column', 'embedding_column', 'dataframe_org'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2024-02-18 12:38:22,096][PROCESS][INFO]: ==================== Finish pipeline: inference_pipeline / step: inference1\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_000</th>\n",
       "      <th>EMB_001</th>\n",
       "      <th>EMB_002</th>\n",
       "      <th>EMB_003</th>\n",
       "      <th>EMB_004</th>\n",
       "      <th>EMB_005</th>\n",
       "      <th>EMB_006</th>\n",
       "      <th>EMB_007</th>\n",
       "      <th>EMB_008</th>\n",
       "      <th>EMB_009</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_118</th>\n",
       "      <th>EMB_119</th>\n",
       "      <th>EMB_120</th>\n",
       "      <th>EMB_121</th>\n",
       "      <th>EMB_122</th>\n",
       "      <th>EMB_123</th>\n",
       "      <th>EMB_124</th>\n",
       "      <th>EMB_125</th>\n",
       "      <th>EMB_126</th>\n",
       "      <th>EMB_127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.006582</td>\n",
       "      <td>-0.030715</td>\n",
       "      <td>-0.033298</td>\n",
       "      <td>0.016711</td>\n",
       "      <td>-0.040304</td>\n",
       "      <td>-0.005820</td>\n",
       "      <td>-0.021985</td>\n",
       "      <td>-0.121384</td>\n",
       "      <td>0.036542</td>\n",
       "      <td>0.057989</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007409</td>\n",
       "      <td>-0.030868</td>\n",
       "      <td>0.021985</td>\n",
       "      <td>0.022022</td>\n",
       "      <td>-0.016511</td>\n",
       "      <td>-0.026778</td>\n",
       "      <td>0.005259</td>\n",
       "      <td>-0.069812</td>\n",
       "      <td>0.046847</td>\n",
       "      <td>-0.005185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.036613</td>\n",
       "      <td>0.010367</td>\n",
       "      <td>-0.022862</td>\n",
       "      <td>0.006432</td>\n",
       "      <td>-0.006352</td>\n",
       "      <td>-0.018229</td>\n",
       "      <td>0.041738</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>-0.023588</td>\n",
       "      <td>-0.007744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016406</td>\n",
       "      <td>-0.008161</td>\n",
       "      <td>-0.003214</td>\n",
       "      <td>-0.011097</td>\n",
       "      <td>-0.014743</td>\n",
       "      <td>0.025421</td>\n",
       "      <td>0.005437</td>\n",
       "      <td>-0.016584</td>\n",
       "      <td>-0.010947</td>\n",
       "      <td>0.010507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.035223</td>\n",
       "      <td>-0.008997</td>\n",
       "      <td>-0.017698</td>\n",
       "      <td>-0.005772</td>\n",
       "      <td>0.007104</td>\n",
       "      <td>-0.015288</td>\n",
       "      <td>0.020383</td>\n",
       "      <td>0.018882</td>\n",
       "      <td>-0.012446</td>\n",
       "      <td>-0.004120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003257</td>\n",
       "      <td>-0.002660</td>\n",
       "      <td>-0.003339</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>-0.011902</td>\n",
       "      <td>0.024652</td>\n",
       "      <td>0.004940</td>\n",
       "      <td>0.005467</td>\n",
       "      <td>0.002907</td>\n",
       "      <td>0.006057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.060184</td>\n",
       "      <td>-0.048973</td>\n",
       "      <td>-0.050838</td>\n",
       "      <td>0.003790</td>\n",
       "      <td>0.006609</td>\n",
       "      <td>-0.016322</td>\n",
       "      <td>0.014977</td>\n",
       "      <td>-0.005287</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.010205</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012011</td>\n",
       "      <td>-0.016546</td>\n",
       "      <td>-0.010955</td>\n",
       "      <td>0.019603</td>\n",
       "      <td>-0.028964</td>\n",
       "      <td>0.026093</td>\n",
       "      <td>0.008958</td>\n",
       "      <td>-0.007763</td>\n",
       "      <td>0.043811</td>\n",
       "      <td>-0.002461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.022729</td>\n",
       "      <td>0.017269</td>\n",
       "      <td>-0.014061</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>-0.012464</td>\n",
       "      <td>-0.016102</td>\n",
       "      <td>0.032073</td>\n",
       "      <td>-0.019775</td>\n",
       "      <td>-0.012860</td>\n",
       "      <td>0.006835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017718</td>\n",
       "      <td>-0.010954</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>-0.009872</td>\n",
       "      <td>-0.010527</td>\n",
       "      <td>0.016775</td>\n",
       "      <td>0.006311</td>\n",
       "      <td>-0.035626</td>\n",
       "      <td>-0.004673</td>\n",
       "      <td>0.007754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.000686</td>\n",
       "      <td>-0.009225</td>\n",
       "      <td>-0.024240</td>\n",
       "      <td>0.028939</td>\n",
       "      <td>-0.040605</td>\n",
       "      <td>-0.002884</td>\n",
       "      <td>-0.008143</td>\n",
       "      <td>-0.104462</td>\n",
       "      <td>0.008309</td>\n",
       "      <td>0.030034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003498</td>\n",
       "      <td>-0.025845</td>\n",
       "      <td>0.008858</td>\n",
       "      <td>0.008207</td>\n",
       "      <td>-0.005420</td>\n",
       "      <td>-0.020894</td>\n",
       "      <td>0.003838</td>\n",
       "      <td>-0.056699</td>\n",
       "      <td>0.004441</td>\n",
       "      <td>0.004468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001991</td>\n",
       "      <td>-0.020987</td>\n",
       "      <td>-0.000745</td>\n",
       "      <td>0.009669</td>\n",
       "      <td>-0.020101</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>-0.004256</td>\n",
       "      <td>-0.037489</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.012287</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004033</td>\n",
       "      <td>-0.011041</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>0.010135</td>\n",
       "      <td>-0.011195</td>\n",
       "      <td>-0.016009</td>\n",
       "      <td>-0.001493</td>\n",
       "      <td>-0.029278</td>\n",
       "      <td>0.022887</td>\n",
       "      <td>-0.006947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.048608</td>\n",
       "      <td>-0.041733</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>0.018432</td>\n",
       "      <td>-0.013950</td>\n",
       "      <td>-0.022629</td>\n",
       "      <td>0.013912</td>\n",
       "      <td>-0.053893</td>\n",
       "      <td>-0.002898</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004983</td>\n",
       "      <td>-0.023749</td>\n",
       "      <td>-0.006918</td>\n",
       "      <td>0.017776</td>\n",
       "      <td>-0.029375</td>\n",
       "      <td>0.011014</td>\n",
       "      <td>0.013531</td>\n",
       "      <td>-0.033731</td>\n",
       "      <td>0.029080</td>\n",
       "      <td>0.001374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.007821</td>\n",
       "      <td>0.013841</td>\n",
       "      <td>-0.006246</td>\n",
       "      <td>0.020809</td>\n",
       "      <td>-0.026219</td>\n",
       "      <td>-0.004054</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>-0.059275</td>\n",
       "      <td>-0.009186</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011782</td>\n",
       "      <td>-0.011115</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>-0.006339</td>\n",
       "      <td>0.002769</td>\n",
       "      <td>-0.008935</td>\n",
       "      <td>0.006211</td>\n",
       "      <td>-0.037140</td>\n",
       "      <td>-0.015952</td>\n",
       "      <td>0.007983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.099372</td>\n",
       "      <td>-0.035823</td>\n",
       "      <td>-0.051990</td>\n",
       "      <td>-0.008933</td>\n",
       "      <td>0.030894</td>\n",
       "      <td>-0.034492</td>\n",
       "      <td>0.063822</td>\n",
       "      <td>0.069110</td>\n",
       "      <td>-0.029069</td>\n",
       "      <td>-0.023238</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001302</td>\n",
       "      <td>-0.006837</td>\n",
       "      <td>-0.027847</td>\n",
       "      <td>0.008006</td>\n",
       "      <td>-0.043292</td>\n",
       "      <td>0.064878</td>\n",
       "      <td>0.013081</td>\n",
       "      <td>0.014527</td>\n",
       "      <td>0.025030</td>\n",
       "      <td>0.001513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    EMB_000   EMB_001   EMB_002   EMB_003   EMB_004   EMB_005   EMB_006  \\\n",
       "0 -0.006582 -0.030715 -0.033298  0.016711 -0.040304 -0.005820 -0.021985   \n",
       "1 -0.036613  0.010367 -0.022862  0.006432 -0.006352 -0.018229  0.041738   \n",
       "2 -0.035223 -0.008997 -0.017698 -0.005772  0.007104 -0.015288  0.020383   \n",
       "3 -0.060184 -0.048973 -0.050838  0.003790  0.006609 -0.016322  0.014977   \n",
       "4 -0.022729  0.017269 -0.014061  0.002197 -0.012464 -0.016102  0.032073   \n",
       "5 -0.000686 -0.009225 -0.024240  0.028939 -0.040605 -0.002884 -0.008143   \n",
       "6  0.001991 -0.020987 -0.000745  0.009669 -0.020101  0.001262 -0.004256   \n",
       "7 -0.048608 -0.041733 -0.051635  0.018432 -0.013950 -0.022629  0.013912   \n",
       "8  0.007821  0.013841 -0.006246  0.020809 -0.026219 -0.004054  0.004046   \n",
       "9 -0.099372 -0.035823 -0.051990 -0.008933  0.030894 -0.034492  0.063822   \n",
       "\n",
       "    EMB_007   EMB_008   EMB_009  ...   EMB_118   EMB_119   EMB_120   EMB_121  \\\n",
       "0 -0.121384  0.036542  0.057989  ... -0.007409 -0.030868  0.021985  0.022022   \n",
       "1  0.000693 -0.023588 -0.007744  ...  0.016406 -0.008161 -0.003214 -0.011097   \n",
       "2  0.018882 -0.012446 -0.004120  ...  0.003257 -0.002660 -0.003339  0.000454   \n",
       "3 -0.005287  0.010419  0.010205  ... -0.012011 -0.016546 -0.010955  0.019603   \n",
       "4 -0.019775 -0.012860  0.006835  ...  0.017718 -0.010954  0.005000 -0.009872   \n",
       "5 -0.104462  0.008309  0.030034  ...  0.003498 -0.025845  0.008858  0.008207   \n",
       "6 -0.037489  0.006000  0.012287  ... -0.004033 -0.011041  0.003463  0.010135   \n",
       "7 -0.053893 -0.002898  0.014815  ... -0.004983 -0.023749 -0.006918  0.017776   \n",
       "8 -0.059275 -0.009186  0.007548  ...  0.011782 -0.011115  0.001337 -0.006339   \n",
       "9  0.069110 -0.029069 -0.023238  ... -0.001302 -0.006837 -0.027847  0.008006   \n",
       "\n",
       "    EMB_122   EMB_123   EMB_124   EMB_125   EMB_126   EMB_127  \n",
       "0 -0.016511 -0.026778  0.005259 -0.069812  0.046847 -0.005185  \n",
       "1 -0.014743  0.025421  0.005437 -0.016584 -0.010947  0.010507  \n",
       "2 -0.011902  0.024652  0.004940  0.005467  0.002907  0.006057  \n",
       "3 -0.028964  0.026093  0.008958 -0.007763  0.043811 -0.002461  \n",
       "4 -0.010527  0.016775  0.006311 -0.035626 -0.004673  0.007754  \n",
       "5 -0.005420 -0.020894  0.003838 -0.056699  0.004441  0.004468  \n",
       "6 -0.011195 -0.016009 -0.001493 -0.029278  0.022887 -0.006947  \n",
       "7 -0.029375  0.011014  0.013531 -0.033731  0.029080  0.001374  \n",
       "8  0.002769 -0.008935  0.006211 -0.037140 -0.015952  0.007983  \n",
       "9 -0.043292  0.064878  0.013081  0.014527  0.025030  0.001513  \n",
       "\n",
       "[10 rows x 128 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asset 실행\n",
    "inference1_asset_structure=run(step, pipeline, alo.asset_structure)\n",
    "\n",
    "# inference1 asset의 결과 dataframe은 inference1_asset_structure.data['dataframe']으로 확인할 수 있습니다.  \n",
    "inference1_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9f3b11-9b2d-4bd0-87f2-25b23495758f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [3] Preprocess asset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ddf39d-af5c-48bd-9a96-a3543e030b2c",
   "metadata": {},
   "source": [
    "GCR은 데이터 전처리가 불필요하기 때문에 Preprocess asset의 역할은 크지 않습니다. 다만 사용자가 임베딩 외에 raw data를 학습에 사용하는 경우 (즉, extra_columns_for_ml 설정시) 결측치를 처리하기 위한 용도입니다.\n",
    "#### 주요 Parameter\n",
    "- handling_missing: 결측치 처리 방식을 지정합니다. 'interpolation' 또는 'fill_number' 중에 선택할 수 있으며 GCR에서는 'interpolation'을 권장합니다.\n",
    "- ***handling_encoding_y_column***: None으로 설정합니다.\n",
    "- limit_encoding_categories: onehot이나 hashing 인코딩 진행 시 컬럼이 너무 많아지는 것에 대한 한계치를 설정합니다.\n",
    "- load_train_preprocess: 반드시 True로 설정합니다. train workflow의 preprocess를 참조하여 진행합니다.\n",
    "\n",
    "#### Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b486fd42-87de-4c15-aa77-324cb77415b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'handling_missing': 'interpolation',\n",
       " 'handling_encoding_y_column': None,\n",
       " 'limit_encoding_categories': 30,\n",
       " 'load_train_preprocess': True}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - readiness (1) - inference1 (2) - preprocess(3) - inference2(4) - output(5))\n",
    "step = 3 \n",
    "alo.asset_structure = copy.deepcopy(inference1_asset_structure)\n",
    "alo.asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 preprocess asset argument를 원하는 값으로 수정합니다.\n",
    "# asset_structure.args['x_columns'] = ['']\n",
    "alo.asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6a36f8-55c3-4e3e-8879-d8aeb7f9ecac",
   "metadata": {},
   "source": [
    "#### Preprocess asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f690c108-afc0-48e3-9b04-85a4f498c8da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[2024-02-18 12:39:08,577][ASSET][INFO][inference_pipeline][preprocess]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/preprocess/\u001b[0m\n",
      "\u001b[94m[2024-02-18 12:39:08,580][ASSET][INFO][inference_pipeline][preprocess]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2024-02-18 12:39:08\n",
      "- current step      : preprocess\n",
      "- asset branch.     : release-1.2\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : 2.1\n",
      "- load envs. keys   : dict_keys(['project_home', 'solution_metadata_version', 'artifacts', 'alo_version', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'pipeline', 'step', 'num_step', 'asset_branch', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['handling_missing', 'handling_encoding_y_column', 'limit_encoding_categories', 'load_train_preprocess'])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'center_node_column', 'embedding_column', 'dataframe_org'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "['EMB_000_nan', 'EMB_001_nan', 'EMB_002_nan', 'EMB_003_nan', 'EMB_004_nan', 'EMB_005_nan', 'EMB_006_nan', 'EMB_007_nan', 'EMB_008_nan', 'EMB_009_nan', 'EMB_010_nan', 'EMB_011_nan', 'EMB_012_nan', 'EMB_013_nan', 'EMB_014_nan', 'EMB_015_nan', 'EMB_016_nan', 'EMB_017_nan', 'EMB_018_nan', 'EMB_019_nan', 'EMB_020_nan', 'EMB_021_nan', 'EMB_022_nan', 'EMB_023_nan', 'EMB_024_nan', 'EMB_025_nan', 'EMB_026_nan', 'EMB_027_nan', 'EMB_028_nan', 'EMB_029_nan', 'EMB_030_nan', 'EMB_031_nan', 'EMB_032_nan', 'EMB_033_nan', 'EMB_034_nan', 'EMB_035_nan', 'EMB_036_nan', 'EMB_037_nan', 'EMB_038_nan', 'EMB_039_nan', 'EMB_040_nan', 'EMB_041_nan', 'EMB_042_nan', 'EMB_043_nan', 'EMB_044_nan', 'EMB_045_nan', 'EMB_046_nan', 'EMB_047_nan', 'EMB_048_nan', 'EMB_049_nan', 'EMB_050_nan', 'EMB_051_nan', 'EMB_052_nan', 'EMB_053_nan', 'EMB_054_nan', 'EMB_055_nan', 'EMB_056_nan', 'EMB_057_nan', 'EMB_058_nan', 'EMB_059_nan', 'EMB_060_nan', 'EMB_061_nan', 'EMB_062_nan', 'EMB_063_nan', 'EMB_064_nan', 'EMB_065_nan', 'EMB_066_nan', 'EMB_067_nan', 'EMB_068_nan', 'EMB_069_nan', 'EMB_070_nan', 'EMB_071_nan', 'EMB_072_nan', 'EMB_073_nan', 'EMB_074_nan', 'EMB_075_nan', 'EMB_076_nan', 'EMB_077_nan', 'EMB_078_nan', 'EMB_079_nan', 'EMB_080_nan', 'EMB_081_nan', 'EMB_082_nan', 'EMB_083_nan', 'EMB_084_nan', 'EMB_085_nan', 'EMB_086_nan', 'EMB_087_nan', 'EMB_088_nan', 'EMB_089_nan', 'EMB_090_nan', 'EMB_091_nan', 'EMB_092_nan', 'EMB_093_nan', 'EMB_094_nan', 'EMB_095_nan', 'EMB_096_nan', 'EMB_097_nan', 'EMB_098_nan', 'EMB_099_nan', 'EMB_100_nan', 'EMB_101_nan', 'EMB_102_nan', 'EMB_103_nan', 'EMB_104_nan', 'EMB_105_nan', 'EMB_106_nan', 'EMB_107_nan', 'EMB_108_nan', 'EMB_109_nan', 'EMB_110_nan', 'EMB_111_nan', 'EMB_112_nan', 'EMB_113_nan', 'EMB_114_nan', 'EMB_115_nan', 'EMB_116_nan', 'EMB_117_nan', 'EMB_118_nan', 'EMB_119_nan', 'EMB_120_nan', 'EMB_121_nan', 'EMB_122_nan', 'EMB_123_nan', 'EMB_124_nan', 'EMB_125_nan', 'EMB_126_nan', 'EMB_127_nan'] \n",
      "\u001b[94m[2024-02-18 12:39:08,615][ASSET][INFO][inference_pipeline][preprocess]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2024-02-18 12:39:08\n",
      "- current step      : preprocess\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'center_node_column', 'embedding_column', 'dataframe_org', 'columns_map', 'preprocess'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2024-02-18 12:39:08,618][PROCESS][INFO]: ==================== Finish pipeline: inference_pipeline / step: preprocess\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_000</th>\n",
       "      <th>EMB_001</th>\n",
       "      <th>EMB_002</th>\n",
       "      <th>EMB_003</th>\n",
       "      <th>EMB_004</th>\n",
       "      <th>EMB_005</th>\n",
       "      <th>EMB_006</th>\n",
       "      <th>EMB_007</th>\n",
       "      <th>EMB_008</th>\n",
       "      <th>EMB_009</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_118_nan</th>\n",
       "      <th>EMB_119_nan</th>\n",
       "      <th>EMB_120_nan</th>\n",
       "      <th>EMB_121_nan</th>\n",
       "      <th>EMB_122_nan</th>\n",
       "      <th>EMB_123_nan</th>\n",
       "      <th>EMB_124_nan</th>\n",
       "      <th>EMB_125_nan</th>\n",
       "      <th>EMB_126_nan</th>\n",
       "      <th>EMB_127_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.006582</td>\n",
       "      <td>-0.030715</td>\n",
       "      <td>-0.033298</td>\n",
       "      <td>0.016711</td>\n",
       "      <td>-0.040304</td>\n",
       "      <td>-0.005820</td>\n",
       "      <td>-0.021985</td>\n",
       "      <td>-0.121384</td>\n",
       "      <td>0.036542</td>\n",
       "      <td>0.057989</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007409</td>\n",
       "      <td>-0.030868</td>\n",
       "      <td>0.021985</td>\n",
       "      <td>0.022022</td>\n",
       "      <td>-0.016511</td>\n",
       "      <td>-0.026778</td>\n",
       "      <td>0.005259</td>\n",
       "      <td>-0.069812</td>\n",
       "      <td>0.046847</td>\n",
       "      <td>-0.005185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.036613</td>\n",
       "      <td>0.010367</td>\n",
       "      <td>-0.022862</td>\n",
       "      <td>0.006432</td>\n",
       "      <td>-0.006352</td>\n",
       "      <td>-0.018229</td>\n",
       "      <td>0.041738</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>-0.023588</td>\n",
       "      <td>-0.007744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016406</td>\n",
       "      <td>-0.008161</td>\n",
       "      <td>-0.003214</td>\n",
       "      <td>-0.011097</td>\n",
       "      <td>-0.014743</td>\n",
       "      <td>0.025421</td>\n",
       "      <td>0.005437</td>\n",
       "      <td>-0.016584</td>\n",
       "      <td>-0.010947</td>\n",
       "      <td>0.010507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.035223</td>\n",
       "      <td>-0.008997</td>\n",
       "      <td>-0.017698</td>\n",
       "      <td>-0.005772</td>\n",
       "      <td>0.007104</td>\n",
       "      <td>-0.015288</td>\n",
       "      <td>0.020383</td>\n",
       "      <td>0.018882</td>\n",
       "      <td>-0.012446</td>\n",
       "      <td>-0.004120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003257</td>\n",
       "      <td>-0.002660</td>\n",
       "      <td>-0.003339</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>-0.011902</td>\n",
       "      <td>0.024652</td>\n",
       "      <td>0.004940</td>\n",
       "      <td>0.005467</td>\n",
       "      <td>0.002907</td>\n",
       "      <td>0.006057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.060184</td>\n",
       "      <td>-0.048973</td>\n",
       "      <td>-0.050838</td>\n",
       "      <td>0.003790</td>\n",
       "      <td>0.006609</td>\n",
       "      <td>-0.016322</td>\n",
       "      <td>0.014977</td>\n",
       "      <td>-0.005287</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.010205</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012011</td>\n",
       "      <td>-0.016546</td>\n",
       "      <td>-0.010955</td>\n",
       "      <td>0.019603</td>\n",
       "      <td>-0.028964</td>\n",
       "      <td>0.026093</td>\n",
       "      <td>0.008958</td>\n",
       "      <td>-0.007763</td>\n",
       "      <td>0.043811</td>\n",
       "      <td>-0.002461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.022729</td>\n",
       "      <td>0.017269</td>\n",
       "      <td>-0.014061</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>-0.012464</td>\n",
       "      <td>-0.016102</td>\n",
       "      <td>0.032073</td>\n",
       "      <td>-0.019775</td>\n",
       "      <td>-0.012860</td>\n",
       "      <td>0.006835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017718</td>\n",
       "      <td>-0.010954</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>-0.009872</td>\n",
       "      <td>-0.010527</td>\n",
       "      <td>0.016775</td>\n",
       "      <td>0.006311</td>\n",
       "      <td>-0.035626</td>\n",
       "      <td>-0.004673</td>\n",
       "      <td>0.007754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.000686</td>\n",
       "      <td>-0.009225</td>\n",
       "      <td>-0.024240</td>\n",
       "      <td>0.028939</td>\n",
       "      <td>-0.040605</td>\n",
       "      <td>-0.002884</td>\n",
       "      <td>-0.008143</td>\n",
       "      <td>-0.104462</td>\n",
       "      <td>0.008309</td>\n",
       "      <td>0.030034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003498</td>\n",
       "      <td>-0.025845</td>\n",
       "      <td>0.008858</td>\n",
       "      <td>0.008207</td>\n",
       "      <td>-0.005420</td>\n",
       "      <td>-0.020894</td>\n",
       "      <td>0.003838</td>\n",
       "      <td>-0.056699</td>\n",
       "      <td>0.004441</td>\n",
       "      <td>0.004468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001991</td>\n",
       "      <td>-0.020987</td>\n",
       "      <td>-0.000745</td>\n",
       "      <td>0.009669</td>\n",
       "      <td>-0.020101</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>-0.004256</td>\n",
       "      <td>-0.037489</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.012287</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004033</td>\n",
       "      <td>-0.011041</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>0.010135</td>\n",
       "      <td>-0.011195</td>\n",
       "      <td>-0.016009</td>\n",
       "      <td>-0.001493</td>\n",
       "      <td>-0.029278</td>\n",
       "      <td>0.022887</td>\n",
       "      <td>-0.006947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.048608</td>\n",
       "      <td>-0.041733</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>0.018432</td>\n",
       "      <td>-0.013950</td>\n",
       "      <td>-0.022629</td>\n",
       "      <td>0.013912</td>\n",
       "      <td>-0.053893</td>\n",
       "      <td>-0.002898</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004983</td>\n",
       "      <td>-0.023749</td>\n",
       "      <td>-0.006918</td>\n",
       "      <td>0.017776</td>\n",
       "      <td>-0.029375</td>\n",
       "      <td>0.011014</td>\n",
       "      <td>0.013531</td>\n",
       "      <td>-0.033731</td>\n",
       "      <td>0.029080</td>\n",
       "      <td>0.001374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.007821</td>\n",
       "      <td>0.013841</td>\n",
       "      <td>-0.006246</td>\n",
       "      <td>0.020809</td>\n",
       "      <td>-0.026219</td>\n",
       "      <td>-0.004054</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>-0.059275</td>\n",
       "      <td>-0.009186</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011782</td>\n",
       "      <td>-0.011115</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>-0.006339</td>\n",
       "      <td>0.002769</td>\n",
       "      <td>-0.008935</td>\n",
       "      <td>0.006211</td>\n",
       "      <td>-0.037140</td>\n",
       "      <td>-0.015952</td>\n",
       "      <td>0.007983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.099372</td>\n",
       "      <td>-0.035823</td>\n",
       "      <td>-0.051990</td>\n",
       "      <td>-0.008933</td>\n",
       "      <td>0.030894</td>\n",
       "      <td>-0.034492</td>\n",
       "      <td>0.063822</td>\n",
       "      <td>0.069110</td>\n",
       "      <td>-0.029069</td>\n",
       "      <td>-0.023238</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001302</td>\n",
       "      <td>-0.006837</td>\n",
       "      <td>-0.027847</td>\n",
       "      <td>0.008006</td>\n",
       "      <td>-0.043292</td>\n",
       "      <td>0.064878</td>\n",
       "      <td>0.013081</td>\n",
       "      <td>0.014527</td>\n",
       "      <td>0.025030</td>\n",
       "      <td>0.001513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    EMB_000   EMB_001   EMB_002   EMB_003   EMB_004   EMB_005   EMB_006  \\\n",
       "0 -0.006582 -0.030715 -0.033298  0.016711 -0.040304 -0.005820 -0.021985   \n",
       "1 -0.036613  0.010367 -0.022862  0.006432 -0.006352 -0.018229  0.041738   \n",
       "2 -0.035223 -0.008997 -0.017698 -0.005772  0.007104 -0.015288  0.020383   \n",
       "3 -0.060184 -0.048973 -0.050838  0.003790  0.006609 -0.016322  0.014977   \n",
       "4 -0.022729  0.017269 -0.014061  0.002197 -0.012464 -0.016102  0.032073   \n",
       "5 -0.000686 -0.009225 -0.024240  0.028939 -0.040605 -0.002884 -0.008143   \n",
       "6  0.001991 -0.020987 -0.000745  0.009669 -0.020101  0.001262 -0.004256   \n",
       "7 -0.048608 -0.041733 -0.051635  0.018432 -0.013950 -0.022629  0.013912   \n",
       "8  0.007821  0.013841 -0.006246  0.020809 -0.026219 -0.004054  0.004046   \n",
       "9 -0.099372 -0.035823 -0.051990 -0.008933  0.030894 -0.034492  0.063822   \n",
       "\n",
       "    EMB_007   EMB_008   EMB_009  ...  EMB_118_nan  EMB_119_nan  EMB_120_nan  \\\n",
       "0 -0.121384  0.036542  0.057989  ...    -0.007409    -0.030868     0.021985   \n",
       "1  0.000693 -0.023588 -0.007744  ...     0.016406    -0.008161    -0.003214   \n",
       "2  0.018882 -0.012446 -0.004120  ...     0.003257    -0.002660    -0.003339   \n",
       "3 -0.005287  0.010419  0.010205  ...    -0.012011    -0.016546    -0.010955   \n",
       "4 -0.019775 -0.012860  0.006835  ...     0.017718    -0.010954     0.005000   \n",
       "5 -0.104462  0.008309  0.030034  ...     0.003498    -0.025845     0.008858   \n",
       "6 -0.037489  0.006000  0.012287  ...    -0.004033    -0.011041     0.003463   \n",
       "7 -0.053893 -0.002898  0.014815  ...    -0.004983    -0.023749    -0.006918   \n",
       "8 -0.059275 -0.009186  0.007548  ...     0.011782    -0.011115     0.001337   \n",
       "9  0.069110 -0.029069 -0.023238  ...    -0.001302    -0.006837    -0.027847   \n",
       "\n",
       "   EMB_121_nan  EMB_122_nan  EMB_123_nan  EMB_124_nan  EMB_125_nan  \\\n",
       "0     0.022022    -0.016511    -0.026778     0.005259    -0.069812   \n",
       "1    -0.011097    -0.014743     0.025421     0.005437    -0.016584   \n",
       "2     0.000454    -0.011902     0.024652     0.004940     0.005467   \n",
       "3     0.019603    -0.028964     0.026093     0.008958    -0.007763   \n",
       "4    -0.009872    -0.010527     0.016775     0.006311    -0.035626   \n",
       "5     0.008207    -0.005420    -0.020894     0.003838    -0.056699   \n",
       "6     0.010135    -0.011195    -0.016009    -0.001493    -0.029278   \n",
       "7     0.017776    -0.029375     0.011014     0.013531    -0.033731   \n",
       "8    -0.006339     0.002769    -0.008935     0.006211    -0.037140   \n",
       "9     0.008006    -0.043292     0.064878     0.013081     0.014527   \n",
       "\n",
       "   EMB_126_nan  EMB_127_nan  \n",
       "0     0.046847    -0.005185  \n",
       "1    -0.010947     0.010507  \n",
       "2     0.002907     0.006057  \n",
       "3     0.043811    -0.002461  \n",
       "4    -0.004673     0.007754  \n",
       "5     0.004441     0.004468  \n",
       "6     0.022887    -0.006947  \n",
       "7     0.029080     0.001374  \n",
       "8    -0.015952     0.007983  \n",
       "9     0.025030     0.001513  \n",
       "\n",
       "[10 rows x 256 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asset 실행\n",
    "preprocess_asset_structure=run(step, pipeline, alo.asset_structure)\n",
    "\n",
    "# preprocess asset의 결과 dataframe은 preprocess_asset_structure.data['dataframe']으로 확인할 수 있습니다.  \n",
    "preprocess_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c107ab-179a-4ad9-b3ec-ba5195910eb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [4] Inference2 asset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a66a6da-7efd-4c9f-92b4-b91366365db7",
   "metadata": {},
   "source": [
    "GCR 2.0.0부터 inductive ML이 지원됨에 따라 inference pipeline도 graph embedding asset (inference1 asset)이 포함되므로, classification 및 regression을 위한 기존의 inference asset이 inference2 asset으로 이름 변경되었습니다.\n",
    "\n",
    "#### 주요 Parameter\n",
    "- model_type: Train workflow의 Train asset과 동일하게 classification/regression 중 설정하면 됩니다. [*classification / regression*]\n",
    "- run_shapley: shapley 실행 여부를 선택합니다. [*True / False*]\n",
    "\n",
    "#### Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13b21092-9b65-4de1-874e-46e8cbd96075",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_type': 'classification', 'run_shapley': False}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - readiness (1) - inference1 (2) - preprocess(3) - inference2(4) - output(5))\n",
    "step = 4 \n",
    "alo.asset_structure = copy.deepcopy(preprocess_asset_structure)\n",
    "alo.asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 inference asset argument를 원하는 값으로 수정합니다.\n",
    "# asset_structure.args['x_columns'] = ['']\n",
    "alo.asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5544d75-1e2f-4009-9e24-7cb91bc18ced",
   "metadata": {},
   "source": [
    "#### Inference2 asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf9857de-2615-41ec-829f-cddb0808aab2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "font: ['NanumBarunGothic']\n",
      "\n",
      " ################################### inference_init (sec):  0.00021457672119140625 ################################### \n",
      "\n",
      "\u001b[94m[2024-02-18 12:42:33,751][ASSET][INFO][inference_pipeline][inference2]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2024-02-18 12:42:33\n",
      "- current step      : inference2\n",
      "- asset branch.     : tcr_v1.1.4\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : 2.1\n",
      "- load envs. keys   : dict_keys(['project_home', 'solution_metadata_version', 'artifacts', 'alo_version', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'pipeline', 'step', 'num_step', 'asset_branch', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['model_type', 'run_shapley'])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'center_node_column', 'embedding_column', 'dataframe_org', 'columns_map', 'preprocess'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[92m[2024-02-18 12:42:33,755][ASSET][INFO][inference_pipeline][inference2]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/\u001b[0m\n",
      "\u001b[92m[2024-02-18 12:42:33,776][ASSET][INFO][inference_pipeline][inference2]: Successfully got << output path >> for saving your data into csv or jpg file: \n",
      " /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.inference_artifacts/output/inference2/ \n",
      " - [NOTE] The names of output file must be fixed as << output.csv, output.jpg >> \u001b[0m\n",
      "해당 column 은 Training 과정에 사용되지 않습니다. (column_name: ['EMB_052', 'EMB_020', 'EMB_078', 'EMB_018', 'EMB_077', 'EMB_088', 'EMB_028', 'EMB_106', 'EMB_039', 'EMB_105', 'EMB_000', 'EMB_046', 'EMB_069', 'EMB_009', 'EMB_076', 'EMB_007', 'EMB_056', 'EMB_045', 'EMB_050', 'EMB_087', 'EMB_075', 'EMB_015', 'EMB_104', 'EMB_041', 'EMB_048', 'EMB_101', 'EMB_111', 'EMB_116', 'EMB_107', 'EMB_127', 'EMB_113', 'EMB_068', 'EMB_103', 'EMB_070', 'EMB_100', 'EMB_124', 'EMB_049', 'EMB_006', 'EMB_054', 'EMB_044', 'EMB_074', 'EMB_108', 'EMB_012', 'EMB_038', 'EMB_057', 'EMB_034', 'EMB_062', 'EMB_122', 'EMB_025', 'EMB_051', 'EMB_055', 'EMB_080', 'EMB_084', 'EMB_005', 'EMB_083', 'EMB_004', 'EMB_063', 'EMB_024', 'EMB_033', 'EMB_059', 'EMB_120', 'EMB_089', 'EMB_030', 'EMB_114', 'EMB_097', 'EMB_014', 'EMB_036', 'EMB_042', 'EMB_060', 'EMB_109', 'EMB_061', 'EMB_095', 'EMB_110', 'EMB_065', 'EMB_079', 'EMB_091', 'EMB_053', 'EMB_011', 'EMB_058', 'EMB_019', 'EMB_023', 'EMB_098', 'EMB_066', 'EMB_047', 'EMB_016', 'EMB_029', 'EMB_021', 'EMB_090', 'EMB_043', 'EMB_072', 'EMB_125', 'EMB_126', 'EMB_067', 'EMB_002', 'EMB_092', 'EMB_118', 'EMB_037', 'EMB_093', 'EMB_082', 'EMB_115', 'EMB_094', 'EMB_026', 'EMB_027', 'EMB_112', 'EMB_121', 'EMB_064', 'EMB_040', 'EMB_073', 'EMB_013', 'EMB_096', 'EMB_022', 'EMB_123', 'EMB_010', 'EMB_008', 'EMB_086', 'EMB_001', 'EMB_099', 'EMB_071', 'EMB_017', 'EMB_119', 'EMB_035', 'EMB_085', 'EMB_031', 'EMB_102', 'EMB_117', 'EMB_081', 'EMB_032', 'EMB_003'])\n",
      "\u001b[92m[2024-02-18 12:42:33,789][ASSET][INFO][inference_pipeline][inference2]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/\u001b[0m\n",
      "[INFO] XAI 분석 시, 활용할 모델을 로드합니다.\n",
      "모델을 Load 완료 하였습니다. (모델 위치: /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.train_artifacts/models/train/best_model_top0.pkl)\n",
      "[추론 데이터에 대한 모델 성능을 저장하기 위해 model_performance.json 파일을 생성합니다.\n",
      "\u001b[93m[2024-02-18 12:42:34,183][ASSET][WARNING][inference_pipeline][inference2]: Please enter the << external_path - save_inference_artifacts_path >> in the experimental_plan.yaml.\u001b[0m\n",
      "\u001b[92m[2024-02-18 12:42:34,215][ASSET][INFO][inference_pipeline][inference2]: Successfully saved inference summary yaml. \n",
      " >> /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.inference_artifacts/score/inference_summary.yaml\u001b[0m\n",
      "\n",
      " ################################### inference_user_run (sec):  3.217634677886963 ################################### \n",
      "\n",
      "\u001b[94m[2024-02-18 12:42:36,997][ASSET][INFO][inference_pipeline][inference2]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2024-02-18 12:42:36\n",
      "- current step      : inference2\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'center_node_column', 'embedding_column', 'dataframe_org', 'columns_map', 'preprocess'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2024-02-18 12:42:36,999][PROCESS][INFO]: ==================== Finish pipeline: inference_pipeline / step: inference2\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_000</th>\n",
       "      <th>EMB_001</th>\n",
       "      <th>EMB_002</th>\n",
       "      <th>EMB_003</th>\n",
       "      <th>EMB_004</th>\n",
       "      <th>EMB_005</th>\n",
       "      <th>EMB_006</th>\n",
       "      <th>EMB_007</th>\n",
       "      <th>EMB_008</th>\n",
       "      <th>EMB_009</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_123_nan</th>\n",
       "      <th>EMB_124_nan</th>\n",
       "      <th>EMB_125_nan</th>\n",
       "      <th>EMB_126_nan</th>\n",
       "      <th>EMB_127_nan</th>\n",
       "      <th>train_test</th>\n",
       "      <th>pred_</th>\n",
       "      <th>prediction_score</th>\n",
       "      <th>prob_0</th>\n",
       "      <th>prob_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.006582</td>\n",
       "      <td>-0.030715</td>\n",
       "      <td>-0.033298</td>\n",
       "      <td>0.016711</td>\n",
       "      <td>-0.040304</td>\n",
       "      <td>-0.005820</td>\n",
       "      <td>-0.021985</td>\n",
       "      <td>-0.121384</td>\n",
       "      <td>0.036542</td>\n",
       "      <td>0.057989</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026778</td>\n",
       "      <td>0.005259</td>\n",
       "      <td>-0.069812</td>\n",
       "      <td>0.046847</td>\n",
       "      <td>-0.005185</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5150744928098547, 0.4849255071901452]</td>\n",
       "      <td>0.515074</td>\n",
       "      <td>0.484926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.036613</td>\n",
       "      <td>0.010367</td>\n",
       "      <td>-0.022862</td>\n",
       "      <td>0.006432</td>\n",
       "      <td>-0.006352</td>\n",
       "      <td>-0.018229</td>\n",
       "      <td>0.041738</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>-0.023588</td>\n",
       "      <td>-0.007744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025421</td>\n",
       "      <td>0.005437</td>\n",
       "      <td>-0.016584</td>\n",
       "      <td>-0.010947</td>\n",
       "      <td>0.010507</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.4839683381813784, 0.5160316618186216]</td>\n",
       "      <td>0.483968</td>\n",
       "      <td>0.516032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.035223</td>\n",
       "      <td>-0.008997</td>\n",
       "      <td>-0.017698</td>\n",
       "      <td>-0.005772</td>\n",
       "      <td>0.007104</td>\n",
       "      <td>-0.015288</td>\n",
       "      <td>0.020383</td>\n",
       "      <td>0.018882</td>\n",
       "      <td>-0.012446</td>\n",
       "      <td>-0.004120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024652</td>\n",
       "      <td>0.004940</td>\n",
       "      <td>0.005467</td>\n",
       "      <td>0.002907</td>\n",
       "      <td>0.006057</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.18866384155711147, 0.8113361584428885]</td>\n",
       "      <td>0.188664</td>\n",
       "      <td>0.811336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.060184</td>\n",
       "      <td>-0.048973</td>\n",
       "      <td>-0.050838</td>\n",
       "      <td>0.003790</td>\n",
       "      <td>0.006609</td>\n",
       "      <td>-0.016322</td>\n",
       "      <td>0.014977</td>\n",
       "      <td>-0.005287</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.010205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026093</td>\n",
       "      <td>0.008958</td>\n",
       "      <td>-0.007763</td>\n",
       "      <td>0.043811</td>\n",
       "      <td>-0.002461</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0034548822989322048, 0.9965451177010678]</td>\n",
       "      <td>0.003455</td>\n",
       "      <td>0.996545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.022729</td>\n",
       "      <td>0.017269</td>\n",
       "      <td>-0.014061</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>-0.012464</td>\n",
       "      <td>-0.016102</td>\n",
       "      <td>0.032073</td>\n",
       "      <td>-0.019775</td>\n",
       "      <td>-0.012860</td>\n",
       "      <td>0.006835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016775</td>\n",
       "      <td>0.006311</td>\n",
       "      <td>-0.035626</td>\n",
       "      <td>-0.004673</td>\n",
       "      <td>0.007754</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6471172289045981, 0.35288277109540195]</td>\n",
       "      <td>0.647117</td>\n",
       "      <td>0.352883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.000686</td>\n",
       "      <td>-0.009225</td>\n",
       "      <td>-0.024240</td>\n",
       "      <td>0.028939</td>\n",
       "      <td>-0.040605</td>\n",
       "      <td>-0.002884</td>\n",
       "      <td>-0.008143</td>\n",
       "      <td>-0.104462</td>\n",
       "      <td>0.008309</td>\n",
       "      <td>0.030034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020894</td>\n",
       "      <td>0.003838</td>\n",
       "      <td>-0.056699</td>\n",
       "      <td>0.004441</td>\n",
       "      <td>0.004468</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7347797741003542, 0.2652202258996458]</td>\n",
       "      <td>0.734780</td>\n",
       "      <td>0.265220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001991</td>\n",
       "      <td>-0.020987</td>\n",
       "      <td>-0.000745</td>\n",
       "      <td>0.009669</td>\n",
       "      <td>-0.020101</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>-0.004256</td>\n",
       "      <td>-0.037489</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.012287</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016009</td>\n",
       "      <td>-0.001493</td>\n",
       "      <td>-0.029278</td>\n",
       "      <td>0.022887</td>\n",
       "      <td>-0.006947</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6103225519052327, 0.3896774480947674]</td>\n",
       "      <td>0.610323</td>\n",
       "      <td>0.389677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.048608</td>\n",
       "      <td>-0.041733</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>0.018432</td>\n",
       "      <td>-0.013950</td>\n",
       "      <td>-0.022629</td>\n",
       "      <td>0.013912</td>\n",
       "      <td>-0.053893</td>\n",
       "      <td>-0.002898</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011014</td>\n",
       "      <td>0.013531</td>\n",
       "      <td>-0.033731</td>\n",
       "      <td>0.029080</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2262415791488076, 0.7737584208511924]</td>\n",
       "      <td>0.226242</td>\n",
       "      <td>0.773758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.007821</td>\n",
       "      <td>0.013841</td>\n",
       "      <td>-0.006246</td>\n",
       "      <td>0.020809</td>\n",
       "      <td>-0.026219</td>\n",
       "      <td>-0.004054</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>-0.059275</td>\n",
       "      <td>-0.009186</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008935</td>\n",
       "      <td>0.006211</td>\n",
       "      <td>-0.037140</td>\n",
       "      <td>-0.015952</td>\n",
       "      <td>0.007983</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7995174698140237, 0.20048253018597628]</td>\n",
       "      <td>0.799517</td>\n",
       "      <td>0.200483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.099372</td>\n",
       "      <td>-0.035823</td>\n",
       "      <td>-0.051990</td>\n",
       "      <td>-0.008933</td>\n",
       "      <td>0.030894</td>\n",
       "      <td>-0.034492</td>\n",
       "      <td>0.063822</td>\n",
       "      <td>0.069110</td>\n",
       "      <td>-0.029069</td>\n",
       "      <td>-0.023238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064878</td>\n",
       "      <td>0.013081</td>\n",
       "      <td>0.014527</td>\n",
       "      <td>0.025030</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0044173301105442375, 0.9955826698894558]</td>\n",
       "      <td>0.004417</td>\n",
       "      <td>0.995583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 261 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    EMB_000   EMB_001   EMB_002   EMB_003   EMB_004   EMB_005   EMB_006  \\\n",
       "0 -0.006582 -0.030715 -0.033298  0.016711 -0.040304 -0.005820 -0.021985   \n",
       "1 -0.036613  0.010367 -0.022862  0.006432 -0.006352 -0.018229  0.041738   \n",
       "2 -0.035223 -0.008997 -0.017698 -0.005772  0.007104 -0.015288  0.020383   \n",
       "3 -0.060184 -0.048973 -0.050838  0.003790  0.006609 -0.016322  0.014977   \n",
       "4 -0.022729  0.017269 -0.014061  0.002197 -0.012464 -0.016102  0.032073   \n",
       "5 -0.000686 -0.009225 -0.024240  0.028939 -0.040605 -0.002884 -0.008143   \n",
       "6  0.001991 -0.020987 -0.000745  0.009669 -0.020101  0.001262 -0.004256   \n",
       "7 -0.048608 -0.041733 -0.051635  0.018432 -0.013950 -0.022629  0.013912   \n",
       "8  0.007821  0.013841 -0.006246  0.020809 -0.026219 -0.004054  0.004046   \n",
       "9 -0.099372 -0.035823 -0.051990 -0.008933  0.030894 -0.034492  0.063822   \n",
       "\n",
       "    EMB_007   EMB_008   EMB_009  ...  EMB_123_nan  EMB_124_nan  EMB_125_nan  \\\n",
       "0 -0.121384  0.036542  0.057989  ...    -0.026778     0.005259    -0.069812   \n",
       "1  0.000693 -0.023588 -0.007744  ...     0.025421     0.005437    -0.016584   \n",
       "2  0.018882 -0.012446 -0.004120  ...     0.024652     0.004940     0.005467   \n",
       "3 -0.005287  0.010419  0.010205  ...     0.026093     0.008958    -0.007763   \n",
       "4 -0.019775 -0.012860  0.006835  ...     0.016775     0.006311    -0.035626   \n",
       "5 -0.104462  0.008309  0.030034  ...    -0.020894     0.003838    -0.056699   \n",
       "6 -0.037489  0.006000  0.012287  ...    -0.016009    -0.001493    -0.029278   \n",
       "7 -0.053893 -0.002898  0.014815  ...     0.011014     0.013531    -0.033731   \n",
       "8 -0.059275 -0.009186  0.007548  ...    -0.008935     0.006211    -0.037140   \n",
       "9  0.069110 -0.029069 -0.023238  ...     0.064878     0.013081     0.014527   \n",
       "\n",
       "   EMB_126_nan  EMB_127_nan  train_test  pred_  \\\n",
       "0     0.046847    -0.005185        test      0   \n",
       "1    -0.010947     0.010507        test      1   \n",
       "2     0.002907     0.006057        test      1   \n",
       "3     0.043811    -0.002461        test      1   \n",
       "4    -0.004673     0.007754        test      0   \n",
       "5     0.004441     0.004468        test      0   \n",
       "6     0.022887    -0.006947        test      0   \n",
       "7     0.029080     0.001374        test      1   \n",
       "8    -0.015952     0.007983        test      0   \n",
       "9     0.025030     0.001513        test      1   \n",
       "\n",
       "                              prediction_score    prob_0    prob_1  \n",
       "0     [0.5150744928098547, 0.4849255071901452]  0.515074  0.484926  \n",
       "1     [0.4839683381813784, 0.5160316618186216]  0.483968  0.516032  \n",
       "2    [0.18866384155711147, 0.8113361584428885]  0.188664  0.811336  \n",
       "3  [0.0034548822989322048, 0.9965451177010678]  0.003455  0.996545  \n",
       "4    [0.6471172289045981, 0.35288277109540195]  0.647117  0.352883  \n",
       "5     [0.7347797741003542, 0.2652202258996458]  0.734780  0.265220  \n",
       "6     [0.6103225519052327, 0.3896774480947674]  0.610323  0.389677  \n",
       "7     [0.2262415791488076, 0.7737584208511924]  0.226242  0.773758  \n",
       "8    [0.7995174698140237, 0.20048253018597628]  0.799517  0.200483  \n",
       "9  [0.0044173301105442375, 0.9955826698894558]  0.004417  0.995583  \n",
       "\n",
       "[10 rows x 261 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asset 실행\n",
    "inference2_asset_structure=run(step, pipeline, alo.asset_structure)\n",
    "\n",
    "# inference2 asset의 결과 dataframe은 inference2_asset_structure.data['dataframe']으로 확인할 수 있습니다.  \n",
    "inference2_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b9e329-d954-411c-9c33-42656d29bf01",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [5] Output asset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e749a64d",
   "metadata": {},
   "source": [
    "AI content asset sequence 표준화에 따라 GCR 2.0.0부터는 train 및 output pipeline 모두 input asset으로 시작해 output asset으로 종료하도록 변경되었습니다.   \n",
    "Output asset은 pipeline의 산출물들을 올바른 위치에 옮겨주는 asset입니다. AI advisor 규약에 따라 내부적으로 미리 지정된 위치를 이용하므로 사용자가 별도로 입력해 줘야 할 parameter는 없습니다.   \n",
    "<br />\n",
    "\n",
    "#### 주요 Parameter\n",
    "* 사용자가 설정해 줘야 할 parameter 없음\n",
    "\n",
    "#### Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17905521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - readiness (1) - inference1 (2) - preprocess(3) - inference2(4) - output(5))\n",
    "step = 5\n",
    "alo.asset_structure = copy.deepcopy(inference2_asset_structure)\n",
    "alo.asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 result asset argument를 원하는 값으로 수정합니다.\n",
    "# asset_structure.args['x_columns'] = ['']\n",
    "alo.asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c2f5c",
   "metadata": {},
   "source": [
    "#### Output asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c899d49e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2024-02-18 12:47:42,316][ASSET][INFO][inference_pipeline][output]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2024-02-18 12:47:42\n",
      "- current step      : output\n",
      "- asset branch.     : output_dev\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : 2.1\n",
      "- load envs. keys   : dict_keys(['project_home', 'solution_metadata_version', 'artifacts', 'alo_version', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'pipeline', 'step', 'num_step', 'asset_branch', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys([])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'center_node_column', 'embedding_column', 'dataframe_org', 'columns_map', 'preprocess'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "{'note': 'score is mean of probability having label: 1', 'probability': {}, 'result': '1', 'score': 0.88}\n",
      "\u001b[93m[2024-02-18 12:47:42,327][ASSET][WARNING][inference_pipeline][output]: Please enter the << external_path - save_inference_artifacts_path >> in the experimental_plan.yaml.\u001b[0m\n",
      "\u001b[92m[2024-02-18 12:47:42,331][ASSET][INFO][inference_pipeline][output]: Successfully saved inference summary yaml. \n",
      " >> /home/jovyan/240216_aicontents_gcr_2.0.0/gcr/alo/.inference_artifacts/score/inference_summary.yaml\u001b[0m\n",
      "\u001b[94m[2024-02-18 12:47:42,339][ASSET][INFO][inference_pipeline][output]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2024-02-18 12:47:42\n",
      "- current step      : output\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'center_node_column', 'embedding_column', 'dataframe_org', 'columns_map', 'preprocess'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2024-02-18 12:47:42,342][PROCESS][INFO]: ==================== Finish pipeline: inference_pipeline / step: output\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_000</th>\n",
       "      <th>EMB_001</th>\n",
       "      <th>EMB_002</th>\n",
       "      <th>EMB_003</th>\n",
       "      <th>EMB_004</th>\n",
       "      <th>EMB_005</th>\n",
       "      <th>EMB_006</th>\n",
       "      <th>EMB_007</th>\n",
       "      <th>EMB_008</th>\n",
       "      <th>EMB_009</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_123_nan</th>\n",
       "      <th>EMB_124_nan</th>\n",
       "      <th>EMB_125_nan</th>\n",
       "      <th>EMB_126_nan</th>\n",
       "      <th>EMB_127_nan</th>\n",
       "      <th>train_test</th>\n",
       "      <th>pred_</th>\n",
       "      <th>prediction_score</th>\n",
       "      <th>prob_0</th>\n",
       "      <th>prob_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.006582</td>\n",
       "      <td>-0.030715</td>\n",
       "      <td>-0.033298</td>\n",
       "      <td>0.016711</td>\n",
       "      <td>-0.040304</td>\n",
       "      <td>-0.005820</td>\n",
       "      <td>-0.021985</td>\n",
       "      <td>-0.121384</td>\n",
       "      <td>0.036542</td>\n",
       "      <td>0.057989</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026778</td>\n",
       "      <td>0.005259</td>\n",
       "      <td>-0.069812</td>\n",
       "      <td>0.046847</td>\n",
       "      <td>-0.005185</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5150744928098547, 0.4849255071901452]</td>\n",
       "      <td>0.515074</td>\n",
       "      <td>0.484926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.036613</td>\n",
       "      <td>0.010367</td>\n",
       "      <td>-0.022862</td>\n",
       "      <td>0.006432</td>\n",
       "      <td>-0.006352</td>\n",
       "      <td>-0.018229</td>\n",
       "      <td>0.041738</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>-0.023588</td>\n",
       "      <td>-0.007744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025421</td>\n",
       "      <td>0.005437</td>\n",
       "      <td>-0.016584</td>\n",
       "      <td>-0.010947</td>\n",
       "      <td>0.010507</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.4839683381813784, 0.5160316618186216]</td>\n",
       "      <td>0.483968</td>\n",
       "      <td>0.516032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.035223</td>\n",
       "      <td>-0.008997</td>\n",
       "      <td>-0.017698</td>\n",
       "      <td>-0.005772</td>\n",
       "      <td>0.007104</td>\n",
       "      <td>-0.015288</td>\n",
       "      <td>0.020383</td>\n",
       "      <td>0.018882</td>\n",
       "      <td>-0.012446</td>\n",
       "      <td>-0.004120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024652</td>\n",
       "      <td>0.004940</td>\n",
       "      <td>0.005467</td>\n",
       "      <td>0.002907</td>\n",
       "      <td>0.006057</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.18866384155711147, 0.8113361584428885]</td>\n",
       "      <td>0.188664</td>\n",
       "      <td>0.811336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.060184</td>\n",
       "      <td>-0.048973</td>\n",
       "      <td>-0.050838</td>\n",
       "      <td>0.003790</td>\n",
       "      <td>0.006609</td>\n",
       "      <td>-0.016322</td>\n",
       "      <td>0.014977</td>\n",
       "      <td>-0.005287</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.010205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026093</td>\n",
       "      <td>0.008958</td>\n",
       "      <td>-0.007763</td>\n",
       "      <td>0.043811</td>\n",
       "      <td>-0.002461</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0034548822989322048, 0.9965451177010678]</td>\n",
       "      <td>0.003455</td>\n",
       "      <td>0.996545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.022729</td>\n",
       "      <td>0.017269</td>\n",
       "      <td>-0.014061</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>-0.012464</td>\n",
       "      <td>-0.016102</td>\n",
       "      <td>0.032073</td>\n",
       "      <td>-0.019775</td>\n",
       "      <td>-0.012860</td>\n",
       "      <td>0.006835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016775</td>\n",
       "      <td>0.006311</td>\n",
       "      <td>-0.035626</td>\n",
       "      <td>-0.004673</td>\n",
       "      <td>0.007754</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6471172289045981, 0.35288277109540195]</td>\n",
       "      <td>0.647117</td>\n",
       "      <td>0.352883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.000686</td>\n",
       "      <td>-0.009225</td>\n",
       "      <td>-0.024240</td>\n",
       "      <td>0.028939</td>\n",
       "      <td>-0.040605</td>\n",
       "      <td>-0.002884</td>\n",
       "      <td>-0.008143</td>\n",
       "      <td>-0.104462</td>\n",
       "      <td>0.008309</td>\n",
       "      <td>0.030034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020894</td>\n",
       "      <td>0.003838</td>\n",
       "      <td>-0.056699</td>\n",
       "      <td>0.004441</td>\n",
       "      <td>0.004468</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7347797741003542, 0.2652202258996458]</td>\n",
       "      <td>0.734780</td>\n",
       "      <td>0.265220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001991</td>\n",
       "      <td>-0.020987</td>\n",
       "      <td>-0.000745</td>\n",
       "      <td>0.009669</td>\n",
       "      <td>-0.020101</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>-0.004256</td>\n",
       "      <td>-0.037489</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.012287</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016009</td>\n",
       "      <td>-0.001493</td>\n",
       "      <td>-0.029278</td>\n",
       "      <td>0.022887</td>\n",
       "      <td>-0.006947</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6103225519052327, 0.3896774480947674]</td>\n",
       "      <td>0.610323</td>\n",
       "      <td>0.389677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.048608</td>\n",
       "      <td>-0.041733</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>0.018432</td>\n",
       "      <td>-0.013950</td>\n",
       "      <td>-0.022629</td>\n",
       "      <td>0.013912</td>\n",
       "      <td>-0.053893</td>\n",
       "      <td>-0.002898</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011014</td>\n",
       "      <td>0.013531</td>\n",
       "      <td>-0.033731</td>\n",
       "      <td>0.029080</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2262415791488076, 0.7737584208511924]</td>\n",
       "      <td>0.226242</td>\n",
       "      <td>0.773758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.007821</td>\n",
       "      <td>0.013841</td>\n",
       "      <td>-0.006246</td>\n",
       "      <td>0.020809</td>\n",
       "      <td>-0.026219</td>\n",
       "      <td>-0.004054</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>-0.059275</td>\n",
       "      <td>-0.009186</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008935</td>\n",
       "      <td>0.006211</td>\n",
       "      <td>-0.037140</td>\n",
       "      <td>-0.015952</td>\n",
       "      <td>0.007983</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7995174698140237, 0.20048253018597628]</td>\n",
       "      <td>0.799517</td>\n",
       "      <td>0.200483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.099372</td>\n",
       "      <td>-0.035823</td>\n",
       "      <td>-0.051990</td>\n",
       "      <td>-0.008933</td>\n",
       "      <td>0.030894</td>\n",
       "      <td>-0.034492</td>\n",
       "      <td>0.063822</td>\n",
       "      <td>0.069110</td>\n",
       "      <td>-0.029069</td>\n",
       "      <td>-0.023238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064878</td>\n",
       "      <td>0.013081</td>\n",
       "      <td>0.014527</td>\n",
       "      <td>0.025030</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0044173301105442375, 0.9955826698894558]</td>\n",
       "      <td>0.004417</td>\n",
       "      <td>0.995583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 261 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    EMB_000   EMB_001   EMB_002   EMB_003   EMB_004   EMB_005   EMB_006  \\\n",
       "0 -0.006582 -0.030715 -0.033298  0.016711 -0.040304 -0.005820 -0.021985   \n",
       "1 -0.036613  0.010367 -0.022862  0.006432 -0.006352 -0.018229  0.041738   \n",
       "2 -0.035223 -0.008997 -0.017698 -0.005772  0.007104 -0.015288  0.020383   \n",
       "3 -0.060184 -0.048973 -0.050838  0.003790  0.006609 -0.016322  0.014977   \n",
       "4 -0.022729  0.017269 -0.014061  0.002197 -0.012464 -0.016102  0.032073   \n",
       "5 -0.000686 -0.009225 -0.024240  0.028939 -0.040605 -0.002884 -0.008143   \n",
       "6  0.001991 -0.020987 -0.000745  0.009669 -0.020101  0.001262 -0.004256   \n",
       "7 -0.048608 -0.041733 -0.051635  0.018432 -0.013950 -0.022629  0.013912   \n",
       "8  0.007821  0.013841 -0.006246  0.020809 -0.026219 -0.004054  0.004046   \n",
       "9 -0.099372 -0.035823 -0.051990 -0.008933  0.030894 -0.034492  0.063822   \n",
       "\n",
       "    EMB_007   EMB_008   EMB_009  ...  EMB_123_nan  EMB_124_nan  EMB_125_nan  \\\n",
       "0 -0.121384  0.036542  0.057989  ...    -0.026778     0.005259    -0.069812   \n",
       "1  0.000693 -0.023588 -0.007744  ...     0.025421     0.005437    -0.016584   \n",
       "2  0.018882 -0.012446 -0.004120  ...     0.024652     0.004940     0.005467   \n",
       "3 -0.005287  0.010419  0.010205  ...     0.026093     0.008958    -0.007763   \n",
       "4 -0.019775 -0.012860  0.006835  ...     0.016775     0.006311    -0.035626   \n",
       "5 -0.104462  0.008309  0.030034  ...    -0.020894     0.003838    -0.056699   \n",
       "6 -0.037489  0.006000  0.012287  ...    -0.016009    -0.001493    -0.029278   \n",
       "7 -0.053893 -0.002898  0.014815  ...     0.011014     0.013531    -0.033731   \n",
       "8 -0.059275 -0.009186  0.007548  ...    -0.008935     0.006211    -0.037140   \n",
       "9  0.069110 -0.029069 -0.023238  ...     0.064878     0.013081     0.014527   \n",
       "\n",
       "   EMB_126_nan  EMB_127_nan  train_test  pred_  \\\n",
       "0     0.046847    -0.005185        test      0   \n",
       "1    -0.010947     0.010507        test      1   \n",
       "2     0.002907     0.006057        test      1   \n",
       "3     0.043811    -0.002461        test      1   \n",
       "4    -0.004673     0.007754        test      0   \n",
       "5     0.004441     0.004468        test      0   \n",
       "6     0.022887    -0.006947        test      0   \n",
       "7     0.029080     0.001374        test      1   \n",
       "8    -0.015952     0.007983        test      0   \n",
       "9     0.025030     0.001513        test      1   \n",
       "\n",
       "                              prediction_score    prob_0    prob_1  \n",
       "0     [0.5150744928098547, 0.4849255071901452]  0.515074  0.484926  \n",
       "1     [0.4839683381813784, 0.5160316618186216]  0.483968  0.516032  \n",
       "2    [0.18866384155711147, 0.8113361584428885]  0.188664  0.811336  \n",
       "3  [0.0034548822989322048, 0.9965451177010678]  0.003455  0.996545  \n",
       "4    [0.6471172289045981, 0.35288277109540195]  0.647117  0.352883  \n",
       "5     [0.7347797741003542, 0.2652202258996458]  0.734780  0.265220  \n",
       "6     [0.6103225519052327, 0.3896774480947674]  0.610323  0.389677  \n",
       "7     [0.2262415791488076, 0.7737584208511924]  0.226242  0.773758  \n",
       "8    [0.7995174698140237, 0.20048253018597628]  0.799517  0.200483  \n",
       "9  [0.0044173301105442375, 0.9955826698894558]  0.004417  0.995583  \n",
       "\n",
       "[10 rows x 261 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asset 실행\n",
    "output_asset_structure=run(step, pipeline, alo.asset_structure)\n",
    "\n",
    "# output asset의 결과 dataframe은 output_asset_structure.data['dataframe']으로 확인할 수 있습니다.\n",
    "output_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36aa7cc-9b57-48ea-bec5-2b3cb0c02946",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **4. Batch Running**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ba7a95-1c7b-4e33-8eab-3362dcd7402e",
   "metadata": {},
   "source": [
    "Asset 단위가 아닌 전체 workflows에 대해 한번에 동작 시킬 수 있습니다.   \n",
    "<br />\n",
    "\n",
    "*Sample Notebook에서 반영한 parameter는 experimental_plan.yaml에 반영되지 않습니다.*   \n",
    "*config/experimental_plan.yaml을 직접 수정하여 사용하시길 바랍니다.*   \n",
    "<br />\n",
    "\n",
    "***NOTE!***\n",
    "<br />\n",
    "ALO 2.1 기준으로 설명된 이전 chapter들과 달리, batch running chapter에서는 ALO 2.2 기준 사용법을 설명합니다.  \n",
    "ALO 2.1과 2.2는 설치 방법과 환경 설정이 다르므로, batch running 시에는 sample notebook을 위해 설치한 개발 환경을 이용할 수 없으므로, 아래 guide에 따라 별도의 개발 환경을 준비해 주시기 바랍니다.\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd3eefc",
   "metadata": {},
   "source": [
    "### 4.1. 데이터 준비\n",
    "\n",
    "#### GCR을 사용하기 위해서는 아래와 같은 방법으로 데이터를 준비해야 합니다.\n",
    "> 1. Train, Inference 두 개의 데이터셋을 준비합니다. 본 notebook에서는 GCR 설치 시 함께 제공되는 default sample data를 이용합니다.  \n",
    "> 2. GCR은 supervised learning을 제공하는 AI content이므로 train set에는 label에 해당하는 column이 존재해야 합니다. 또한, label에 해당하는 column은 결측치가 있어서는 안됩니다.\n",
    "> 3. Train set과 inference set은 label column을 제외하면 column명 list가 일치해야 합니다.\n",
    "> 4. Graph 구성을 위해 사용자가 지정해줘야 할 center node column ('center_node_column')도 결측치가 있어서는 안됩니다.   \n",
    "\n",
    "***GCR은 Graph-powered ML을 제공하므로, label 및 center node columns 외의 column들에 대해서는 결측치에 대한 전처리가 불필요하며, 모든 columns에 대해 범주형 데이터에 대한 전처리도 필요하지 않습니다***   \n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db650f8",
   "metadata": {},
   "source": [
    "### 4.2. ALO와 GCR AI content 설치\n",
    "\n",
    "아래와 같이 ALO와 GCR AI content를 설치하여 개발 환경을 준비합니다.   \n",
    "<br />\n",
    "\n",
    "+ ***먼저 ALO를 설치합니다.***    \n",
    "<br />\n",
    "\n",
    "이 때, 사용할 GCR version에 맞는 ALO version 설치가 필요합니다.   \n",
    "본 sample notebook에서는 2024년 2월 현재 최신 GCR version인 2.0.0을 다루고 있으므로, 거기에 맞는 ALO version 2.2를 설치합니다.   \n",
    "GCR version과 ALO version 간 mapping 정보는 http://collab.lge.com/main/pages/viewpage.action?pageId=2338397990의 guide를 참고해 주십시오.\n",
    "<br />\n",
    "\n",
    "작업 directory를 준비합니다 (여기에서는 예를 들어 aisolution_gcr_2.0.0이라는 directory를 생성합니다).   \n",
    "<br />\n",
    "\n",
    "\\\\$ mkdir aisolution_gcr_2.0.0   \n",
    "\\\\$ cd aisolution_gcr_2.0.0   \n",
    "<br />\n",
    "\n",
    "AI solution name을 예를 들어 gcr_solution으로 하여 ALO를 설치합니다.   \n",
    "<br />\n",
    "\n",
    "\\\\$ git clone http://mod.lge.com/hub/dxadvtech/aicontents-framework/alo.git -b release-2.2 gcr_solution   \n",
    "\\\\$ cd gcr_solution   \n",
    "<br />\n",
    "\n",
    "gcr_solution이라는 AI solution 개발을 위한 가상환경을 만들어 줍니다.   \n",
    "이미 어떤 가상환경에 진입한 상태라면, conda deactivate를 수행해 해당 환경에서 빠져나간 뒤 수행해 주십시오.   \n",
    "<br />\n",
    "\n",
    "\\\\$ conda create -n gcr_solution python=3.10  => 3.10 필수   \n",
    "\\\\$ conda init bash   \n",
    "\\\\$ source ~/.bashrc => 이 명령은 아래 conda activate gcr_solution을 바로 수행했을 때 동작하지 않는 경우에만 수행해 주십시오.   \n",
    "\\\\$ conda activate gcr_solution   \n",
    "\\\\$ pip install -r requirements.txt   \n",
    "<br />\n",
    "\n",
    "+ ***ALO 설치가 완료되었으면, GCR 2.0.0을 설치합니다.***   \n",
    "<br />\n",
    "\n",
    "\\\\$ git clone http://mod.lge.com/hub/dxadvtech/aicontents/gcr.git solution   \n",
    "<br />\n",
    "\n",
    "GCR version 2.0.0이 올바로 설치되었는지 확인합니다.   \n",
    "<br />\n",
    "\n",
    "\\\\$ cd solution   \n",
    "\\\\$ git status => 결과가 'On branch release-2.0.0'이 맞는 지 확인합니다.   \n",
    "<br />\n",
    "\n",
    "만일 다른 version이 설치되었다면, release-2.0.0을 다시 설치합니다.   \n",
    "<br />\n",
    "\n",
    "\\\\$ cd ..   \n",
    "\\\\$ \\rm -rf solution   \n",
    "\\\\$ git clone -b release-2.0.0 --single-branch http://mod.lge.com/hub/dxadvtech/aicontents/gcr.git solution   \n",
    "<br />\n",
    "\n",
    "Default로 제공되는 sample data 대신 다른 data를 이용하려면 아래와 같이 experimental_plan.yaml을 수정합니다.   \n",
    "<br />\n",
    "\n",
    "\\\\$ vi aisolution_gcr_2.0.0/gcr_solution/solution/experimental_plan.yaml   \n",
    "<br />\n",
    "\n",
    "external_path의 load_train_data_path에 아래와 같이 사용할 데이터의 경로(디렉토리)를 입력합니다.   \n",
    "<br />\n",
    "\n",
    ">```   \n",
    ">external_path:   \n",
    ">    - load_train_data_path: /nas001/gcr_test_data/sample/   \n",
    ">    - load_inference_data_path:   \n",
    ">    - save_train_artifacts_path:   \n",
    ">    - save_inference_artifacts_path:   \n",
    ">```   \n",
    "<br />\n",
    "\n",
    "또한 GCR의 동작 설정 변경을 원할 경우에도 experimental_plan.yaml 내의 필수 변경 parameter를 변경합니다. 나머지 parameter는 컨텐츠 yaml에 제공된 default 값을 사용해도 괜찮습니다.   \n",
    "<br />\n",
    "\n",
    "+ ***Sample Jupyter notebook인 'GCR_asset_run_template.ipynb'을 수행하기 위해, ipykernel을 설치해 줍니다.***   \n",
    "<br />\n",
    "\n",
    "\\\\$ pip install ipykernel   \n",
    "\\\\$ python -m ipykernel install --user --name gcr_solution   \n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b413f8cc",
   "metadata": {},
   "source": [
    "### 4.3. Batch Running\n",
    "\n",
    "이상과 같이 설치한 ALO와 GCR AI content를 main.py를 이용해 batch running합니다.   \n",
    "여기서 주어진 문제를 위해 input과 output asset들이 customized되어 있다면 이것이 곧 GCR 기반의 AI solution입니다.     \n",
    "<br />\n",
    "\n",
    "\\\\$ cd aisolution_gcr_2.0.0/gcr_solution   \n",
    "\\\\$ python main.py   \n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a35634d-a522-4903-bc86-a89bb7303d90",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **5. 문의 및 기능 개발 요청**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7446e64b-5a59-42da-bcb2-bd7ed88e0ec3",
   "metadata": {},
   "source": [
    "사용중 **Issue 발생** 또는 **기능 요청** 건이 있으실 경우 아래 CLM을 통해 문의/요청 바랍니다.   \n",
    "CLM : http://clm.lge.com/issue/projects/DXADVTECH/\n",
    "\n",
    "담당자: 공성우 선임, 김정원 연구원, 김수경 책임\n",
    "\n",
    "*긴급한 건에 대해서는 담당자에게 연락 바랍니다.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf86e25-352f-434e-88b7-4a4d7deff069",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **6. References**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0296eed9-6dc4-4de8-bb8d-d2f81c8014d7",
   "metadata": {},
   "source": [
    "GCR Release Note : http://collab.lge.com/main/pages/viewpage.action?pageId=2178779272\n",
    "\n",
    "User Guide : http://collab.lge.com/main/pages/viewpage.action?pageId=2184972859\n",
    "\n",
    "데이터 명세서 : http://collab.lge.com/main/pages/viewpage.action?pageId=2184972864\n",
    "\n",
    "알고리즘 설명서 : http://collab.lge.com/main/pages/viewpage.action?pageId=2184972902\n",
    "\n",
    "GCR Contents Git : http://mod.lge.com/hub/dxadvtech/aicontents/gcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d0ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcr",
   "language": "python",
   "name": "gcr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
