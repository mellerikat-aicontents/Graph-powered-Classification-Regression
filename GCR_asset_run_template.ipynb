{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "194fe113-d67a-476d-ae96-7f72605a4af1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Jupyter Notebook for GCR**\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a9faf9-02ec-4c90-9af4-dfe0f02f29cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Table of Content**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbe8002-b1cf-432b-8a26-4e8472c83df9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    ">### 0. Introduction\n",
    ">### 1. 환경 구성\n",
    ">### 2. Train Workflow\n",
    ">### 3. Inference Workflow\n",
    ">### 4. Batch Running\n",
    ">### 5. 문의 및 기능 개발 요청\n",
    ">### 6. References   \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adcc16c-5678-4a69-9a45-9f46f6162e44",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **0. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d751b6-26d5-4ec6-bd94-f44dc1599898",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 본 sample notebook은 GCR의 구조와 각 asset의 역할과 산출물, 그 사용법을 처음 접하는 분들이 알기 쉽게 이해할 수 있도록 제작되었습니다.   \n",
    "\n",
    "[ **1. 환경 구성** ]에서는 ALO 등 환경 설치 방법을 설명하고,   \n",
    "[ **2. Train workflow** ]와 [ **3. Inference workflow** ]는 각각 train workflow와 inference workflow의 사용 방법과 산출물을 설명하며,   \n",
    "[ **4. Batch running** ]에서는 sample notebook이 아닌 실제 과제 운용 시에 GCR contents를 수행하는 방법을 설명하고   \n",
    "[ **5. 문의 및 기능 개발 요청** ]에서는 사용 중 문의 사항이나 기능에 대한 수정/개발 요청 방법을,   \n",
    "[ **6. References** ]에서는 추가로 참고하실 collab 문서 등에 대한 links를 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a97420-7def-4cf2-ae32-51ee8fbdf4fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Notebook의 Workflow는 다음과 같이 구성됩니다.\n",
    "> Workflow NAME (ex. Train Workflow)\n",
    "\n",
    ">> Workflow 구성 설명\n",
    ">>> **A** asset : A asset 설명   \n",
    ">>> **B** asset : B asset 설명   \n",
    ">>> ...\n",
    "\n",
    ">> Workflow Setup \n",
    ">>> Workflow Setup 코드\n",
    "\n",
    ">> [0] **A** asset\n",
    ">>> parameter 설명 및 실행 코드 \n",
    "\n",
    ">> [1] **B** asset\n",
    ">>> parameter 설명 및 실행 코드\n",
    "\n",
    ">> ..\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f392e9ef-2821-4b89-9499-9d7932f2d0b3",
   "metadata": {},
   "source": [
    "#### 각 Asset은 동작확인을 위해 다음과 같이 구성됩니다.\n",
    "> ASSET NAME\n",
    "\n",
    ">> 주요 Parameter 설명\n",
    ">>> param1: param1 설명   \n",
    ">>> ***param2***: param2 설명 [*option1 / option2 / option3*]   \n",
    ">>> param3: param3 설명 [*option1 / option2*]   \n",
    ">>> ..\n",
    "\n",
    ">> Parameter 설정부\n",
    ">>> #################   \n",
    ">>> parameter 설정 코드   \n",
    ">>> #################   \n",
    "\n",
    ">> Asset 실행부\n",
    ">>> #################   \n",
    ">>> Asset 실행 코드   \n",
    ">>> #################   \n",
    "\n",
    "Asset 별로 experimental_plan.yaml에 주어진 parameter에 대한 설명이 주어집니다.   \n",
    "설명에 따라 parameter 변경 시 experimental_plan.yaml을 직접 수정하거나, Parameter 설정부에서 코드 실행을 통해 바꿀 수 있습니다.   \n",
    "위에서 param2와 같이 ***Bold Italic***으로 표기된 변수는 필수 설정 변수로, 최초 실행 또는 데이터가 바뀔 시 꼭 다시 설정해주어야 하는 변수를 의미합니다.   \n",
    "설정된 parameter로 asset을 실행할 수 있습니다. 또한 Parameter를 변경해가며 Asset 실행 결과 변화를 관찰할 수 있습니다.\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d624da5c-a13c-4d2d-946a-697c7aced3f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **1. 환경 구성**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3858a5e5-db47-49aa-bcf6-08fcb4561ed2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### GCR을 사용하기 위해서는 아래와 같은 방법으로 데이터를 준비해야 합니다.\n",
    "> 1. Train, Inference 두 개의 데이터셋을 준비합니다.\n",
    "> 2. 각 데이터에 FLAG_TRAIN_INFERENCE 컬럼을 추가합니다. \n",
    ">    - 각 'Train', 'Inference'가 flag로 들어가야 합니다.\n",
    "> 3. 두 데이터를 합쳐 하나의 데이터셋으로 구성합니다.\n",
    "\n",
    "***GCR은 결측치와 범주형 데이터에 대한 전처리가 필요하지 않습니다***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e07bc62-da8e-4304-80b5-0790dbbdb0b2",
   "metadata": {},
   "source": [
    "#### ALO 설치 및 configuration 설정 방법은 다음과 같습니다.\n",
    "\n",
    "1. 최상위 디렉토리에서 install.sh를 실행합니다\n",
    "> source install.sh\n",
    "2. install.sh를 실행하면 alo 디렉토리가 설치됩니다.\n",
    "3. 가상환경을 설치 및 실행합니다.\n",
    "> conda create -n gcr python=3.10   \n",
    "> conda init bash   \n",
    "> conda activate gcr    \n",
    "3. alo/config 디렉토리로 이동하여 experimental_plan.yaml 파일을 오픈합니다.\n",
    "4. external_path의 load_train_data_path에 아래와 같이 사용할 데이터의 경로(디렉토리)를 입력합니다.\n",
    "\n",
    ">```\n",
    ">external_path:\n",
    ">    - load_train_data_path: /nas001/gcr_test_data/sample/\n",
    ">    - load_inference_data_path:\n",
    ">    - save_train_artifacts_path:\n",
    ">    - save_inference_artifacts_path:\n",
    ">```\n",
    "\n",
    "5. 필수 변경 parameter를 변경합니다. 나머지 parameter는 컨텐츠 yaml에 제공된 default 값을 사용해도 괜찮습니다.\n",
    "6. 아래 **ALO Setup**을 실행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b6b5c-8668-46b6-b5d8-00888c5580fa",
   "metadata": {},
   "source": [
    "### ALO Setup\n",
    "라이브러리 설치 및 컨텐츠 다운로드를 위해 아래 코드를 실행 해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "137cd365-9ed9-4941-aa2e-67fca06e1a4f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[2023-11-17 06:31:54,605][PROCESS][INFO]: Success versioning up experimental_plan.yaml : 2.0 --> 2.1 (version ref. : compare yaml version)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "os.chdir(os.path.abspath(os.path.join('./alo')))\n",
    "from src.alo import ALO\n",
    "from src.alo import AssetStructure\n",
    "alo = ALO(); alo.preset(); pipelines = list(alo.asset_source.keys())\n",
    "from src.external import external_load_data, external_save_artifacts\n",
    "\n",
    "def run(step, pipeline, asset_structure):\n",
    "    # 반복되는 작업을 함수로 변환\n",
    "    asset_config = alo.asset_source[pipeline]\n",
    "    return alo.process_asset_step(asset_config[step], step, pipeline, asset_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7337fdb-836f-4519-869b-76c6089743d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **2. Train Workflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171f4e9-d318-47ec-8374-f49fb6b30356",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### GCR의 Train Workflow 구성은 다음과 같습니다.\n",
    "> **[0]** Input asset : *사용자가 지정한 경로로부터 데이터를 Import*   \n",
    "> **[1]** Graph asset : *데이터를 토대로 그래프를 구성하고 필요한 임베딩 추출*   \n",
    "> **[2]** Preprocess asset : *(필요시) 결측치 처리 및 라벨 인코딩*   \n",
    "> **[3]** Sampling asset : *(필요시) Imbalance 데이터의 Undersampling*   \n",
    "> **[4]** Train asset : *ML 모델 학습*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2425131a-066e-40e4-99d9-d90e3c858fac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Train Workflow Setup\n",
    "아래 코드를 실행하여 Train Workflow에 필요한 라이브러리를 먼저 설치 해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b7d0f4d-05da-40d0-9b5d-75c496b5794a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2023-11-17 06:31:55,903][PROCESS][INFO]: You did not write any << s3_private_key_file >> in the config yaml file. When you wanna get data from s3 storage, \n",
      "                                 you have to write the s3_private_key_file path or set << AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY >> in your os environment. \n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:55,951][PROCESS][INFO]:  Skip loading external data. << /nas001/users/seongwoo.kong/gcr_test_data/sample/ >> \n",
      " << sample >> already exists in << /home/jovyan/gcr/alo/input/train/ >>. \n",
      " & << get_external_data >> is set as << once >>. \n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:55,954][PROCESS][INFO]: Start setting-up << input >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:55,967][PROCESS][INFO]: << input >> asset had already been created at 2023-11-16 02:04:56.709955\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:55,970][PROCESS][INFO]: Start setting-up << graph >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:55,987][PROCESS][INFO]: << graph >> asset had already been created at 2023-11-16 02:04:29.860787\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:55,990][PROCESS][INFO]: Start setting-up << preprocess >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,009][PROCESS][INFO]: << preprocess >> asset had already been created at 2023-11-16 02:04:30.524791\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,012][PROCESS][INFO]: Start setting-up << sampling >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,016][PROCESS][INFO]: << sampling >> asset had already been created at 2023-11-16 02:04:30.958793\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,019][PROCESS][INFO]: Start setting-up << train >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,034][PROCESS][INFO]: << train >> asset had already been created at 2023-11-16 02:04:31.333796\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,040][PROCESS][INFO]: >>> Ignored installing << torch==2.0.0 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,043][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,045][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,048][PROCESS][INFO]: >>> Ignored installing << numpy==1.25.2 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,050][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,053][PROCESS][INFO]: >>> Ignored installing << scikit-learn >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,055][PROCESS][INFO]: >>> Ignored installing << matplotlib >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,058][PROCESS][INFO]: ======================================== Start dependency installation : << input >> \u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,060][PROCESS][INFO]: Start checking existence & installing package - pandas==1.5.3 | Progress: ( 1 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-17 06:31:56,063][PROCESS][INFO]: [OK] << pandas==1.5.3 >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,066][PROCESS][INFO]: ======================================== Start dependency installation : << graph >> \u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,069][PROCESS][INFO]: Start checking existence & installing package - torch==2.0.0 | Progress: ( 2 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-17 06:31:56,072][PROCESS][INFO]: [OK] << torch==2.0.0 >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,074][PROCESS][INFO]: Start checking existence & installing package - torchbiggraph@git+https://github.com/facebookresearch/PyTorch-BigGraph.git | Progress: ( 3 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-17 06:31:56,077][PROCESS][INFO]: [OK] << torchbiggraph@git+https://github.com/facebookresearch/PyTorch-BigGraph.git >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,079][PROCESS][INFO]: ======================================== Start dependency installation : << preprocess >> \u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,081][PROCESS][INFO]: Start checking existence & installing package - category_encoders | Progress: ( 4 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-17 06:31:56,084][PROCESS][INFO]: [OK] << category_encoders >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,087][PROCESS][INFO]: ======================================== Start dependency installation : << sampling >> \u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,089][PROCESS][INFO]: Start checking existence & installing package - numpy==1.25.2 | Progress: ( 5 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-17 06:31:56,092][PROCESS][INFO]: [OK] << numpy==1.25.2 >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,095][PROCESS][INFO]: Start checking existence & installing package - numba==0.58.0 | Progress: ( 6 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-17 06:31:56,098][PROCESS][INFO]: [OK] << numba==0.58.0 >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,101][PROCESS][INFO]: Start checking existence & installing package - scikit-learn | Progress: ( 7 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-17 06:31:56,103][PROCESS][INFO]: [OK] << scikit-learn >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,107][PROCESS][INFO]: Start checking existence & installing package - umap-learn | Progress: ( 8 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-17 06:31:56,110][PROCESS][INFO]: [OK] << umap-learn >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,113][PROCESS][INFO]: Start checking existence & installing package - matplotlib | Progress: ( 9 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-17 06:31:56,115][PROCESS][INFO]: [OK] << matplotlib >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,118][PROCESS][INFO]: ======================================== Start dependency installation : << train >> \u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,120][PROCESS][INFO]: Start checking existence & installing package - seaborn | Progress: ( 10 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-17 06:31:56,122][PROCESS][INFO]: [OK] << seaborn >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,125][PROCESS][INFO]: Start checking existence & installing package - shap | Progress: ( 11 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-17 06:31:56,127][PROCESS][INFO]: [OK] << shap >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,130][PROCESS][INFO]: Start checking existence & installing package - lightgbm | Progress: ( 12 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-17 06:31:56,133][PROCESS][INFO]: [OK] << lightgbm >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,135][PROCESS][INFO]: Start checking existence & installing package - catboost | Progress: ( 13 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-17 06:31:56,137][PROCESS][INFO]: [OK] << catboost >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,140][PROCESS][INFO]: Start checking existence & installing package - ngboost | Progress: ( 14 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-17 06:31:56,142][PROCESS][INFO]: [OK] << ngboost >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,144][PROCESS][INFO]: ======================================== Start dependency installation : << force-reinstall >> \u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,147][PROCESS][INFO]: Start checking existence & installing package - numpy==1.25.2 --force-reinstall | Progress: ( 15 / 15 total packages ) \u001b[0m\n",
      "\u001b[94m[2023-11-17 06:31:56,149][PROCESS][INFO]: >>> Start installing package - numpy==1.25.2 --force-reinstall\u001b[0m\n",
      "Collecting numpy==1.25.2\n",
      "  Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.2\n",
      "    Uninstalling numpy-1.25.2:\n",
      "      Successfully uninstalled numpy-1.25.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/~-%py.libs'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/~-%py'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed numpy-1.25.2\n",
      "\u001b[94m[2023-11-17 06:32:13,767][PROCESS][INFO]: ======================================== Finish dependency installation \n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "external_load_data(pipelines[0], alo.external_path, alo.external_path_permission, alo.control['get_external_data'])\n",
    "pipeline = pipelines[0]\n",
    "alo.install_steps(pipeline, alo.control[\"get_asset_source\"])\n",
    "\n",
    "# 초기 data structure 구성\n",
    "envs, args, data, config = {}, {}, {}, {}\n",
    "init_asset_structure = AssetStructure(envs, args, data, config)\n",
    "# logger init\n",
    "alo.set_proc_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c18b98-847e-4c58-848c-bec26204375f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [0] Input asset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9f36d6-84e0-440e-80d3-b462254b006a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 주요 Parameter\n",
    "- ***input_path*** : Extenal Path에서 받은 데이터는 *alo/input/train*에 저장됩니다. 이 중에서 사용할 데이터가 저장된 디렉터리 명을 작성해주시면 됩니다.\n",
    "- x_columns : 데이터의 모든 컬럼을 활용하지 않는 경우엔 직접 선택해서 사용할 수 있습니다.\n",
    "- use_all_x : 데이터의 모든 컬럼을 사용하는 경우 True로 설정하고 사용합니다. 이 경우 x_columns는 빈칸이어야 합니다. [*True / False*]\n",
    "- ***y_column*** : Classification, Regression을 위해서는 Label이 있어야 합니다. Label에 해당하는 컬럼을 작성합니다.\n",
    "- groupkey_columns : 특정 컬럼 명을 기준으로 데이터를 그룹으로 나누어 모델링을 하고 싶을 경우에 사용합니다.\n",
    "- drop_columns : use_all_x가 True일 때 삭제하고 싶은 컬럼을 입력합니다.\n",
    "- time_column : 데이터에 시간 컬럼이 있을 경우 입력합니다.\n",
    "- concat_dataframes : 같은 형태 csv 파일 여러 개를 input data로 불러올 시, concat 여부를 선택합니다. [*True / False*]\n",
    "- encoding : pd.read_csv() 시에 사용할 encoding 방법을 설정합니다.\n",
    "\n",
    "#### Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90e2c80e-0a14-4e19-b50f-2aa621aec2da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_path': 'sample',\n",
       " 'x_columns': None,\n",
       " 'use_all_x': True,\n",
       " 'y_column': 'is_married',\n",
       " 'groupkey_columns': None,\n",
       " 'drop_columns': None,\n",
       " 'time_column': None,\n",
       " 'concat_dataframes': None,\n",
       " 'encoding': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - graph(1) - preprocess(2) - sampling(3) - train(4))\n",
    "step = 0 \n",
    "asset_structure = copy.deepcopy(init_asset_structure)\n",
    "asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 input asset argument를 원하는 값으로 수정합니다. \n",
    "#asset_structure.args['x_columns'] = ['']\n",
    "asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017bbb5c-b15e-4b8b-98c9-9b355da55263",
   "metadata": {},
   "source": [
    "#### Input asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ac5a67f-02e5-49f5-9d44-34f24b08baaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-11-17 06:32:15,204][USER][INFO][train_pipeline][input]: >> Load path : ['/home/jovyan/gcr/alo/input/train/sample/']\n",
      "[2023-11-17 06:32:15,233][USER][INFO][train_pipeline][input]: >> The file for batch data has been loaded. (File name: /home/jovyan/gcr/alo/input/train/sample/customers.csv)\n",
      "[2023-11-17 06:32:15,236][USER][INFO][train_pipeline][input]: You set the << use_all_x >> as << True >> in the yaml file. So skip checking dataframe columns existence.\n",
      "[2023-11-17 06:32:15,240][USER][INFO][train_pipeline][input]: ==================== Success loading dataframe ====================\n",
      "[2023-11-17 06:32:15,243][USER][INFO][train_pipeline][input]: >> Start processing ignore columns & drop columns: ['/home/jovyan/gcr/alo/input/train/sample/customers.csv']\n",
      "[2023-11-17 06:32:15,247][USER][INFO][train_pipeline][input]: >> You set the << use_all_x >> parameter as << True >> in your config yaml. (So, these x_columns are used: ['gender', 'FLAG_TRAIN_INFERENCE', 'hobbies', 'spent', 'name', 'address', 'job', 'age', 'orders'] )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2023-11-17 06:32:15,199][ASSET][INFO][train_pipeline][input]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2023-11-17 06:32:15\n",
      "- current step      : input\n",
      "- asset branch.     : tabular_2.0\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : release-2.1\n",
      "- load envs. keys   : dict_keys(['solution_metadata_version', 'project_home', 'pipeline', 'step', 'num_step', 'artifacts', 'alo_version', 'asset_branch', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path'])\n",
      "- load args. keys   : dict_keys(['input_path', 'x_columns', 'use_all_x', 'y_column', 'groupkey_columns', 'drop_columns', 'time_column', 'concat_dataframes', 'encoding'])\n",
      "- load config. keys : dict_keys(['meta'])\n",
      "- load data keys    : dict_keys([])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:32:15,248][ASSET][INFO][train_pipeline][input]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2023-11-17 06:32:15\n",
      "- current step      : input\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:32:15,250][PROCESS][INFO]: ==================== Finish pipeline: train_pipeline / step: input\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>orders</th>\n",
       "      <th>spent</th>\n",
       "      <th>job</th>\n",
       "      <th>hobbies</th>\n",
       "      <th>is_married</th>\n",
       "      <th>FLAG_TRAIN_INFERENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jasmine_Young</td>\n",
       "      <td>TN17745</td>\n",
       "      <td>female</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>233.44</td>\n",
       "      <td>Receptionist</td>\n",
       "      <td>Photography</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jeffery_Robinson</td>\n",
       "      <td>CT69980</td>\n",
       "      <td>male</td>\n",
       "      <td>42</td>\n",
       "      <td>15</td>\n",
       "      <td>264.70</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>Fishing</td>\n",
       "      <td>True</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Steven_Sullivan</td>\n",
       "      <td>CT13314</td>\n",
       "      <td>male</td>\n",
       "      <td>70</td>\n",
       "      <td>13</td>\n",
       "      <td>339.10</td>\n",
       "      <td>Janitor</td>\n",
       "      <td>Hiking</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jay_Williams</td>\n",
       "      <td>TN68283</td>\n",
       "      <td>male</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>70.61</td>\n",
       "      <td>Waitress</td>\n",
       "      <td>Playing musical instruments</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Benjamin_Beck</td>\n",
       "      <td>AE11377</td>\n",
       "      <td>male</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>748.94</td>\n",
       "      <td>Farmer</td>\n",
       "      <td>Playing sports</td>\n",
       "      <td>True</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gregory_Gomez</td>\n",
       "      <td>FM04887</td>\n",
       "      <td>male</td>\n",
       "      <td>75</td>\n",
       "      <td>10</td>\n",
       "      <td>937.97</td>\n",
       "      <td>Unkown</td>\n",
       "      <td>Running</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mary_Harris</td>\n",
       "      <td>KS55063</td>\n",
       "      <td>female</td>\n",
       "      <td>60</td>\n",
       "      <td>12</td>\n",
       "      <td>60.97</td>\n",
       "      <td>Librarian</td>\n",
       "      <td>Reading</td>\n",
       "      <td>True</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jimmy_Smith</td>\n",
       "      <td>AL47190</td>\n",
       "      <td>male</td>\n",
       "      <td>72</td>\n",
       "      <td>5</td>\n",
       "      <td>468.64</td>\n",
       "      <td>Waitress</td>\n",
       "      <td>Sewing</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kenneth_Rubio</td>\n",
       "      <td>RI07301</td>\n",
       "      <td>male</td>\n",
       "      <td>74</td>\n",
       "      <td>15</td>\n",
       "      <td>482.72</td>\n",
       "      <td>Polic</td>\n",
       "      <td>Dancing</td>\n",
       "      <td>True</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jordan_Simmons</td>\n",
       "      <td>AA06497</td>\n",
       "      <td>female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>156.16</td>\n",
       "      <td>Cashier</td>\n",
       "      <td>Baking</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name  address  gender  age  orders   spent           job  \\\n",
       "0     Jasmine_Young  TN17745  female   80       0  233.44  Receptionist   \n",
       "1  Jeffery_Robinson  CT69980    male   42      15  264.70       Teacher   \n",
       "2   Steven_Sullivan  CT13314    male   70      13  339.10       Janitor   \n",
       "3      Jay_Williams  TN68283    male   27       7   70.61      Waitress   \n",
       "4     Benjamin_Beck  AE11377    male   21       9  748.94        Farmer   \n",
       "5     Gregory_Gomez  FM04887    male   75      10  937.97        Unkown   \n",
       "6       Mary_Harris  KS55063  female   60      12   60.97     Librarian   \n",
       "7       Jimmy_Smith  AL47190    male   72       5  468.64      Waitress   \n",
       "8     Kenneth_Rubio  RI07301    male   74      15  482.72         Polic   \n",
       "9    Jordan_Simmons  AA06497  female   41       1  156.16       Cashier   \n",
       "\n",
       "                       hobbies is_married FLAG_TRAIN_INFERENCE  \n",
       "0                  Photography      False                TRAIN  \n",
       "1                      Fishing       True                TRAIN  \n",
       "2                       Hiking      False                TRAIN  \n",
       "3  Playing musical instruments      False                TRAIN  \n",
       "4               Playing sports       True                TRAIN  \n",
       "5                      Running      False                TRAIN  \n",
       "6                      Reading       True                TRAIN  \n",
       "7                       Sewing      False                TRAIN  \n",
       "8                      Dancing       True                TRAIN  \n",
       "9                       Baking      False                TRAIN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_asset_structure=run(step, pipeline, asset_structure)\n",
    "\n",
    "# input asset의 결과 dataframe은 input_asset_structure.data['dataframe']으로 확인할 수 있습니다. \n",
    "input_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e402b7cc-2391-45dd-8a0d-5be4eb8b9b8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [1] Graph asset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b53121",
   "metadata": {},
   "source": [
    "GCR의 가장 핵심이 되는 asset입니다. Raw Data를 임베딩으로 모두 변환하여 별도의 전처리 없이 더 높은 성능의 ML모델을 획득하기 위한 Tool이 됩니다.\n",
    "#### 주요 Parameter\n",
    "- graph_type : 그래프의 구조를 선택할 수 있습니다. radial은 방사형 구조, relational은 관계형 구조입니다. [*radial / relational*]\n",
    "- center_node_column : 방사형 그래프는 중심 컬럼을 기준으로 그래프를 구성합니다. 데이터의 대표가 되는 컬럼을 선택하면 됩니다.(ex. ID, Name 등)\n",
    "- embedding_column : 임베딩의 대상이 되는 컬럼을 선택합니다. 라벨이 center_node_column과 대응하지 않는 경우가 있을 수 있습니다. 이때는 label이 대응하는 컬럼을 embedding column으로 설정합니다.\n",
    "- drop_columns : 그래프 구성에서 제외할 컬럼을 선택합니다.\n",
    "- num_epochs: Embedding 학습을 위한 epoch을 설정합니다.\n",
    "- workers : worker process 수를 설정합니다.\n",
    "- num_partitions : partition 수를 설정합니다. 증가시켜 메모리 사용량을 줄일 수 있습니다.\n",
    "- extra_columns_for_ml : ml 모델 학습에 임베딩 외에 사용될 컬럼을 지정합니다. 도메인 지식에 기반하여 선택할 필요가 있습니다.\n",
    "- custom_connection : 그래프를 활용할 줄 안다면 직접 relation을 정의할 수 있습니다. 자세한 내용은 manual을 참고.\n",
    "\n",
    "#### Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05226c4c-981b-46e6-a187-72e3d47433f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'graph_type': None,\n",
       " 'center_node_column': 'name',\n",
       " 'embedding_column': None,\n",
       " 'train_inference_column': None,\n",
       " 'drop_columns': [],\n",
       " 'dimension': 128,\n",
       " 'num_epochs': None,\n",
       " 'workers': None,\n",
       " 'num_partitions': None,\n",
       " 'extra_columns_for_ml': [],\n",
       " 'custom_connection': []}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - graph(1) - preprocess(2) - sampling(3) - train(4))\n",
    "step = 1 \n",
    "asset_structure = copy.deepcopy(input_asset_structure)\n",
    "asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 graph asset argument를 원하는 값으로 수정합니다. \n",
    "asset_structure.args['dimension'] = 128\n",
    "asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2084ad",
   "metadata": {},
   "source": [
    "#### Graph asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be198a60-4793-4639-a709-3cb74263d1a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[2023-11-17 06:32:19,941][ASSET][INFO][train_pipeline][graph]: Successfully got << output path >> for saving your data into csv or jpg file: \n",
      " /home/jovyan/gcr/alo/.train_artifacts/output/graph/ \n",
      " - [NOTE] The names of output file must be fixed as << output.csv, output.jpg >> \u001b[0m\n",
      "\u001b[94m[2023-11-17 06:32:19,944][ASSET][INFO][train_pipeline][graph]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2023-11-17 06:32:19\n",
      "- current step      : graph\n",
      "- asset branch.     : release-1.2\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : release-2.1\n",
      "- load envs. keys   : dict_keys(['solution_metadata_version', 'project_home', 'pipeline', 'step', 'num_step', 'artifacts', 'alo_version', 'asset_branch', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['graph_type', 'center_node_column', 'embedding_column', 'train_inference_column', 'drop_columns', 'dimension', 'num_epochs', 'workers', 'num_partitions', 'extra_columns_for_ml', 'custom_connection'])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "preprocessing blank space...\n",
      "In __init__: pbg ready\n",
      "[2023-11-17 06:32:20.061433] Using the 7 relation types given in the config\n",
      "[2023-11-17 06:32:20.062647] Searching for the entities in the edge files...\n",
      "[2023-11-17 06:32:20.089078] Entity type address:\n",
      "[2023-11-17 06:32:20.089948] - Found 1000 entities\n",
      "[2023-11-17 06:32:20.090750] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-17 06:32:20.091686] - Left with 1000 entities\n",
      "[2023-11-17 06:32:20.092411] - Shuffling them...\n",
      "[2023-11-17 06:32:20.094360] Entity type name:\n",
      "[2023-11-17 06:32:20.095259] - Found 985 entities\n",
      "[2023-11-17 06:32:20.096017] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-17 06:32:20.097122] - Left with 985 entities\n",
      "[2023-11-17 06:32:20.097983] - Shuffling them...\n",
      "[2023-11-17 06:32:20.099853] Entity type age:\n",
      "[2023-11-17 06:32:20.100644] - Found 63 entities\n",
      "[2023-11-17 06:32:20.101384] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-17 06:32:20.102322] - Left with 63 entities\n",
      "[2023-11-17 06:32:20.103028] - Shuffling them...\n",
      "[2023-11-17 06:32:20.103962] Entity type job:\n",
      "[2023-11-17 06:32:20.104776] - Found 37 entities\n",
      "[2023-11-17 06:32:20.105445] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-17 06:32:20.106136] - Left with 37 entities\n",
      "[2023-11-17 06:32:20.106823] - Shuffling them...\n",
      "[2023-11-17 06:32:20.107551] Entity type hobbies:\n",
      "[2023-11-17 06:32:20.108219] - Found 27 entities\n",
      "[2023-11-17 06:32:20.108885] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-17 06:32:20.109566] - Left with 27 entities\n",
      "[2023-11-17 06:32:20.110260] - Shuffling them...\n",
      "[2023-11-17 06:32:20.110959] Entity type orders:\n",
      "[2023-11-17 06:32:20.111618] - Found 21 entities\n",
      "[2023-11-17 06:32:20.112280] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-17 06:32:20.112966] - Left with 21 entities\n",
      "[2023-11-17 06:32:20.113657] - Shuffling them...\n",
      "[2023-11-17 06:32:20.114431] Entity type gender:\n",
      "[2023-11-17 06:32:20.115099] - Found 2 entities\n",
      "[2023-11-17 06:32:20.115682] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-17 06:32:20.116289] - Left with 2 entities\n",
      "[2023-11-17 06:32:20.116879] - Shuffling them...\n",
      "[2023-11-17 06:32:20.122074] Entity type spent:\n",
      "[2023-11-17 06:32:20.122665] - Found 360 entities\n",
      "[2023-11-17 06:32:20.123238] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-17 06:32:20.123887] - Left with 360 entities\n",
      "[2023-11-17 06:32:20.124447] - Shuffling them...\n",
      "[2023-11-17 06:32:20.125474] Preparing counts and dictionaries for entities and relation types:\n",
      "[2023-11-17 06:32:20.126999] - Writing count of entity type address and partition 0\n",
      "[2023-11-17 06:32:20.133014] - Writing count of entity type name and partition 0\n",
      "[2023-11-17 06:32:20.137263] - Writing count of entity type age and partition 0\n",
      "[2023-11-17 06:32:20.139881] - Writing count of entity type job and partition 0\n",
      "[2023-11-17 06:32:20.142266] - Writing count of entity type hobbies and partition 0\n",
      "[2023-11-17 06:32:20.145071] - Writing count of entity type orders and partition 0\n",
      "[2023-11-17 06:32:20.147846] - Writing count of entity type gender and partition 0\n",
      "[2023-11-17 06:32:20.150464] - Writing count of entity type spent and partition 0\n",
      "[2023-11-17 06:32:20.154272] Preparing edge path /home/jovyan/gcr/alo/.train_artifacts/output/graph/partitions/edges_partitioned_rel_6, out of the edges found in /home/jovyan/gcr/alo/.train_artifacts/output/graph/tsvs/rel_6.tsv\n",
      "[2023-11-17 06:32:20.155226] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-11-17 06:32:21.727431] - Processed 985 edges in total\n",
      "[2023-11-17 06:32:21.729320] Preparing edge path /home/jovyan/gcr/alo/.train_artifacts/output/graph/partitions/edges_partitioned_rel_3, out of the edges found in /home/jovyan/gcr/alo/.train_artifacts/output/graph/tsvs/rel_3.tsv\n",
      "[2023-11-17 06:32:21.730606] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-11-17 06:32:23.282969] - Processed 999 edges in total\n",
      "[2023-11-17 06:32:23.284586] Preparing edge path /home/jovyan/gcr/alo/.train_artifacts/output/graph/partitions/edges_partitioned_rel_0, out of the edges found in /home/jovyan/gcr/alo/.train_artifacts/output/graph/tsvs/rel_0.tsv\n",
      "[2023-11-17 06:32:23.285748] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-11-17 06:32:24.832649] - Processed 1000 edges in total\n",
      "[2023-11-17 06:32:24.834325] Preparing edge path /home/jovyan/gcr/alo/.train_artifacts/output/graph/partitions/edges_partitioned_rel_5, out of the edges found in /home/jovyan/gcr/alo/.train_artifacts/output/graph/tsvs/rel_5.tsv\n",
      "[2023-11-17 06:32:24.835592] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-11-17 06:32:26.375831] - Processed 1000 edges in total\n",
      "[2023-11-17 06:32:26.377621] Preparing edge path /home/jovyan/gcr/alo/.train_artifacts/output/graph/partitions/edges_partitioned_rel_8, out of the edges found in /home/jovyan/gcr/alo/.train_artifacts/output/graph/tsvs/rel_8.tsv\n",
      "[2023-11-17 06:32:26.378813] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-11-17 06:32:27.930480] - Processed 1000 edges in total\n",
      "[2023-11-17 06:32:27.932445] Preparing edge path /home/jovyan/gcr/alo/.train_artifacts/output/graph/partitions/edges_partitioned_rel_4, out of the edges found in /home/jovyan/gcr/alo/.train_artifacts/output/graph/tsvs/rel_4.tsv\n",
      "[2023-11-17 06:32:27.934076] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-11-17 06:32:29.512566] - Processed 1000 edges in total\n",
      "[2023-11-17 06:32:29.514481] Preparing edge path /home/jovyan/gcr/alo/.train_artifacts/output/graph/partitions/edges_partitioned_rel_2, out of the edges found in /home/jovyan/gcr/alo/.train_artifacts/output/graph/tsvs/rel_2.tsv\n",
      "[2023-11-17 06:32:29.515654] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-11-17 06:32:31.064980] - Processed 1000 edges in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torchbiggraph/util.py:222: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = tensor.storage_type()._new_shared(size.numel())\n",
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torch/storage.py:959: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if self.device.type not in ['cpu', 'cuda']:\n",
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torch/storage.py:962: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  module = torch if self.device.type == 'cpu' else torch.cuda\n",
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torch/storage.py:985: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  untyped_storage = torch.UntypedStorage._new_shared(size * cls()._element_size())\n",
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torch/storage.py:986: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return cls(wrap_storage=untyped_storage)\n",
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torchbiggraph/train_cpu.py:304: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  ).storage()\n",
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torchbiggraph/train_cpu.py:821: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  self.embedding_storage_freelist[entity].add(embs.storage())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Embedding Complete]\n",
      "[Embeddig result saved at /home/jovyan/gcr/alo/.train_artifacts/output/graph/RESULT]\n",
      "\u001b[92m[2023-11-17 06:32:59,163][ASSET][INFO][train_pipeline][graph]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr/alo/.train_artifacts/models/graph/\u001b[0m\n",
      "In __del__: pbg deleted\n",
      "\u001b[94m[2023-11-17 06:32:59,211][ASSET][INFO][train_pipeline][graph]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2023-11-17 06:32:59\n",
      "- current step      : graph\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:32:59,218][PROCESS][INFO]: ==================== Finish pipeline: train_pipeline / step: graph\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>is_married</th>\n",
       "      <th>EMB_000</th>\n",
       "      <th>EMB_001</th>\n",
       "      <th>EMB_002</th>\n",
       "      <th>EMB_003</th>\n",
       "      <th>EMB_004</th>\n",
       "      <th>EMB_005</th>\n",
       "      <th>EMB_006</th>\n",
       "      <th>EMB_007</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_118</th>\n",
       "      <th>EMB_119</th>\n",
       "      <th>EMB_120</th>\n",
       "      <th>EMB_121</th>\n",
       "      <th>EMB_122</th>\n",
       "      <th>EMB_123</th>\n",
       "      <th>EMB_124</th>\n",
       "      <th>EMB_125</th>\n",
       "      <th>EMB_126</th>\n",
       "      <th>EMB_127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jasmine_Young</td>\n",
       "      <td>False</td>\n",
       "      <td>0.024974</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>-0.004251</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.015333</td>\n",
       "      <td>-0.004593</td>\n",
       "      <td>-0.011245</td>\n",
       "      <td>-0.022816</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004635</td>\n",
       "      <td>0.006578</td>\n",
       "      <td>-0.018206</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>-0.007354</td>\n",
       "      <td>0.008252</td>\n",
       "      <td>0.008084</td>\n",
       "      <td>-0.009249</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>-0.001155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jeffery_Robinson</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.007763</td>\n",
       "      <td>-0.022430</td>\n",
       "      <td>0.008236</td>\n",
       "      <td>-0.001891</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.009380</td>\n",
       "      <td>0.006806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015006</td>\n",
       "      <td>-0.005936</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>-0.009295</td>\n",
       "      <td>0.011241</td>\n",
       "      <td>-0.007399</td>\n",
       "      <td>-0.019122</td>\n",
       "      <td>0.009324</td>\n",
       "      <td>-0.034060</td>\n",
       "      <td>-0.019440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Steven_Sullivan</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.019424</td>\n",
       "      <td>0.007726</td>\n",
       "      <td>0.012575</td>\n",
       "      <td>-0.001292</td>\n",
       "      <td>0.006189</td>\n",
       "      <td>0.012953</td>\n",
       "      <td>0.008530</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006618</td>\n",
       "      <td>0.006113</td>\n",
       "      <td>0.008421</td>\n",
       "      <td>-0.004329</td>\n",
       "      <td>0.014880</td>\n",
       "      <td>-0.003377</td>\n",
       "      <td>-0.010354</td>\n",
       "      <td>0.009553</td>\n",
       "      <td>-0.015781</td>\n",
       "      <td>-0.001166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jay_Williams</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.010120</td>\n",
       "      <td>-0.009104</td>\n",
       "      <td>-0.015809</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>-0.015064</td>\n",
       "      <td>-0.003561</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>-0.004213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003262</td>\n",
       "      <td>-0.004391</td>\n",
       "      <td>0.023065</td>\n",
       "      <td>0.006224</td>\n",
       "      <td>0.020297</td>\n",
       "      <td>-0.009825</td>\n",
       "      <td>-0.021217</td>\n",
       "      <td>-0.006951</td>\n",
       "      <td>0.004516</td>\n",
       "      <td>-0.010117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Benjamin_Beck</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.009757</td>\n",
       "      <td>-0.008288</td>\n",
       "      <td>0.011543</td>\n",
       "      <td>-0.002391</td>\n",
       "      <td>0.010626</td>\n",
       "      <td>-0.006650</td>\n",
       "      <td>-0.023285</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>-0.035844</td>\n",
       "      <td>-0.005745</td>\n",
       "      <td>0.013526</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>-0.022697</td>\n",
       "      <td>-0.011086</td>\n",
       "      <td>-0.000214</td>\n",
       "      <td>-0.030524</td>\n",
       "      <td>-0.001854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gregory_Gomez</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.011007</td>\n",
       "      <td>0.011702</td>\n",
       "      <td>0.010162</td>\n",
       "      <td>0.003859</td>\n",
       "      <td>-0.005959</td>\n",
       "      <td>-0.011005</td>\n",
       "      <td>0.005286</td>\n",
       "      <td>-0.017144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006860</td>\n",
       "      <td>-0.020605</td>\n",
       "      <td>0.013268</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>0.025282</td>\n",
       "      <td>-0.014628</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.024624</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>-0.003882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mary_Harris</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.003335</td>\n",
       "      <td>-0.008783</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>-0.023075</td>\n",
       "      <td>0.003709</td>\n",
       "      <td>0.011911</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>-0.013779</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022717</td>\n",
       "      <td>0.016143</td>\n",
       "      <td>-0.002019</td>\n",
       "      <td>-0.001472</td>\n",
       "      <td>0.009565</td>\n",
       "      <td>0.012271</td>\n",
       "      <td>-0.003367</td>\n",
       "      <td>-0.015318</td>\n",
       "      <td>0.010930</td>\n",
       "      <td>0.015784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jimmy_Smith</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.006352</td>\n",
       "      <td>-0.019224</td>\n",
       "      <td>-0.037491</td>\n",
       "      <td>-0.002359</td>\n",
       "      <td>-0.020693</td>\n",
       "      <td>-0.006890</td>\n",
       "      <td>0.008439</td>\n",
       "      <td>0.012539</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004563</td>\n",
       "      <td>-0.019579</td>\n",
       "      <td>0.011839</td>\n",
       "      <td>0.012159</td>\n",
       "      <td>0.005548</td>\n",
       "      <td>-0.008609</td>\n",
       "      <td>-0.017191</td>\n",
       "      <td>-0.001574</td>\n",
       "      <td>-0.010521</td>\n",
       "      <td>-0.012327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kenneth_Rubio</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.025676</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>-0.016240</td>\n",
       "      <td>0.003634</td>\n",
       "      <td>0.002559</td>\n",
       "      <td>0.003469</td>\n",
       "      <td>-0.001965</td>\n",
       "      <td>0.026829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012854</td>\n",
       "      <td>-0.021040</td>\n",
       "      <td>0.021807</td>\n",
       "      <td>0.004527</td>\n",
       "      <td>0.010248</td>\n",
       "      <td>-0.008834</td>\n",
       "      <td>-0.015866</td>\n",
       "      <td>-0.017164</td>\n",
       "      <td>-0.003658</td>\n",
       "      <td>-0.006843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jordan_Simmons</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.006510</td>\n",
       "      <td>0.005137</td>\n",
       "      <td>0.007197</td>\n",
       "      <td>-0.006148</td>\n",
       "      <td>-0.017680</td>\n",
       "      <td>0.004268</td>\n",
       "      <td>0.009188</td>\n",
       "      <td>0.015250</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009158</td>\n",
       "      <td>0.014449</td>\n",
       "      <td>-0.021523</td>\n",
       "      <td>-0.000791</td>\n",
       "      <td>0.008259</td>\n",
       "      <td>-0.013100</td>\n",
       "      <td>0.011254</td>\n",
       "      <td>0.015127</td>\n",
       "      <td>0.005240</td>\n",
       "      <td>-0.015437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               name is_married   EMB_000   EMB_001   EMB_002   EMB_003  \\\n",
       "0     Jasmine_Young      False  0.024974  0.007638 -0.004251  0.002464   \n",
       "1  Jeffery_Robinson       True -0.008449 -0.007763 -0.022430  0.008236   \n",
       "2   Steven_Sullivan      False -0.019424  0.007726  0.012575 -0.001292   \n",
       "3      Jay_Williams      False -0.010120 -0.009104 -0.015809  0.001753   \n",
       "4     Benjamin_Beck       True -0.009757 -0.008288  0.011543 -0.002391   \n",
       "5     Gregory_Gomez      False -0.011007  0.011702  0.010162  0.003859   \n",
       "6       Mary_Harris       True -0.003335 -0.008783  0.001342 -0.023075   \n",
       "7       Jimmy_Smith      False -0.006352 -0.019224 -0.037491 -0.002359   \n",
       "8     Kenneth_Rubio       True -0.025676  0.000930 -0.016240  0.003634   \n",
       "9    Jordan_Simmons      False -0.006510  0.005137  0.007197 -0.006148   \n",
       "\n",
       "    EMB_004   EMB_005   EMB_006   EMB_007  ...   EMB_118   EMB_119   EMB_120  \\\n",
       "0  0.015333 -0.004593 -0.011245 -0.022816  ... -0.004635  0.006578 -0.018206   \n",
       "1 -0.001891  0.000767  0.009380  0.006806  ...  0.015006 -0.005936  0.001356   \n",
       "2  0.006189  0.012953  0.008530  0.002165  ...  0.006618  0.006113  0.008421   \n",
       "3 -0.015064 -0.003561  0.000679 -0.004213  ...  0.003262 -0.004391  0.023065   \n",
       "4  0.010626 -0.006650 -0.023285  0.008229  ...  0.004394 -0.035844 -0.005745   \n",
       "5 -0.005959 -0.011005  0.005286 -0.017144  ...  0.006860 -0.020605  0.013268   \n",
       "6  0.003709  0.011911  0.000960 -0.013779  ... -0.022717  0.016143 -0.002019   \n",
       "7 -0.020693 -0.006890  0.008439  0.012539  ... -0.004563 -0.019579  0.011839   \n",
       "8  0.002559  0.003469 -0.001965  0.026829  ...  0.012854 -0.021040  0.021807   \n",
       "9 -0.017680  0.004268  0.009188  0.015250  ... -0.009158  0.014449 -0.021523   \n",
       "\n",
       "    EMB_121   EMB_122   EMB_123   EMB_124   EMB_125   EMB_126   EMB_127  \n",
       "0  0.001426 -0.007354  0.008252  0.008084 -0.009249  0.014574 -0.001155  \n",
       "1 -0.009295  0.011241 -0.007399 -0.019122  0.009324 -0.034060 -0.019440  \n",
       "2 -0.004329  0.014880 -0.003377 -0.010354  0.009553 -0.015781 -0.001166  \n",
       "3  0.006224  0.020297 -0.009825 -0.021217 -0.006951  0.004516 -0.010117  \n",
       "4  0.013526  0.000501 -0.022697 -0.011086 -0.000214 -0.030524 -0.001854  \n",
       "5  0.003548  0.025282 -0.014628  0.000838  0.024624  0.000534 -0.003882  \n",
       "6 -0.001472  0.009565  0.012271 -0.003367 -0.015318  0.010930  0.015784  \n",
       "7  0.012159  0.005548 -0.008609 -0.017191 -0.001574 -0.010521 -0.012327  \n",
       "8  0.004527  0.010248 -0.008834 -0.015866 -0.017164 -0.003658 -0.006843  \n",
       "9 -0.000791  0.008259 -0.013100  0.011254  0.015127  0.005240 -0.015437  \n",
       "\n",
       "[10 rows x 130 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asset 실행\n",
    "graph_asset_structure=run(step, pipeline, asset_structure)\n",
    "\n",
    "# graph asset의 결과 dataframe은 graph_asset_structure.data['dataframe']으로 확인할 수 있습니다. \n",
    "graph_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9047dd80-a5c6-4393-aeb3-327e02a73ddd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [2] Preprocess asset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7698dd0-9153-46c0-97d8-6971ab8f3b2c",
   "metadata": {},
   "source": [
    "GCR은 데이터 전처리가 불필요하기 때문에 Preprocess asset의 역할은 크지 않습니다. 다만 사용자가 임베딩 외에 raw data를 학습에 사용하는 경우 (즉, extra_columns_for_ml 설정시) 결측치를 처리하기 위한 용도입니다.\n",
    "#### 주요 Parameter\n",
    "- handling_missing : 결측치 처리 방식을 지정합니다. 'interpolation' 또는 'fill_number' 중에 선택할 수 있으며 GCR에서는 'interpolation'을 권장합니다.\n",
    "- ***handling_encoding_y_column*** : input asset의 y_column과 동일하게 설정합니다. (필수)\n",
    "- ***handling_encoding_y*** : y_column의 인코딩 방식을 설정합니다. GCR에서는 'label'로 설정합니다.\n",
    "- handling_scaling_x : X 컬럼의 scaling 방식을 선택합니다.\n",
    "- load_train_preprocess : False로 설정합니다. (inference workflow 전용)\n",
    "\n",
    "#### Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91e75b62-dd0c-441a-a8db-eb6d15a4345b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'handling_missing': 'interpolation',\n",
       " 'handling_encoding_y_column': 'is_married',\n",
       " 'handling_encoding_y': 'label',\n",
       " 'handling_scaling_x': 'none',\n",
       " 'load_train_preprocess': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - graph(1) - preprocess(2) - sampling(3) - train(4))\n",
    "step = 2 \n",
    "asset_structure = copy.deepcopy(graph_asset_structure)\n",
    "asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 preprocess asset argument를 원하는 값으로 수정합니다. \n",
    "# asset_structure.args['handling_missing'] = dropna\n",
    "asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d78a8-0a32-4f54-aac7-bff907e2ae94",
   "metadata": {},
   "source": [
    "#### Preprocess asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a116e5ee-0eaa-4cad-9e8b-b7452f7068be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[2023-11-17 06:33:01,546][ASSET][INFO][train_pipeline][preprocess]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr/alo/.train_artifacts/models/preprocess/\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:33:01,548][ASSET][INFO][train_pipeline][preprocess]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2023-11-17 06:33:01\n",
      "- current step      : preprocess\n",
      "- asset branch.     : release-1.2\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : release-2.1\n",
      "- load envs. keys   : dict_keys(['solution_metadata_version', 'project_home', 'pipeline', 'step', 'num_step', 'artifacts', 'alo_version', 'asset_branch', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['handling_missing', 'handling_encoding_y_column', 'handling_encoding_y', 'handling_scaling_x', 'load_train_preprocess'])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "is_married column : label Encoder saved : /home/jovyan/gcr/alo/.train_artifacts/models/preprocess/\n",
      "['EMB_000_nan', 'EMB_001_nan', 'EMB_002_nan', 'EMB_003_nan', 'EMB_004_nan', 'EMB_005_nan', 'EMB_006_nan', 'EMB_007_nan', 'EMB_008_nan', 'EMB_009_nan', 'EMB_010_nan', 'EMB_011_nan', 'EMB_012_nan', 'EMB_013_nan', 'EMB_014_nan', 'EMB_015_nan', 'EMB_016_nan', 'EMB_017_nan', 'EMB_018_nan', 'EMB_019_nan', 'EMB_020_nan', 'EMB_021_nan', 'EMB_022_nan', 'EMB_023_nan', 'EMB_024_nan', 'EMB_025_nan', 'EMB_026_nan', 'EMB_027_nan', 'EMB_028_nan', 'EMB_029_nan', 'EMB_030_nan', 'EMB_031_nan', 'EMB_032_nan', 'EMB_033_nan', 'EMB_034_nan', 'EMB_035_nan', 'EMB_036_nan', 'EMB_037_nan', 'EMB_038_nan', 'EMB_039_nan', 'EMB_040_nan', 'EMB_041_nan', 'EMB_042_nan', 'EMB_043_nan', 'EMB_044_nan', 'EMB_045_nan', 'EMB_046_nan', 'EMB_047_nan', 'EMB_048_nan', 'EMB_049_nan', 'EMB_050_nan', 'EMB_051_nan', 'EMB_052_nan', 'EMB_053_nan', 'EMB_054_nan', 'EMB_055_nan', 'EMB_056_nan', 'EMB_057_nan', 'EMB_058_nan', 'EMB_059_nan', 'EMB_060_nan', 'EMB_061_nan', 'EMB_062_nan', 'EMB_063_nan', 'EMB_064_nan', 'EMB_065_nan', 'EMB_066_nan', 'EMB_067_nan', 'EMB_068_nan', 'EMB_069_nan', 'EMB_070_nan', 'EMB_071_nan', 'EMB_072_nan', 'EMB_073_nan', 'EMB_074_nan', 'EMB_075_nan', 'EMB_076_nan', 'EMB_077_nan', 'EMB_078_nan', 'EMB_079_nan', 'EMB_080_nan', 'EMB_081_nan', 'EMB_082_nan', 'EMB_083_nan', 'EMB_084_nan', 'EMB_085_nan', 'EMB_086_nan', 'EMB_087_nan', 'EMB_088_nan', 'EMB_089_nan', 'EMB_090_nan', 'EMB_091_nan', 'EMB_092_nan', 'EMB_093_nan', 'EMB_094_nan', 'EMB_095_nan', 'EMB_096_nan', 'EMB_097_nan', 'EMB_098_nan', 'EMB_099_nan', 'EMB_100_nan', 'EMB_101_nan', 'EMB_102_nan', 'EMB_103_nan', 'EMB_104_nan', 'EMB_105_nan', 'EMB_106_nan', 'EMB_107_nan', 'EMB_108_nan', 'EMB_109_nan', 'EMB_110_nan', 'EMB_111_nan', 'EMB_112_nan', 'EMB_113_nan', 'EMB_114_nan', 'EMB_115_nan', 'EMB_116_nan', 'EMB_117_nan', 'EMB_118_nan', 'EMB_119_nan', 'EMB_120_nan', 'EMB_121_nan', 'EMB_122_nan', 'EMB_123_nan', 'EMB_124_nan', 'EMB_125_nan', 'EMB_126_nan', 'EMB_127_nan'] is_married_encoded_nan\n",
      "\u001b[94m[2023-11-17 06:33:01,574][ASSET][INFO][train_pipeline][preprocess]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2023-11-17 06:33:01\n",
      "- current step      : preprocess\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'columns_map', 'preprocess'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:33:01,576][PROCESS][INFO]: ==================== Finish pipeline: train_pipeline / step: preprocess\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_000</th>\n",
       "      <th>EMB_001</th>\n",
       "      <th>EMB_002</th>\n",
       "      <th>EMB_003</th>\n",
       "      <th>EMB_004</th>\n",
       "      <th>EMB_005</th>\n",
       "      <th>EMB_006</th>\n",
       "      <th>EMB_007</th>\n",
       "      <th>EMB_008</th>\n",
       "      <th>EMB_009</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_119_nan</th>\n",
       "      <th>EMB_120_nan</th>\n",
       "      <th>EMB_121_nan</th>\n",
       "      <th>EMB_122_nan</th>\n",
       "      <th>EMB_123_nan</th>\n",
       "      <th>EMB_124_nan</th>\n",
       "      <th>EMB_125_nan</th>\n",
       "      <th>EMB_126_nan</th>\n",
       "      <th>EMB_127_nan</th>\n",
       "      <th>is_married_encoded_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024974</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>-0.004251</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.015333</td>\n",
       "      <td>-0.004593</td>\n",
       "      <td>-0.011245</td>\n",
       "      <td>-0.022816</td>\n",
       "      <td>-0.007108</td>\n",
       "      <td>0.019167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006578</td>\n",
       "      <td>-0.018206</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>-0.007354</td>\n",
       "      <td>0.008252</td>\n",
       "      <td>0.008084</td>\n",
       "      <td>-0.009249</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>-0.001155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.007763</td>\n",
       "      <td>-0.022430</td>\n",
       "      <td>0.008236</td>\n",
       "      <td>-0.001891</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.009380</td>\n",
       "      <td>0.006806</td>\n",
       "      <td>-0.004699</td>\n",
       "      <td>-0.011067</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005936</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>-0.009295</td>\n",
       "      <td>0.011241</td>\n",
       "      <td>-0.007399</td>\n",
       "      <td>-0.019122</td>\n",
       "      <td>0.009324</td>\n",
       "      <td>-0.034060</td>\n",
       "      <td>-0.019440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.019424</td>\n",
       "      <td>0.007726</td>\n",
       "      <td>0.012575</td>\n",
       "      <td>-0.001292</td>\n",
       "      <td>0.006189</td>\n",
       "      <td>0.012953</td>\n",
       "      <td>0.008530</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>0.008563</td>\n",
       "      <td>-0.012499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006113</td>\n",
       "      <td>0.008421</td>\n",
       "      <td>-0.004329</td>\n",
       "      <td>0.014880</td>\n",
       "      <td>-0.003377</td>\n",
       "      <td>-0.010354</td>\n",
       "      <td>0.009553</td>\n",
       "      <td>-0.015781</td>\n",
       "      <td>-0.001166</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.010120</td>\n",
       "      <td>-0.009104</td>\n",
       "      <td>-0.015809</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>-0.015064</td>\n",
       "      <td>-0.003561</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>-0.004213</td>\n",
       "      <td>0.010689</td>\n",
       "      <td>-0.008654</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004391</td>\n",
       "      <td>0.023065</td>\n",
       "      <td>0.006224</td>\n",
       "      <td>0.020297</td>\n",
       "      <td>-0.009825</td>\n",
       "      <td>-0.021217</td>\n",
       "      <td>-0.006951</td>\n",
       "      <td>0.004516</td>\n",
       "      <td>-0.010117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.009757</td>\n",
       "      <td>-0.008288</td>\n",
       "      <td>0.011543</td>\n",
       "      <td>-0.002391</td>\n",
       "      <td>0.010626</td>\n",
       "      <td>-0.006650</td>\n",
       "      <td>-0.023285</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>0.007657</td>\n",
       "      <td>-0.008563</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035844</td>\n",
       "      <td>-0.005745</td>\n",
       "      <td>0.013526</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>-0.022697</td>\n",
       "      <td>-0.011086</td>\n",
       "      <td>-0.000214</td>\n",
       "      <td>-0.030524</td>\n",
       "      <td>-0.001854</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.011007</td>\n",
       "      <td>0.011702</td>\n",
       "      <td>0.010162</td>\n",
       "      <td>0.003859</td>\n",
       "      <td>-0.005959</td>\n",
       "      <td>-0.011005</td>\n",
       "      <td>0.005286</td>\n",
       "      <td>-0.017144</td>\n",
       "      <td>-0.000583</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020605</td>\n",
       "      <td>0.013268</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>0.025282</td>\n",
       "      <td>-0.014628</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.024624</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>-0.003882</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.003335</td>\n",
       "      <td>-0.008783</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>-0.023075</td>\n",
       "      <td>0.003709</td>\n",
       "      <td>0.011911</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>-0.013779</td>\n",
       "      <td>-0.015822</td>\n",
       "      <td>-0.008788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016143</td>\n",
       "      <td>-0.002019</td>\n",
       "      <td>-0.001472</td>\n",
       "      <td>0.009565</td>\n",
       "      <td>0.012271</td>\n",
       "      <td>-0.003367</td>\n",
       "      <td>-0.015318</td>\n",
       "      <td>0.010930</td>\n",
       "      <td>0.015784</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.006352</td>\n",
       "      <td>-0.019224</td>\n",
       "      <td>-0.037491</td>\n",
       "      <td>-0.002359</td>\n",
       "      <td>-0.020693</td>\n",
       "      <td>-0.006890</td>\n",
       "      <td>0.008439</td>\n",
       "      <td>0.012539</td>\n",
       "      <td>0.016908</td>\n",
       "      <td>-0.011289</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019579</td>\n",
       "      <td>0.011839</td>\n",
       "      <td>0.012159</td>\n",
       "      <td>0.005548</td>\n",
       "      <td>-0.008609</td>\n",
       "      <td>-0.017191</td>\n",
       "      <td>-0.001574</td>\n",
       "      <td>-0.010521</td>\n",
       "      <td>-0.012327</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.025676</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>-0.016240</td>\n",
       "      <td>0.003634</td>\n",
       "      <td>0.002559</td>\n",
       "      <td>0.003469</td>\n",
       "      <td>-0.001965</td>\n",
       "      <td>0.026829</td>\n",
       "      <td>0.004360</td>\n",
       "      <td>-0.016075</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021040</td>\n",
       "      <td>0.021807</td>\n",
       "      <td>0.004527</td>\n",
       "      <td>0.010248</td>\n",
       "      <td>-0.008834</td>\n",
       "      <td>-0.015866</td>\n",
       "      <td>-0.017164</td>\n",
       "      <td>-0.003658</td>\n",
       "      <td>-0.006843</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.006510</td>\n",
       "      <td>0.005137</td>\n",
       "      <td>0.007197</td>\n",
       "      <td>-0.006148</td>\n",
       "      <td>-0.017680</td>\n",
       "      <td>0.004268</td>\n",
       "      <td>0.009188</td>\n",
       "      <td>0.015250</td>\n",
       "      <td>0.002683</td>\n",
       "      <td>0.015016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014449</td>\n",
       "      <td>-0.021523</td>\n",
       "      <td>-0.000791</td>\n",
       "      <td>0.008259</td>\n",
       "      <td>-0.013100</td>\n",
       "      <td>0.011254</td>\n",
       "      <td>0.015127</td>\n",
       "      <td>0.005240</td>\n",
       "      <td>-0.015437</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 259 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    EMB_000   EMB_001   EMB_002   EMB_003   EMB_004   EMB_005   EMB_006  \\\n",
       "0  0.024974  0.007638 -0.004251  0.002464  0.015333 -0.004593 -0.011245   \n",
       "1 -0.008449 -0.007763 -0.022430  0.008236 -0.001891  0.000767  0.009380   \n",
       "2 -0.019424  0.007726  0.012575 -0.001292  0.006189  0.012953  0.008530   \n",
       "3 -0.010120 -0.009104 -0.015809  0.001753 -0.015064 -0.003561  0.000679   \n",
       "4 -0.009757 -0.008288  0.011543 -0.002391  0.010626 -0.006650 -0.023285   \n",
       "5 -0.011007  0.011702  0.010162  0.003859 -0.005959 -0.011005  0.005286   \n",
       "6 -0.003335 -0.008783  0.001342 -0.023075  0.003709  0.011911  0.000960   \n",
       "7 -0.006352 -0.019224 -0.037491 -0.002359 -0.020693 -0.006890  0.008439   \n",
       "8 -0.025676  0.000930 -0.016240  0.003634  0.002559  0.003469 -0.001965   \n",
       "9 -0.006510  0.005137  0.007197 -0.006148 -0.017680  0.004268  0.009188   \n",
       "\n",
       "    EMB_007   EMB_008   EMB_009  ...  EMB_119_nan  EMB_120_nan  EMB_121_nan  \\\n",
       "0 -0.022816 -0.007108  0.019167  ...     0.006578    -0.018206     0.001426   \n",
       "1  0.006806 -0.004699 -0.011067  ...    -0.005936     0.001356    -0.009295   \n",
       "2  0.002165  0.008563 -0.012499  ...     0.006113     0.008421    -0.004329   \n",
       "3 -0.004213  0.010689 -0.008654  ...    -0.004391     0.023065     0.006224   \n",
       "4  0.008229  0.007657 -0.008563  ...    -0.035844    -0.005745     0.013526   \n",
       "5 -0.017144 -0.000583 -0.000018  ...    -0.020605     0.013268     0.003548   \n",
       "6 -0.013779 -0.015822 -0.008788  ...     0.016143    -0.002019    -0.001472   \n",
       "7  0.012539  0.016908 -0.011289  ...    -0.019579     0.011839     0.012159   \n",
       "8  0.026829  0.004360 -0.016075  ...    -0.021040     0.021807     0.004527   \n",
       "9  0.015250  0.002683  0.015016  ...     0.014449    -0.021523    -0.000791   \n",
       "\n",
       "   EMB_122_nan  EMB_123_nan  EMB_124_nan  EMB_125_nan  EMB_126_nan  \\\n",
       "0    -0.007354     0.008252     0.008084    -0.009249     0.014574   \n",
       "1     0.011241    -0.007399    -0.019122     0.009324    -0.034060   \n",
       "2     0.014880    -0.003377    -0.010354     0.009553    -0.015781   \n",
       "3     0.020297    -0.009825    -0.021217    -0.006951     0.004516   \n",
       "4     0.000501    -0.022697    -0.011086    -0.000214    -0.030524   \n",
       "5     0.025282    -0.014628     0.000838     0.024624     0.000534   \n",
       "6     0.009565     0.012271    -0.003367    -0.015318     0.010930   \n",
       "7     0.005548    -0.008609    -0.017191    -0.001574    -0.010521   \n",
       "8     0.010248    -0.008834    -0.015866    -0.017164    -0.003658   \n",
       "9     0.008259    -0.013100     0.011254     0.015127     0.005240   \n",
       "\n",
       "   EMB_127_nan  is_married_encoded_nan  \n",
       "0    -0.001155                       0  \n",
       "1    -0.019440                       1  \n",
       "2    -0.001166                       0  \n",
       "3    -0.010117                       0  \n",
       "4    -0.001854                       1  \n",
       "5    -0.003882                       0  \n",
       "6     0.015784                       1  \n",
       "7    -0.012327                       0  \n",
       "8    -0.006843                       1  \n",
       "9    -0.015437                       0  \n",
       "\n",
       "[10 rows x 259 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asset 실행\n",
    "preprocess_asset_structure=run(step, pipeline, asset_structure)\n",
    "\n",
    "# preprocess asset의 결과 dataframe은 preprocess_asset_structure.data['dataframe']으로 확인할 수 있습니다.  \n",
    "preprocess_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0158d579-4e19-401f-8a1b-ac06a92f0e9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [3] Sampling asset  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd463df-8ac2-4eeb-a020-49f89a05c5a1",
   "metadata": {},
   "source": [
    "데이터 imbalance가 심한경우엔 sampling이 필요할 수 있습니다. 현재는 undersampling만 제공됩니다.\n",
    "#### 주요 Parameter\n",
    "- sampling_type : sampling이 필요한 경우 설정합니다. 필요없는 경우엔 'none'으로 설정합니다. [*none / under*]\n",
    "- sampling_method : sampling 방법을 선택합니다. [*random / cluster / negative*]\n",
    "- label_sampling : 데이터의 Y 라벨을 기준으로 데이터를 그룹핑하여 샘플링을 할 지 선택합니다. [*True / False*]\n",
    "- ignore_label_class : label_sampling이 True일 때, 라벨의 특정 클래스는 샘플링하지 않고 전부 사용하고 싶을 때 사용합니다. EX) Y 라벨 클래스가 'OK' 또는 'NG'일 때 'NG'클래스를 입력할 경우 NG클래스는 샘플링하지 않습니다.\n",
    "- negative_target_class : Negative sampling(sampling_method가 negative일 때)에만 사용되는 argument입니다. 기준이 되는 Y 라벨의 특정 클래스를 기입합니다.\n",
    "- label_sampling_num_type : Y 라벨의 각 클래스에 샘플링할 데이터 수를 계산하기 위한 방법들입니다. [*ratio / number / compare / mingroup*]\n",
    "- label_sampling_num : label_sampilng_num_type에 따라 값을 입력합니다. (user manual 참조)\n",
    "- sampling_groupkey_columns : 특정 컬럼을 기준으로 데이터를 그룹핑하여 샘플링을 할 때 사용합니다. 컬럼명을 list로 입력합니다.\n",
    "- sampling_num_type : 샘플링할 데이터 수를 결정하는 방법입니다. [*ratio / number / compare / mingroup*]\n",
    "- sampling_num : sampling_num_type에 따라 아래 예시의 형식으로 값을 입력합니다. (user manual 참조)\n",
    "\n",
    "#### Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a20b2f60-8fb3-4115-9006-8bee0b3c5ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sampling_type': 'none',\n",
       " 'sampling_method': 'negative',\n",
       " 'label_sampling': True,\n",
       " 'ignore_label_class': 1,\n",
       " 'negative_target_class': None,\n",
       " 'label_sampling_num_type': 'compare',\n",
       " 'label_sampling_num': {1: 1, 0: 25},\n",
       " 'sampling_groupkey_columns': None,\n",
       " 'sampling_num_type': None,\n",
       " 'sampling_num': None}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - graph(1) - preprocess(2) - sampling(3) - train(4))\n",
    "step = 3 \n",
    "asset_structure = copy.deepcopy(preprocess_asset_structure)\n",
    "asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 sampling asset argument를 원하는 값으로 수정합니다. \n",
    "# asset_structure.args['sampling_type'] = 'under'\n",
    "asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4230191c-ec22-4e85-9a18-82817a90e2fb",
   "metadata": {},
   "source": [
    "#### Sampling asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "025f4f8c-f430-4c28-b8e0-e27e7fe1a0db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2023-11-17 06:33:12,476][ASSET][INFO][train_pipeline][sampling]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2023-11-17 06:33:12\n",
      "- current step      : sampling\n",
      "- asset branch.     : release-1.2\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : release-2.1\n",
      "- load envs. keys   : dict_keys(['solution_metadata_version', 'project_home', 'pipeline', 'step', 'num_step', 'artifacts', 'alo_version', 'asset_branch', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['sampling_type', 'sampling_method', 'label_sampling', 'ignore_label_class', 'negative_target_class', 'label_sampling_num_type', 'label_sampling_num', 'sampling_groupkey_columns', 'sampling_num_type', 'sampling_num'])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'columns_map', 'preprocess'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[92m[2023-11-17 06:33:12,479][ASSET][INFO][train_pipeline][sampling]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr/alo/.train_artifacts/models/sampling/\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:33:12,481][ASSET][INFO][train_pipeline][sampling]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2023-11-17 06:33:12\n",
      "- current step      : sampling\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'columns_map', 'preprocess', 'sampling_type'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:33:12,483][PROCESS][INFO]: ==================== Finish pipeline: train_pipeline / step: sampling\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_000</th>\n",
       "      <th>EMB_001</th>\n",
       "      <th>EMB_002</th>\n",
       "      <th>EMB_003</th>\n",
       "      <th>EMB_004</th>\n",
       "      <th>EMB_005</th>\n",
       "      <th>EMB_006</th>\n",
       "      <th>EMB_007</th>\n",
       "      <th>EMB_008</th>\n",
       "      <th>EMB_009</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_119_nan</th>\n",
       "      <th>EMB_120_nan</th>\n",
       "      <th>EMB_121_nan</th>\n",
       "      <th>EMB_122_nan</th>\n",
       "      <th>EMB_123_nan</th>\n",
       "      <th>EMB_124_nan</th>\n",
       "      <th>EMB_125_nan</th>\n",
       "      <th>EMB_126_nan</th>\n",
       "      <th>EMB_127_nan</th>\n",
       "      <th>is_married_encoded_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024974</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>-0.004251</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.015333</td>\n",
       "      <td>-0.004593</td>\n",
       "      <td>-0.011245</td>\n",
       "      <td>-0.022816</td>\n",
       "      <td>-0.007108</td>\n",
       "      <td>0.019167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006578</td>\n",
       "      <td>-0.018206</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>-0.007354</td>\n",
       "      <td>0.008252</td>\n",
       "      <td>0.008084</td>\n",
       "      <td>-0.009249</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>-0.001155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.007763</td>\n",
       "      <td>-0.022430</td>\n",
       "      <td>0.008236</td>\n",
       "      <td>-0.001891</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.009380</td>\n",
       "      <td>0.006806</td>\n",
       "      <td>-0.004699</td>\n",
       "      <td>-0.011067</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005936</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>-0.009295</td>\n",
       "      <td>0.011241</td>\n",
       "      <td>-0.007399</td>\n",
       "      <td>-0.019122</td>\n",
       "      <td>0.009324</td>\n",
       "      <td>-0.034060</td>\n",
       "      <td>-0.019440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.019424</td>\n",
       "      <td>0.007726</td>\n",
       "      <td>0.012575</td>\n",
       "      <td>-0.001292</td>\n",
       "      <td>0.006189</td>\n",
       "      <td>0.012953</td>\n",
       "      <td>0.008530</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>0.008563</td>\n",
       "      <td>-0.012499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006113</td>\n",
       "      <td>0.008421</td>\n",
       "      <td>-0.004329</td>\n",
       "      <td>0.014880</td>\n",
       "      <td>-0.003377</td>\n",
       "      <td>-0.010354</td>\n",
       "      <td>0.009553</td>\n",
       "      <td>-0.015781</td>\n",
       "      <td>-0.001166</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.010120</td>\n",
       "      <td>-0.009104</td>\n",
       "      <td>-0.015809</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>-0.015064</td>\n",
       "      <td>-0.003561</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>-0.004213</td>\n",
       "      <td>0.010689</td>\n",
       "      <td>-0.008654</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004391</td>\n",
       "      <td>0.023065</td>\n",
       "      <td>0.006224</td>\n",
       "      <td>0.020297</td>\n",
       "      <td>-0.009825</td>\n",
       "      <td>-0.021217</td>\n",
       "      <td>-0.006951</td>\n",
       "      <td>0.004516</td>\n",
       "      <td>-0.010117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.009757</td>\n",
       "      <td>-0.008288</td>\n",
       "      <td>0.011543</td>\n",
       "      <td>-0.002391</td>\n",
       "      <td>0.010626</td>\n",
       "      <td>-0.006650</td>\n",
       "      <td>-0.023285</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>0.007657</td>\n",
       "      <td>-0.008563</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035844</td>\n",
       "      <td>-0.005745</td>\n",
       "      <td>0.013526</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>-0.022697</td>\n",
       "      <td>-0.011086</td>\n",
       "      <td>-0.000214</td>\n",
       "      <td>-0.030524</td>\n",
       "      <td>-0.001854</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.011007</td>\n",
       "      <td>0.011702</td>\n",
       "      <td>0.010162</td>\n",
       "      <td>0.003859</td>\n",
       "      <td>-0.005959</td>\n",
       "      <td>-0.011005</td>\n",
       "      <td>0.005286</td>\n",
       "      <td>-0.017144</td>\n",
       "      <td>-0.000583</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020605</td>\n",
       "      <td>0.013268</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>0.025282</td>\n",
       "      <td>-0.014628</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.024624</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>-0.003882</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.003335</td>\n",
       "      <td>-0.008783</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>-0.023075</td>\n",
       "      <td>0.003709</td>\n",
       "      <td>0.011911</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>-0.013779</td>\n",
       "      <td>-0.015822</td>\n",
       "      <td>-0.008788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016143</td>\n",
       "      <td>-0.002019</td>\n",
       "      <td>-0.001472</td>\n",
       "      <td>0.009565</td>\n",
       "      <td>0.012271</td>\n",
       "      <td>-0.003367</td>\n",
       "      <td>-0.015318</td>\n",
       "      <td>0.010930</td>\n",
       "      <td>0.015784</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.006352</td>\n",
       "      <td>-0.019224</td>\n",
       "      <td>-0.037491</td>\n",
       "      <td>-0.002359</td>\n",
       "      <td>-0.020693</td>\n",
       "      <td>-0.006890</td>\n",
       "      <td>0.008439</td>\n",
       "      <td>0.012539</td>\n",
       "      <td>0.016908</td>\n",
       "      <td>-0.011289</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019579</td>\n",
       "      <td>0.011839</td>\n",
       "      <td>0.012159</td>\n",
       "      <td>0.005548</td>\n",
       "      <td>-0.008609</td>\n",
       "      <td>-0.017191</td>\n",
       "      <td>-0.001574</td>\n",
       "      <td>-0.010521</td>\n",
       "      <td>-0.012327</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.025676</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>-0.016240</td>\n",
       "      <td>0.003634</td>\n",
       "      <td>0.002559</td>\n",
       "      <td>0.003469</td>\n",
       "      <td>-0.001965</td>\n",
       "      <td>0.026829</td>\n",
       "      <td>0.004360</td>\n",
       "      <td>-0.016075</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021040</td>\n",
       "      <td>0.021807</td>\n",
       "      <td>0.004527</td>\n",
       "      <td>0.010248</td>\n",
       "      <td>-0.008834</td>\n",
       "      <td>-0.015866</td>\n",
       "      <td>-0.017164</td>\n",
       "      <td>-0.003658</td>\n",
       "      <td>-0.006843</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.006510</td>\n",
       "      <td>0.005137</td>\n",
       "      <td>0.007197</td>\n",
       "      <td>-0.006148</td>\n",
       "      <td>-0.017680</td>\n",
       "      <td>0.004268</td>\n",
       "      <td>0.009188</td>\n",
       "      <td>0.015250</td>\n",
       "      <td>0.002683</td>\n",
       "      <td>0.015016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014449</td>\n",
       "      <td>-0.021523</td>\n",
       "      <td>-0.000791</td>\n",
       "      <td>0.008259</td>\n",
       "      <td>-0.013100</td>\n",
       "      <td>0.011254</td>\n",
       "      <td>0.015127</td>\n",
       "      <td>0.005240</td>\n",
       "      <td>-0.015437</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 259 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    EMB_000   EMB_001   EMB_002   EMB_003   EMB_004   EMB_005   EMB_006  \\\n",
       "0  0.024974  0.007638 -0.004251  0.002464  0.015333 -0.004593 -0.011245   \n",
       "1 -0.008449 -0.007763 -0.022430  0.008236 -0.001891  0.000767  0.009380   \n",
       "2 -0.019424  0.007726  0.012575 -0.001292  0.006189  0.012953  0.008530   \n",
       "3 -0.010120 -0.009104 -0.015809  0.001753 -0.015064 -0.003561  0.000679   \n",
       "4 -0.009757 -0.008288  0.011543 -0.002391  0.010626 -0.006650 -0.023285   \n",
       "5 -0.011007  0.011702  0.010162  0.003859 -0.005959 -0.011005  0.005286   \n",
       "6 -0.003335 -0.008783  0.001342 -0.023075  0.003709  0.011911  0.000960   \n",
       "7 -0.006352 -0.019224 -0.037491 -0.002359 -0.020693 -0.006890  0.008439   \n",
       "8 -0.025676  0.000930 -0.016240  0.003634  0.002559  0.003469 -0.001965   \n",
       "9 -0.006510  0.005137  0.007197 -0.006148 -0.017680  0.004268  0.009188   \n",
       "\n",
       "    EMB_007   EMB_008   EMB_009  ...  EMB_119_nan  EMB_120_nan  EMB_121_nan  \\\n",
       "0 -0.022816 -0.007108  0.019167  ...     0.006578    -0.018206     0.001426   \n",
       "1  0.006806 -0.004699 -0.011067  ...    -0.005936     0.001356    -0.009295   \n",
       "2  0.002165  0.008563 -0.012499  ...     0.006113     0.008421    -0.004329   \n",
       "3 -0.004213  0.010689 -0.008654  ...    -0.004391     0.023065     0.006224   \n",
       "4  0.008229  0.007657 -0.008563  ...    -0.035844    -0.005745     0.013526   \n",
       "5 -0.017144 -0.000583 -0.000018  ...    -0.020605     0.013268     0.003548   \n",
       "6 -0.013779 -0.015822 -0.008788  ...     0.016143    -0.002019    -0.001472   \n",
       "7  0.012539  0.016908 -0.011289  ...    -0.019579     0.011839     0.012159   \n",
       "8  0.026829  0.004360 -0.016075  ...    -0.021040     0.021807     0.004527   \n",
       "9  0.015250  0.002683  0.015016  ...     0.014449    -0.021523    -0.000791   \n",
       "\n",
       "   EMB_122_nan  EMB_123_nan  EMB_124_nan  EMB_125_nan  EMB_126_nan  \\\n",
       "0    -0.007354     0.008252     0.008084    -0.009249     0.014574   \n",
       "1     0.011241    -0.007399    -0.019122     0.009324    -0.034060   \n",
       "2     0.014880    -0.003377    -0.010354     0.009553    -0.015781   \n",
       "3     0.020297    -0.009825    -0.021217    -0.006951     0.004516   \n",
       "4     0.000501    -0.022697    -0.011086    -0.000214    -0.030524   \n",
       "5     0.025282    -0.014628     0.000838     0.024624     0.000534   \n",
       "6     0.009565     0.012271    -0.003367    -0.015318     0.010930   \n",
       "7     0.005548    -0.008609    -0.017191    -0.001574    -0.010521   \n",
       "8     0.010248    -0.008834    -0.015866    -0.017164    -0.003658   \n",
       "9     0.008259    -0.013100     0.011254     0.015127     0.005240   \n",
       "\n",
       "   EMB_127_nan  is_married_encoded_nan  \n",
       "0    -0.001155                       0  \n",
       "1    -0.019440                       1  \n",
       "2    -0.001166                       0  \n",
       "3    -0.010117                       0  \n",
       "4    -0.001854                       1  \n",
       "5    -0.003882                       0  \n",
       "6     0.015784                       1  \n",
       "7    -0.012327                       0  \n",
       "8    -0.006843                       1  \n",
       "9    -0.015437                       0  \n",
       "\n",
       "[10 rows x 259 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asset 실행\n",
    "sampling_asset_structure=run(step, pipeline, asset_structure)\n",
    "\n",
    "# sampling asset의 결과 dataframe은 sampling_asset_structure.data['dataframe']으로 확인할 수 있습니다.\n",
    "sampling_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b09311e-a704-425b-8203-180f3033d808",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [4] Train asset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6a3a7-2204-42b5-8cc8-a5b5f740f457",
   "metadata": {},
   "source": [
    "추출된 임베딩을 활용하여 ML모델을 학습합니다.\n",
    "#### 주요 Parameter\n",
    "- model_type: 목적에 맞는 학습 방식을 선택합니다. (classification/regression)\n",
    "- data_split_method: HPO를 위한 데이터 분할 방식을 선택합니다. (cross_validate/train_test_split)\n",
    "- evaluation_metric: classification의 경우 accuracy, precision, recall, f1-score / regression의 경우 mse, r2, mae, rmse 중 선택합니다.\n",
    "- model_list: lightgbm, random-forest, gbm, Catboost 중 복수 선택 가능합니다.\n",
    "- num_hpo: 설정 범위 내 hpo 횟수를 결정합니다.\n",
    "- param_range: Search 범위를 지정합니다.\n",
    "- shap_ratio: shap value 뽑을 데이터를 sampling 하는 비율을 결정합니다.\n",
    "\n",
    "#### Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39f0149f-bfb4-4223-835a-769fd57594bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_type': 'classification',\n",
       " 'data_split_method': 'cross_validate',\n",
       " 'evaluation_metric': 'accuracy',\n",
       " 'model_list': ['lgb', 'rf', 'cb'],\n",
       " 'num_hpo': 3,\n",
       " 'param_range': {'rf': {'max_depth': 6, 'n_estimators': [300, 500]},\n",
       "  'gbm': {'max_depth': [5, 7], 'n_estimators': [300, 500]},\n",
       "  'ngb': {'col_sample': [0.6, 0.8], 'n_estimators': [100, 300]},\n",
       "  'lgb': {'max_depth': [5, 9], 'n_estimators': [300, 500]},\n",
       "  'cb': {'max_depth': [5, 9], 'n_estimators': [100, 500]}},\n",
       " 'shap_ratio': 1.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - graph(1) - preprocess(2) - sampling(3) - train(4))\n",
    "step = 4 \n",
    "asset_structure = copy.deepcopy(sampling_asset_structure)\n",
    "asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 train asset argument를 원하는 값으로 수정합니다.\n",
    "# asset_structure.args['num_hpo'] = 1\n",
    "asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c0b28-561d-4815-b4fc-f2078e1a0f59",
   "metadata": {},
   "source": [
    "#### Train asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c2a5dcd-b0b8-455d-8b68-3483238a5aae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[2023-11-17 06:33:13,422][ASSET][INFO][train_pipeline][train]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr/alo/.train_artifacts/models/train/\u001b[0m\n",
      "\u001b[92m[2023-11-17 06:33:13,426][ASSET][INFO][train_pipeline][train]: Successfully got << output path >> for saving your data into csv or jpg file: \n",
      " /home/jovyan/gcr/alo/.train_artifacts/output/train/ \n",
      " - [NOTE] The names of output file must be fixed as << output.csv, output.jpg >> \u001b[0m\n",
      "\u001b[94m[2023-11-17 06:33:13,428][ASSET][INFO][train_pipeline][train]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2023-11-17 06:33:13\n",
      "- current step      : train\n",
      "- asset branch.     : tcr_v1.1.2\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : release-2.1\n",
      "- load envs. keys   : dict_keys(['solution_metadata_version', 'project_home', 'pipeline', 'step', 'num_step', 'artifacts', 'alo_version', 'asset_branch', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['model_type', 'data_split_method', 'evaluation_metric', 'model_list', 'num_hpo', 'param_range', 'shap_ratio'])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'columns_map', 'preprocess', 'sampling_type'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[92m[2023-11-17 06:33:13,431][ASSET][INFO][train_pipeline][train]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr/alo/.train_artifacts/models/train/\u001b[0m\n",
      "\u001b[92m[2023-11-17 06:33:13,433][ASSET][INFO][train_pipeline][train]: Successfully got << output path >> for saving your data into csv or jpg file: \n",
      " /home/jovyan/gcr/alo/.train_artifacts/output/train/ \n",
      " - [NOTE] The names of output file must be fixed as << output.csv, output.jpg >> \u001b[0m\n",
      "\u001b[92m[2023-11-17 06:33:13,435][ASSET][INFO][train_pipeline][train]: Successfully got << report path >> for saving your << report.html >> file: \n",
      " /home/jovyan/gcr/alo/.train_artifacts/report/\u001b[0m\n",
      "해당 column 은 Training 과정에 사용되지 않습니다. (column_name: ['EMB_031', 'EMB_104', 'EMB_043', 'EMB_037', 'EMB_038', 'EMB_125', 'EMB_098', 'EMB_077', 'EMB_027', 'EMB_016', 'EMB_081', 'EMB_100', 'EMB_120', 'EMB_062', 'EMB_096', 'EMB_111', 'EMB_071', 'EMB_084', 'EMB_008', 'EMB_014', 'EMB_040', 'EMB_069', 'EMB_032', 'EMB_051', 'EMB_067', 'EMB_034', 'EMB_068', 'EMB_113', 'EMB_013', 'EMB_123', 'EMB_087', 'EMB_112', 'EMB_053', 'EMB_007', 'EMB_099', 'EMB_065', 'EMB_121', 'EMB_083', 'EMB_076', 'EMB_019', 'EMB_055', 'EMB_020', 'EMB_028', 'EMB_039', 'EMB_022', 'EMB_078', 'is_married_encoded_nan', 'EMB_117', 'EMB_114', 'EMB_085', 'EMB_091', 'EMB_015', 'EMB_070', 'EMB_072', 'EMB_049', 'EMB_109', 'EMB_116', 'EMB_103', 'EMB_108', 'EMB_105', 'EMB_003', 'EMB_095', 'EMB_054', 'EMB_042', 'EMB_060', 'EMB_046', 'EMB_073', 'EMB_088', 'EMB_057', 'EMB_002', 'EMB_082', 'EMB_080', 'EMB_058', 'EMB_124', 'EMB_063', 'EMB_045', 'EMB_093', 'EMB_066', 'EMB_021', 'EMB_086', 'EMB_010', 'EMB_029', 'EMB_004', 'is_married_encoded', 'EMB_001', 'EMB_006', 'EMB_097', 'EMB_127', 'EMB_107', 'EMB_056', 'EMB_075', 'EMB_044', 'EMB_024', 'EMB_106', 'EMB_023', 'EMB_079', 'EMB_094', 'EMB_122', 'EMB_115', 'EMB_074', 'EMB_009', 'EMB_061', 'EMB_048', 'EMB_102', 'EMB_047', 'EMB_000', 'EMB_011', 'EMB_089', 'EMB_118', 'EMB_026', 'EMB_050', 'EMB_025', 'EMB_035', 'EMB_059', 'EMB_017', 'EMB_110', 'EMB_119', 'EMB_041', 'EMB_005', 'EMB_052', 'EMB_030', 'EMB_090', 'EMB_126', 'EMB_018', 'EMB_012', 'is_married', 'EMB_064', 'EMB_101', 'EMB_033', 'EMB_036', 'EMB_092'])\n",
      "[INFO] 모델 학습을 시작합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 0th-fold RandomForestClassifier_set0 모델을 학습합니다.(1/36)\n",
      "[INFO] 1th-fold RandomForestClassifier_set0 모델을 학습합니다.(2/36)\n",
      "[INFO] 2th-fold RandomForestClassifier_set0 모델을 학습합니다.(3/36)\n",
      "[INFO] 3th-fold RandomForestClassifier_set0 모델을 학습합니다.(4/36)\n",
      "[INFO] 0th-fold RandomForestClassifier_set1 모델을 학습합니다.(5/36)\n",
      "[INFO] 1th-fold RandomForestClassifier_set1 모델을 학습합니다.(6/36)\n",
      "[INFO] 2th-fold RandomForestClassifier_set1 모델을 학습합니다.(7/36)\n",
      "[INFO] 3th-fold RandomForestClassifier_set1 모델을 학습합니다.(8/36)\n",
      "[INFO] 0th-fold RandomForestClassifier_set2 모델을 학습합니다.(9/36)\n",
      "[INFO] 1th-fold RandomForestClassifier_set2 모델을 학습합니다.(10/36)\n",
      "[INFO] 2th-fold RandomForestClassifier_set2 모델을 학습합니다.(11/36)\n",
      "[INFO] 3th-fold RandomForestClassifier_set2 모델을 학습합니다.(12/36)\n",
      "[INFO] 0th-fold LGBMClassifier_set0 모델을 학습합니다.(13/36)\n",
      "[INFO] 1th-fold LGBMClassifier_set0 모델을 학습합니다.(14/36)\n",
      "[INFO] 2th-fold LGBMClassifier_set0 모델을 학습합니다.(15/36)\n",
      "[INFO] 3th-fold LGBMClassifier_set0 모델을 학습합니다.(16/36)\n",
      "[INFO] 0th-fold LGBMClassifier_set1 모델을 학습합니다.(17/36)\n",
      "[INFO] 1th-fold LGBMClassifier_set1 모델을 학습합니다.(18/36)\n",
      "[INFO] 2th-fold LGBMClassifier_set1 모델을 학습합니다.(19/36)\n",
      "[INFO] 3th-fold LGBMClassifier_set1 모델을 학습합니다.(20/36)\n",
      "[INFO] 0th-fold LGBMClassifier_set2 모델을 학습합니다.(21/36)\n",
      "[INFO] 1th-fold LGBMClassifier_set2 모델을 학습합니다.(22/36)\n",
      "[INFO] 2th-fold LGBMClassifier_set2 모델을 학습합니다.(23/36)\n",
      "[INFO] 3th-fold LGBMClassifier_set2 모델을 학습합니다.(24/36)\n",
      "[INFO] 0th-fold CatBoostClassifier_set0 모델을 학습합니다.(25/36)\n",
      "[INFO] 1th-fold CatBoostClassifier_set0 모델을 학습합니다.(26/36)\n",
      "[INFO] 2th-fold CatBoostClassifier_set0 모델을 학습합니다.(27/36)\n",
      "[INFO] 3th-fold CatBoostClassifier_set0 모델을 학습합니다.(28/36)\n",
      "[INFO] 0th-fold CatBoostClassifier_set1 모델을 학습합니다.(29/36)\n",
      "[INFO] 1th-fold CatBoostClassifier_set1 모델을 학습합니다.(30/36)\n",
      "[INFO] 2th-fold CatBoostClassifier_set1 모델을 학습합니다.(31/36)\n",
      "[INFO] 3th-fold CatBoostClassifier_set1 모델을 학습합니다.(32/36)\n",
      "[INFO] 0th-fold CatBoostClassifier_set2 모델을 학습합니다.(33/36)\n",
      "[INFO] 1th-fold CatBoostClassifier_set2 모델을 학습합니다.(34/36)\n",
      "[INFO] 2th-fold CatBoostClassifier_set2 모델을 학습합니다.(35/36)\n",
      "[INFO] 3th-fold CatBoostClassifier_set2 모델을 학습합니다.(36/36)\n",
      "@scoring_classification func. - label list: @scoring_classification func. - label list: @scoring_classification func. - label list: @scoring_classification func. - label list:  @scoring_classification func. - label list: @scoring_classification func. - label list: @scoring_classification func. - label list:  @scoring_classification func. - label list: @scoring_classification func. - label list:  {0, 1}   {0, 1}  {0, 1}\n",
      " {0, 1}{0, 1}{0, 1}{0, 1}\n",
      "\n",
      "{0, 1}{0, 1}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[INFO] 평가 지표는 ( accuracy ) 를 사용합니다. \n",
      "모델 정보 로그를 저장합니다. (저장위치: /home/jovyan/gcr/alo/.train_artifacts/models/train/model_selection.json)\n",
      "\n",
      "Top 1 model file is saved: /home/jovyan/gcr/alo/.train_artifacts/models/train/best_model_top0.pkl\n",
      "[Score] accuracy: 0.7077\n",
      "[Hyper-parameters] n_estimators: 300, n_jobs: 1, random_state: 1234, max_depth: 6, \n",
      "\n",
      "Top 2 model file is saved: /home/jovyan/gcr/alo/.train_artifacts/models/train/best_model_top1.pkl\n",
      "[Score] accuracy: 0.7077\n",
      "[Hyper-parameters] max_depth: 7, n_estimators: 300, verbose: 0, random_state: 1234, thread_count: 6, allow_writing_files: False, \n",
      "\n",
      "Top 3 model file is saved: /home/jovyan/gcr/alo/.train_artifacts/models/train/best_model_top2.pkl\n",
      "[Score] accuracy: 0.7077\n",
      "[Hyper-parameters] n_estimators: 400, n_jobs: 1, random_state: 1234, max_depth: 6, \n",
      "\n",
      "Following model is the best: RandomForestClassifier_set0 / accuracy:0.7077\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[INFO] Summary_plot for Train data 를 저장했습니다.\n",
      "\n",
      "ignore columns와 X로 지정한 데이터 프레임을 합치는 과정중에 에러가 발생했습니다. 확인 부탁드립니다.\n",
      "\u001b[94m[2023-11-17 06:36:22,223][ASSET][INFO][train_pipeline][train]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2023-11-17 06:36:22\n",
      "- current step      : train\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'columns_map', 'preprocess', 'sampling_type', 'feature_dict'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:36:22,227][PROCESS][INFO]: ==================== Finish pipeline: train_pipeline / step: train\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_000_nan</th>\n",
       "      <th>EMB_001_nan</th>\n",
       "      <th>EMB_002_nan</th>\n",
       "      <th>EMB_003_nan</th>\n",
       "      <th>EMB_004_nan</th>\n",
       "      <th>EMB_005_nan</th>\n",
       "      <th>EMB_006_nan</th>\n",
       "      <th>EMB_007_nan</th>\n",
       "      <th>EMB_008_nan</th>\n",
       "      <th>EMB_009_nan</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_126_nan_shapley</th>\n",
       "      <th>EMB_127_nan_shapley</th>\n",
       "      <th>is_married_encoded_nan</th>\n",
       "      <th>pred_is_married_encoded_nan</th>\n",
       "      <th>pred_is_married_encoded_nan_best0</th>\n",
       "      <th>pred_is_married_encoded_nan_best1</th>\n",
       "      <th>pred_is_married_encoded_nan_best2</th>\n",
       "      <th>prob_0</th>\n",
       "      <th>prob_1</th>\n",
       "      <th>train_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.011092</td>\n",
       "      <td>-0.007647</td>\n",
       "      <td>-0.004009</td>\n",
       "      <td>0.004060</td>\n",
       "      <td>-0.017412</td>\n",
       "      <td>-0.003755</td>\n",
       "      <td>-0.015752</td>\n",
       "      <td>0.011213</td>\n",
       "      <td>0.024591</td>\n",
       "      <td>-0.008447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.778348</td>\n",
       "      <td>0.221652</td>\n",
       "      <td>0th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.026522</td>\n",
       "      <td>-0.003607</td>\n",
       "      <td>-0.002386</td>\n",
       "      <td>-0.004075</td>\n",
       "      <td>0.027163</td>\n",
       "      <td>-0.003498</td>\n",
       "      <td>0.016953</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.009661</td>\n",
       "      <td>0.007398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.660790</td>\n",
       "      <td>0.339210</td>\n",
       "      <td>2th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006398</td>\n",
       "      <td>0.012297</td>\n",
       "      <td>-0.007344</td>\n",
       "      <td>-0.024922</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>-0.008970</td>\n",
       "      <td>0.003872</td>\n",
       "      <td>0.013081</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>-0.007655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>-0.001284</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.679557</td>\n",
       "      <td>0.320443</td>\n",
       "      <td>1th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002254</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>-0.000583</td>\n",
       "      <td>0.016570</td>\n",
       "      <td>-0.008005</td>\n",
       "      <td>0.005667</td>\n",
       "      <td>-0.009199</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.030139</td>\n",
       "      <td>0.001205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>0.002430</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.700551</td>\n",
       "      <td>0.299449</td>\n",
       "      <td>3th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002612</td>\n",
       "      <td>-0.000817</td>\n",
       "      <td>0.013548</td>\n",
       "      <td>-0.007850</td>\n",
       "      <td>0.013460</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>-0.001412</td>\n",
       "      <td>-0.019718</td>\n",
       "      <td>-0.002209</td>\n",
       "      <td>0.019642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002363</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.768134</td>\n",
       "      <td>0.231866</td>\n",
       "      <td>3th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>-0.016618</td>\n",
       "      <td>-0.036875</td>\n",
       "      <td>-0.001627</td>\n",
       "      <td>-0.015449</td>\n",
       "      <td>0.011982</td>\n",
       "      <td>0.009726</td>\n",
       "      <td>0.014693</td>\n",
       "      <td>-0.017954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052680</td>\n",
       "      <td>-0.000718</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.553131</td>\n",
       "      <td>0.446869</td>\n",
       "      <td>3th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.037526</td>\n",
       "      <td>-0.027751</td>\n",
       "      <td>0.003812</td>\n",
       "      <td>-0.011613</td>\n",
       "      <td>0.039342</td>\n",
       "      <td>0.011726</td>\n",
       "      <td>0.009272</td>\n",
       "      <td>-0.002179</td>\n",
       "      <td>-0.015952</td>\n",
       "      <td>0.014562</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001457</td>\n",
       "      <td>-0.008923</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.628638</td>\n",
       "      <td>0.371362</td>\n",
       "      <td>3th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.015889</td>\n",
       "      <td>0.006197</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.006338</td>\n",
       "      <td>0.011664</td>\n",
       "      <td>-0.011931</td>\n",
       "      <td>0.011607</td>\n",
       "      <td>0.016894</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014218</td>\n",
       "      <td>-0.005555</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719627</td>\n",
       "      <td>0.280373</td>\n",
       "      <td>3th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.011632</td>\n",
       "      <td>-0.006201</td>\n",
       "      <td>-0.012002</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>-0.028381</td>\n",
       "      <td>0.013670</td>\n",
       "      <td>-0.003384</td>\n",
       "      <td>0.010842</td>\n",
       "      <td>-0.011445</td>\n",
       "      <td>-0.013768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014613</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.731925</td>\n",
       "      <td>0.268075</td>\n",
       "      <td>3th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.013859</td>\n",
       "      <td>-0.001645</td>\n",
       "      <td>-0.003628</td>\n",
       "      <td>0.021995</td>\n",
       "      <td>-0.034791</td>\n",
       "      <td>0.012426</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.010285</td>\n",
       "      <td>-0.011974</td>\n",
       "      <td>-0.014407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.756649</td>\n",
       "      <td>0.243351</td>\n",
       "      <td>2th_test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EMB_000_nan  EMB_001_nan  EMB_002_nan  EMB_003_nan  EMB_004_nan  \\\n",
       "0    -0.011092    -0.007647    -0.004009     0.004060    -0.017412   \n",
       "1     0.026522    -0.003607    -0.002386    -0.004075     0.027163   \n",
       "2     0.006398     0.012297    -0.007344    -0.024922     0.000486   \n",
       "3     0.002254     0.000630    -0.000583     0.016570    -0.008005   \n",
       "4     0.002612    -0.000817     0.013548    -0.007850     0.013460   \n",
       "5     0.000252     0.001450    -0.016618    -0.036875    -0.001627   \n",
       "6     0.037526    -0.027751     0.003812    -0.011613     0.039342   \n",
       "7    -0.015889     0.006197    -0.019163     0.012900     0.000633   \n",
       "8    -0.011632    -0.006201    -0.012002     0.010641    -0.028381   \n",
       "9    -0.013859    -0.001645    -0.003628     0.021995    -0.034791   \n",
       "\n",
       "   EMB_005_nan  EMB_006_nan  EMB_007_nan  EMB_008_nan  EMB_009_nan  ...  \\\n",
       "0    -0.003755    -0.015752     0.011213     0.024591    -0.008447  ...   \n",
       "1    -0.003498     0.016953     0.000167     0.009661     0.007398  ...   \n",
       "2    -0.008970     0.003872     0.013081     0.008850    -0.007655  ...   \n",
       "3     0.005667    -0.009199     0.000416     0.030139     0.001205  ...   \n",
       "4     0.001660    -0.001412    -0.019718    -0.002209     0.019642  ...   \n",
       "5    -0.015449     0.011982     0.009726     0.014693    -0.017954  ...   \n",
       "6     0.011726     0.009272    -0.002179    -0.015952     0.014562  ...   \n",
       "7     0.006338     0.011664    -0.011931     0.011607     0.016894  ...   \n",
       "8     0.013670    -0.003384     0.010842    -0.011445    -0.013768  ...   \n",
       "9     0.012426     0.000419     0.010285    -0.011974    -0.014407  ...   \n",
       "\n",
       "   EMB_126_nan_shapley  EMB_127_nan_shapley  is_married_encoded_nan  \\\n",
       "0             0.001604             0.001301                       0   \n",
       "1             0.001852             0.000088                       0   \n",
       "2             0.002154            -0.001284                       0   \n",
       "3             0.001181             0.002430                       0   \n",
       "4             0.002363             0.000094                       0   \n",
       "5            -0.052680            -0.000718                       1   \n",
       "6            -0.001457            -0.008923                       1   \n",
       "7            -0.014218            -0.005555                       1   \n",
       "8            -0.014613             0.001889                       0   \n",
       "9             0.001338             0.002251                       0   \n",
       "\n",
       "   pred_is_married_encoded_nan  pred_is_married_encoded_nan_best0  \\\n",
       "0                            0                                  0   \n",
       "1                            0                                  0   \n",
       "2                            0                                  0   \n",
       "3                            0                                  0   \n",
       "4                            0                                  0   \n",
       "5                            0                                  0   \n",
       "6                            0                                  0   \n",
       "7                            0                                  0   \n",
       "8                            0                                  0   \n",
       "9                            0                                  0   \n",
       "\n",
       "   pred_is_married_encoded_nan_best1  pred_is_married_encoded_nan_best2  \\\n",
       "0                                  0                                  0   \n",
       "1                                  0                                  0   \n",
       "2                                  0                                  0   \n",
       "3                                  0                                  0   \n",
       "4                                  0                                  0   \n",
       "5                                  0                                  0   \n",
       "6                                  0                                  0   \n",
       "7                                  0                                  0   \n",
       "8                                  0                                  0   \n",
       "9                                  0                                  0   \n",
       "\n",
       "     prob_0    prob_1  train_test  \n",
       "0  0.778348  0.221652    0th_test  \n",
       "1  0.660790  0.339210    2th_test  \n",
       "2  0.679557  0.320443    1th_test  \n",
       "3  0.700551  0.299449    3th_test  \n",
       "4  0.768134  0.231866    3th_test  \n",
       "5  0.553131  0.446869    3th_test  \n",
       "6  0.628638  0.371362    3th_test  \n",
       "7  0.719627  0.280373    3th_test  \n",
       "8  0.731925  0.268075    3th_test  \n",
       "9  0.756649  0.243351    2th_test  \n",
       "\n",
       "[10 rows x 264 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# asset 실행\n",
    "train_asset_structure=run(step, pipeline, asset_structure)\n",
    "\n",
    "# train asset의 결과 dataframe은 train_asset_structure.data['dataframe']으로 확인할 수 있습니다.\n",
    "train_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e992948d-2fc0-4d6b-8107-833b7fd67eb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **3. Inference Workflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f82f0-1e6c-4af5-b842-78d428c1e6f3",
   "metadata": {},
   "source": [
    "#### GCR의 Inference Workflow 구성은 다음과 같습니다.\n",
    "> **[0]** Input : *사용자가 지정한 경로로부터 데이터를 Import*   \n",
    "> **[1]** Preprocess : *(필요시) 결측치 처리 및 라벨 인코딩*   \n",
    "> **[2]** Inference : *Train Workflow에서 선택된 베스트 모델을 활용해 라벨 추론*   \n",
    "> **[3]** Result : *결과 출력*   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6707f7c-816f-4c29-86af-be1f2d990f4a",
   "metadata": {},
   "source": [
    "#### Train Workflow Setup\n",
    "아래 코드를 실행하여 Train Workflow에 필요한 라이브러리를 먼저 설치 해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e052e1b-3478-47f8-8c6d-62aa4474b591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2023-11-17 06:36:22,463][PROCESS][INFO]: You did not write any << s3_private_key_file >> in the config yaml file. When you wanna get data from s3 storage, \n",
      "                                 you have to write the s3_private_key_file path or set << AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY >> in your os environment. \n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:36:22,468][PROCESS][INFO]: Start setting-up << input >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[92m[2023-11-17 06:36:22,472][PROCESS][INFO]: Now << local >> asset_source_code mode: <input> asset exists.\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:36:22,474][PROCESS][INFO]: Start setting-up << preprocess >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[92m[2023-11-17 06:36:22,477][PROCESS][INFO]: Now << local >> asset_source_code mode: <preprocess> asset exists.\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:36:22,479][PROCESS][INFO]: Start setting-up << inference >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:36:22,482][PROCESS][INFO]: Start renewing asset : /home/jovyan/gcr/alo/assets/inference\u001b[0m\n",
      "\u001b[92m[2023-11-17 06:36:22,869][PROCESS][INFO]: /home/jovyan/gcr/alo/assets/inference successfully pulled.\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:36:22,872][PROCESS][INFO]: Start setting-up << result >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:36:22,875][PROCESS][INFO]: Start renewing asset : /home/jovyan/gcr/alo/assets/result\u001b[0m\n",
      "\u001b[92m[2023-11-17 06:36:23,466][PROCESS][INFO]: /home/jovyan/gcr/alo/assets/result successfully pulled.\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:36:23,469][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:36:23,472][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:36:23,475][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:36:23,478][PROCESS][INFO]: ======================================== Start dependency installation : << input >> \u001b[0m\n",
      "\u001b[94m[2023-11-17 06:36:23,480][PROCESS][INFO]: Start checking existence & installing package - pandas==1.5.3 | Progress: ( 1 / 1 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-17 06:36:23,484][PROCESS][INFO]: [OK] << pandas==1.5.3 >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:36:23,486][PROCESS][INFO]: ======================================== Start dependency installation : << force-reinstall >> \u001b[0m\n",
      "\u001b[94m[2023-11-17 06:36:23,489][PROCESS][INFO]: ======================================== Finish dependency installation \n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 아래는 Inference 시 필요한 라이브러리를 설치하는 코드입니다. library 설치 에러가 발생하면 아래 셀을 재실행 해주세요\n",
    "external_load_data(pipelines[1], alo.external_path, alo.external_path_permission, alo.control['get_external_data'])\n",
    "pipeline = pipelines[1]\n",
    "alo.install_steps(pipeline, alo.control[\"get_asset_source\"])\n",
    "\n",
    "# 초기 data structure 구성\n",
    "envs, args, data, config = {}, {}, {}, {}\n",
    "init_asset_structure = AssetStructure(envs, args, data, config)\n",
    "# logger init\n",
    "alo.set_proc_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d76d2-b3a0-429b-a5de-b74ae6c04176",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [0] Input asset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f681c8a8-ffde-4f59-a694-1b3849b90dd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 주요 Parameter\n",
    "- input_path : GCR에서는 추론데이터가 'inference' 위치에 자동 저장됩니다. 따로 설정할 필요 없이 주어진 'inference'로 놓고 사용합니다.\n",
    "- x_columns : Inference workflow에서는 x_column을 따로 지정하지 않습니다. None으로 설정합니다.\n",
    "- use_all_x : Inference workflow에서는 use_all_x를 True로 놓습니다.\n",
    "- y_column : 추론데이터는 y_column이 없습니다. None으로 설정합니다.\n",
    "- groupkey_columns : 특정 컬럼 명을 기준으로 데이터를 그룹으로 나누어 모델링을 하고 싶을 경우에 사용합니다.\n",
    "- drop_columns : use_all_x가 True일 때 삭제하고 싶은 컬럼을 입력합니다.\n",
    "- time_column : 데이터에 시간 컬럼이 있을 경우 입력합니다.\n",
    "- concat_dataframes : 같은 형태 csv 파일 여러 개를 input data로 불러올 시, concat 여부를 선택합니다. [*True / False*]\n",
    "- encoding : pd.read_csv() 시에 사용할 encoding 방법을 설정합니다.\n",
    "\n",
    "#### Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2f43e8e-5a9a-4cbc-b2b5-f548d92b3feb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_path': 'inference',\n",
       " 'x_columns': None,\n",
       " 'use_all_x': True,\n",
       " 'y_column': None,\n",
       " 'groupkey_columns': None,\n",
       " 'drop_columns': None,\n",
       " 'time_column': None,\n",
       " 'concat_dataframes': None,\n",
       " 'encoding': None}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - preprocess(1) - inference(2) - result(3))\n",
    "step = 0 \n",
    "asset_structure = copy.deepcopy(init_asset_structure)\n",
    "asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 input asset argument를 원하는 값으로 수정합니다.\n",
    "# asset_structure.args['x_columns'] = ['']\n",
    "asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705ac75-11a8-4093-9531-3b09de8fc38e",
   "metadata": {},
   "source": [
    "##### Input asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "026f2355-99c7-437f-8274-dbed7aa968d8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-11-17 06:36:23,514][USER][INFO][inference_pipeline][input]: >> Load path : ['/home/jovyan/gcr/alo/input/inference/inference/']\n",
      "[2023-11-17 06:36:23,529][USER][INFO][inference_pipeline][input]: >> The file for batch data has been loaded. (File name: /home/jovyan/gcr/alo/input/inference/inference/inference.csv)\n",
      "[2023-11-17 06:36:23,532][USER][INFO][inference_pipeline][input]: You set the << use_all_x >> as << True >> in the yaml file. So skip checking dataframe columns existence.\n",
      "[2023-11-17 06:36:23,535][USER][INFO][inference_pipeline][input]: ==================== Success loading dataframe ====================\n",
      "[2023-11-17 06:36:23,538][USER][INFO][inference_pipeline][input]: >> Start processing ignore columns & drop columns: ['/home/jovyan/gcr/alo/input/inference/inference/inference.csv']\n",
      "[2023-11-17 06:36:23,542][USER][INFO][inference_pipeline][input]: >> You set the << use_all_x >> parameter as << True >> in your config yaml. (So, these x_columns are used: ['EMB_031', 'EMB_104', 'EMB_043', 'EMB_037', 'EMB_038', 'EMB_125', 'EMB_098', 'EMB_077', 'EMB_027', 'EMB_016', 'EMB_081', 'EMB_100', 'EMB_120', 'EMB_062', 'EMB_096', 'EMB_111', 'EMB_071', 'EMB_084', 'EMB_008', 'EMB_014', 'EMB_040', 'EMB_069', 'EMB_032', 'EMB_051', 'EMB_067', 'EMB_034', 'EMB_068', 'EMB_113', 'EMB_013', 'EMB_123', 'EMB_087', 'EMB_112', 'EMB_053', 'EMB_007', 'EMB_099', 'EMB_065', 'EMB_121', 'EMB_083', 'EMB_076', 'EMB_019', 'EMB_055', 'EMB_020', 'EMB_028', 'EMB_039', 'EMB_022', 'EMB_078', 'EMB_117', 'EMB_114', 'EMB_085', 'EMB_091', 'EMB_015', 'EMB_070', 'EMB_072', 'EMB_049', 'EMB_109', 'EMB_116', 'EMB_103', 'EMB_108', 'EMB_105', 'EMB_003', 'EMB_095', 'EMB_054', 'EMB_042', 'EMB_060', 'EMB_046', 'EMB_073', 'EMB_088', 'EMB_057', 'EMB_002', 'EMB_082', 'EMB_080', 'EMB_058', 'EMB_124', 'EMB_063', 'EMB_045', 'EMB_093', 'EMB_066', 'EMB_021', 'EMB_086', 'EMB_010', 'EMB_029', 'EMB_004', 'EMB_001', 'EMB_006', 'EMB_097', 'EMB_127', 'EMB_107', 'EMB_056', 'EMB_075', 'EMB_044', 'EMB_024', 'EMB_106', 'EMB_023', 'EMB_079', 'EMB_094', 'EMB_122', 'EMB_115', 'EMB_074', 'EMB_009', 'EMB_061', 'EMB_048', 'EMB_102', 'EMB_047', 'EMB_000', 'EMB_011', 'EMB_089', 'EMB_118', 'EMB_026', 'EMB_050', 'EMB_025', 'EMB_035', 'EMB_059', 'EMB_017', 'EMB_110', 'EMB_119', 'EMB_041', 'EMB_005', 'EMB_052', 'EMB_030', 'EMB_090', 'EMB_126', 'EMB_018', 'EMB_012', 'EMB_064', 'EMB_101', 'EMB_033', 'EMB_036', 'EMB_092'] )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2023-11-17 06:36:23,511][ASSET][INFO][inference_pipeline][input]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2023-11-17 06:36:23\n",
      "- current step      : input\n",
      "- asset branch.     : tabular_2.0\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : release-2.1\n",
      "- load envs. keys   : dict_keys(['solution_metadata_version', 'project_home', 'pipeline', 'step', 'num_step', 'artifacts', 'alo_version', 'asset_branch', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path'])\n",
      "- load args. keys   : dict_keys(['input_path', 'x_columns', 'use_all_x', 'y_column', 'groupkey_columns', 'drop_columns', 'time_column', 'concat_dataframes', 'encoding'])\n",
      "- load config. keys : dict_keys(['meta'])\n",
      "- load data keys    : dict_keys([])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:36:23,543][ASSET][INFO][inference_pipeline][input]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2023-11-17 06:36:23\n",
      "- current step      : input\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:36:23,546][PROCESS][INFO]: ==================== Finish pipeline: inference_pipeline / step: input\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_000</th>\n",
       "      <th>EMB_001</th>\n",
       "      <th>EMB_002</th>\n",
       "      <th>EMB_003</th>\n",
       "      <th>EMB_004</th>\n",
       "      <th>EMB_005</th>\n",
       "      <th>EMB_006</th>\n",
       "      <th>EMB_007</th>\n",
       "      <th>EMB_008</th>\n",
       "      <th>EMB_009</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_118</th>\n",
       "      <th>EMB_119</th>\n",
       "      <th>EMB_120</th>\n",
       "      <th>EMB_121</th>\n",
       "      <th>EMB_122</th>\n",
       "      <th>EMB_123</th>\n",
       "      <th>EMB_124</th>\n",
       "      <th>EMB_125</th>\n",
       "      <th>EMB_126</th>\n",
       "      <th>EMB_127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.011211</td>\n",
       "      <td>0.021192</td>\n",
       "      <td>0.013510</td>\n",
       "      <td>0.010420</td>\n",
       "      <td>0.012326</td>\n",
       "      <td>0.010024</td>\n",
       "      <td>0.022406</td>\n",
       "      <td>-0.009880</td>\n",
       "      <td>0.006558</td>\n",
       "      <td>-0.014621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018959</td>\n",
       "      <td>-0.017662</td>\n",
       "      <td>0.022990</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-0.005146</td>\n",
       "      <td>-0.003938</td>\n",
       "      <td>-0.015009</td>\n",
       "      <td>0.019535</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>-0.016103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008541</td>\n",
       "      <td>0.004134</td>\n",
       "      <td>-0.006659</td>\n",
       "      <td>-0.008561</td>\n",
       "      <td>-0.013276</td>\n",
       "      <td>-0.011484</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.029092</td>\n",
       "      <td>0.010130</td>\n",
       "      <td>-0.020120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003465</td>\n",
       "      <td>-0.009100</td>\n",
       "      <td>0.012163</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>-0.005517</td>\n",
       "      <td>0.005121</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>-0.009514</td>\n",
       "      <td>0.002380</td>\n",
       "      <td>0.002756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.019315</td>\n",
       "      <td>0.004849</td>\n",
       "      <td>-0.022684</td>\n",
       "      <td>0.012220</td>\n",
       "      <td>0.007098</td>\n",
       "      <td>0.011592</td>\n",
       "      <td>0.003066</td>\n",
       "      <td>-0.007337</td>\n",
       "      <td>-0.002128</td>\n",
       "      <td>0.004139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002843</td>\n",
       "      <td>0.013025</td>\n",
       "      <td>-0.014647</td>\n",
       "      <td>0.017651</td>\n",
       "      <td>-0.017356</td>\n",
       "      <td>0.013134</td>\n",
       "      <td>0.002680</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>-0.003784</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013164</td>\n",
       "      <td>-0.009407</td>\n",
       "      <td>0.010454</td>\n",
       "      <td>-0.017179</td>\n",
       "      <td>0.021407</td>\n",
       "      <td>-0.004327</td>\n",
       "      <td>-0.006342</td>\n",
       "      <td>-0.009666</td>\n",
       "      <td>-0.005924</td>\n",
       "      <td>0.015952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019964</td>\n",
       "      <td>-0.013472</td>\n",
       "      <td>-0.006695</td>\n",
       "      <td>-0.011809</td>\n",
       "      <td>-0.010589</td>\n",
       "      <td>-0.001992</td>\n",
       "      <td>0.017462</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.007710</td>\n",
       "      <td>0.016784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.008298</td>\n",
       "      <td>-0.003845</td>\n",
       "      <td>0.022752</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.018120</td>\n",
       "      <td>-0.002713</td>\n",
       "      <td>-0.004593</td>\n",
       "      <td>-0.026710</td>\n",
       "      <td>-0.010649</td>\n",
       "      <td>0.020809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005280</td>\n",
       "      <td>0.009019</td>\n",
       "      <td>-0.013289</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>-0.009036</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.016854</td>\n",
       "      <td>-0.008670</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>-0.012213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.014745</td>\n",
       "      <td>-0.001136</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>-0.010555</td>\n",
       "      <td>0.016204</td>\n",
       "      <td>0.015836</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.023532</td>\n",
       "      <td>0.016864</td>\n",
       "      <td>-0.015578</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009099</td>\n",
       "      <td>0.013923</td>\n",
       "      <td>0.008759</td>\n",
       "      <td>-0.014268</td>\n",
       "      <td>-0.018184</td>\n",
       "      <td>0.024693</td>\n",
       "      <td>0.003871</td>\n",
       "      <td>0.011058</td>\n",
       "      <td>0.008464</td>\n",
       "      <td>0.024613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.031200</td>\n",
       "      <td>-0.008771</td>\n",
       "      <td>0.021719</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>0.006874</td>\n",
       "      <td>-0.011510</td>\n",
       "      <td>-0.011052</td>\n",
       "      <td>0.013309</td>\n",
       "      <td>0.008968</td>\n",
       "      <td>0.010274</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000262</td>\n",
       "      <td>-0.018567</td>\n",
       "      <td>-0.022518</td>\n",
       "      <td>-0.010394</td>\n",
       "      <td>-0.013227</td>\n",
       "      <td>0.001842</td>\n",
       "      <td>-0.011835</td>\n",
       "      <td>-0.047739</td>\n",
       "      <td>0.007006</td>\n",
       "      <td>-0.005282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.004072</td>\n",
       "      <td>0.024870</td>\n",
       "      <td>-0.015082</td>\n",
       "      <td>-0.001626</td>\n",
       "      <td>0.014011</td>\n",
       "      <td>0.003743</td>\n",
       "      <td>0.049421</td>\n",
       "      <td>0.006498</td>\n",
       "      <td>-0.016665</td>\n",
       "      <td>-0.039833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035174</td>\n",
       "      <td>-0.016693</td>\n",
       "      <td>0.042376</td>\n",
       "      <td>0.017134</td>\n",
       "      <td>0.013079</td>\n",
       "      <td>-0.016026</td>\n",
       "      <td>-0.021061</td>\n",
       "      <td>-0.014082</td>\n",
       "      <td>-0.005783</td>\n",
       "      <td>-0.009873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.020969</td>\n",
       "      <td>-0.003268</td>\n",
       "      <td>0.003840</td>\n",
       "      <td>0.008835</td>\n",
       "      <td>0.008807</td>\n",
       "      <td>-0.007809</td>\n",
       "      <td>0.004840</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.017214</td>\n",
       "      <td>-0.011284</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035367</td>\n",
       "      <td>-0.018924</td>\n",
       "      <td>-0.013157</td>\n",
       "      <td>0.006886</td>\n",
       "      <td>-0.000217</td>\n",
       "      <td>0.009760</td>\n",
       "      <td>-0.007533</td>\n",
       "      <td>-0.011800</td>\n",
       "      <td>0.010786</td>\n",
       "      <td>-0.015307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.012875</td>\n",
       "      <td>-0.005404</td>\n",
       "      <td>-0.025041</td>\n",
       "      <td>0.008551</td>\n",
       "      <td>-0.009934</td>\n",
       "      <td>-0.012423</td>\n",
       "      <td>-0.001929</td>\n",
       "      <td>-0.004785</td>\n",
       "      <td>0.002376</td>\n",
       "      <td>-0.020264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.007941</td>\n",
       "      <td>-0.011732</td>\n",
       "      <td>0.004964</td>\n",
       "      <td>-0.006709</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>-0.003956</td>\n",
       "      <td>-0.006232</td>\n",
       "      <td>-0.010216</td>\n",
       "      <td>-0.011842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    EMB_000   EMB_001   EMB_002   EMB_003   EMB_004   EMB_005   EMB_006  \\\n",
       "0 -0.011211  0.021192  0.013510  0.010420  0.012326  0.010024  0.022406   \n",
       "1  0.008541  0.004134 -0.006659 -0.008561 -0.013276 -0.011484  0.001329   \n",
       "2  0.019315  0.004849 -0.022684  0.012220  0.007098  0.011592  0.003066   \n",
       "3  0.013164 -0.009407  0.010454 -0.017179  0.021407 -0.004327 -0.006342   \n",
       "4 -0.008298 -0.003845  0.022752  0.004630  0.018120 -0.002713 -0.004593   \n",
       "5  0.014745 -0.001136  0.002283 -0.010555  0.016204  0.015836  0.001790   \n",
       "6 -0.031200 -0.008771  0.021719  0.036718  0.006874 -0.011510 -0.011052   \n",
       "7  0.004072  0.024870 -0.015082 -0.001626  0.014011  0.003743  0.049421   \n",
       "8 -0.020969 -0.003268  0.003840  0.008835  0.008807 -0.007809  0.004840   \n",
       "9  0.012875 -0.005404 -0.025041  0.008551 -0.009934 -0.012423 -0.001929   \n",
       "\n",
       "    EMB_007   EMB_008   EMB_009  ...   EMB_118   EMB_119   EMB_120   EMB_121  \\\n",
       "0 -0.009880  0.006558 -0.014621  ...  0.018959 -0.017662  0.022990  0.020000   \n",
       "1  0.029092  0.010130 -0.020120  ...  0.003465 -0.009100  0.012163  0.005579   \n",
       "2 -0.007337 -0.002128  0.004139  ...  0.002843  0.013025 -0.014647  0.017651   \n",
       "3 -0.009666 -0.005924  0.015952  ...  0.019964 -0.013472 -0.006695 -0.011809   \n",
       "4 -0.026710 -0.010649  0.020809  ...  0.005280  0.009019 -0.013289  0.000041   \n",
       "5  0.023532  0.016864 -0.015578  ... -0.009099  0.013923  0.008759 -0.014268   \n",
       "6  0.013309  0.008968  0.010274  ... -0.000262 -0.018567 -0.022518 -0.010394   \n",
       "7  0.006498 -0.016665 -0.039833  ...  0.035174 -0.016693  0.042376  0.017134   \n",
       "8  0.002440  0.017214 -0.011284  ... -0.035367 -0.018924 -0.013157  0.006886   \n",
       "9 -0.004785  0.002376 -0.020264  ...  0.002861 -0.007941 -0.011732  0.004964   \n",
       "\n",
       "    EMB_122   EMB_123   EMB_124   EMB_125   EMB_126   EMB_127  \n",
       "0 -0.005146 -0.003938 -0.015009  0.019535  0.002308 -0.016103  \n",
       "1 -0.005517  0.005121  0.002899 -0.009514  0.002380  0.002756  \n",
       "2 -0.017356  0.013134  0.002680  0.001103 -0.003784  0.000100  \n",
       "3 -0.010589 -0.001992  0.017462  0.000677  0.007710  0.016784  \n",
       "4 -0.009036  0.000817  0.016854 -0.008670  0.015686 -0.012213  \n",
       "5 -0.018184  0.024693  0.003871  0.011058  0.008464  0.024613  \n",
       "6 -0.013227  0.001842 -0.011835 -0.047739  0.007006 -0.005282  \n",
       "7  0.013079 -0.016026 -0.021061 -0.014082 -0.005783 -0.009873  \n",
       "8 -0.000217  0.009760 -0.007533 -0.011800  0.010786 -0.015307  \n",
       "9 -0.006709  0.001475 -0.003956 -0.006232 -0.010216 -0.011842  \n",
       "\n",
       "[10 rows x 128 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asset 실행\n",
    "input_asset_structure=run(step, pipeline, asset_structure)\n",
    "\n",
    "# input asset의 결과 dataframe은 input_asset_structure.data['dataframe']으로 확인할 수 있습니다.\n",
    "input_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9f3b11-9b2d-4bd0-87f2-25b23495758f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [1] Preprocess asset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ddf39d-af5c-48bd-9a96-a3543e030b2c",
   "metadata": {},
   "source": [
    "GCR은 데이터 전처리가 불필요하기 때문에 Preprocess asset의 역할은 크지 않습니다. 다만 사용자가 임베딩 외에 raw data를 학습에 사용하는 경우 (즉, extra_columns_for_ml 설정시) 결측치를 처리하기 위한 용도입니다.\n",
    "#### 주요 Parameter\n",
    "- handling_missing: 결측치 처리 방식을 지정합니다. 'interpolation' 또는 'fill_number' 중에 선택할 수 있으며 GCR에서는 'interpolation'을 권장합니다.\n",
    "- ***handling_encoding_y_column***: input asset의 y_column과 동일하게 설정합니다. (필수)\n",
    "- limit_encoding_categories: onehot이나 hashing 인코딩 진행 시 컬럼이 너무 많아지는 것에 대한 한계치를 설정합니다.\n",
    "- load_train_preprocess: 반드시 True로 설정합니다. train workflow의 preprocess를 참조하여 진행합니다.\n",
    "\n",
    "#### Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b486fd42-87de-4c15-aa77-324cb77415b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'handling_missing': 'interpolation',\n",
       " 'handling_encoding_y_column': None,\n",
       " 'limit_encoding_categories': 30,\n",
       " 'load_train_preprocess': True}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - preprocess(1) - inference(2) - result(3))\n",
    "step = 1 \n",
    "asset_structure = copy.deepcopy(input_asset_structure)\n",
    "asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 preprocess asset argument를 원하는 값으로 수정합니다.\n",
    "# asset_structure.args['x_columns'] = ['']\n",
    "asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6a36f8-55c3-4e3e-8879-d8aeb7f9ecac",
   "metadata": {},
   "source": [
    "#### Preprocess asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f690c108-afc0-48e3-9b04-85a4f498c8da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[2023-11-17 06:36:23,585][ASSET][INFO][inference_pipeline][preprocess]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr/alo/.train_artifacts/models/preprocess/\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:36:23,588][ASSET][INFO][inference_pipeline][preprocess]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2023-11-17 06:36:23\n",
      "- current step      : preprocess\n",
      "- asset branch.     : release-1.2\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : release-2.1\n",
      "- load envs. keys   : dict_keys(['solution_metadata_version', 'project_home', 'pipeline', 'step', 'num_step', 'artifacts', 'alo_version', 'asset_branch', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['handling_missing', 'handling_encoding_y_column', 'limit_encoding_categories', 'load_train_preprocess'])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "['EMB_031_nan', 'EMB_104_nan', 'EMB_043_nan', 'EMB_037_nan', 'EMB_038_nan', 'EMB_125_nan', 'EMB_098_nan', 'EMB_077_nan', 'EMB_027_nan', 'EMB_016_nan', 'EMB_081_nan', 'EMB_100_nan', 'EMB_120_nan', 'EMB_062_nan', 'EMB_096_nan', 'EMB_111_nan', 'EMB_071_nan', 'EMB_084_nan', 'EMB_008_nan', 'EMB_014_nan', 'EMB_040_nan', 'EMB_069_nan', 'EMB_032_nan', 'EMB_051_nan', 'EMB_067_nan', 'EMB_034_nan', 'EMB_068_nan', 'EMB_113_nan', 'EMB_013_nan', 'EMB_123_nan', 'EMB_087_nan', 'EMB_112_nan', 'EMB_053_nan', 'EMB_007_nan', 'EMB_099_nan', 'EMB_065_nan', 'EMB_121_nan', 'EMB_083_nan', 'EMB_076_nan', 'EMB_019_nan', 'EMB_055_nan', 'EMB_020_nan', 'EMB_028_nan', 'EMB_039_nan', 'EMB_022_nan', 'EMB_078_nan', 'EMB_117_nan', 'EMB_114_nan', 'EMB_085_nan', 'EMB_091_nan', 'EMB_015_nan', 'EMB_070_nan', 'EMB_072_nan', 'EMB_049_nan', 'EMB_109_nan', 'EMB_116_nan', 'EMB_103_nan', 'EMB_108_nan', 'EMB_105_nan', 'EMB_003_nan', 'EMB_095_nan', 'EMB_054_nan', 'EMB_042_nan', 'EMB_060_nan', 'EMB_046_nan', 'EMB_073_nan', 'EMB_088_nan', 'EMB_057_nan', 'EMB_002_nan', 'EMB_082_nan', 'EMB_080_nan', 'EMB_058_nan', 'EMB_124_nan', 'EMB_063_nan', 'EMB_045_nan', 'EMB_093_nan', 'EMB_066_nan', 'EMB_021_nan', 'EMB_086_nan', 'EMB_010_nan', 'EMB_029_nan', 'EMB_004_nan', 'EMB_001_nan', 'EMB_006_nan', 'EMB_097_nan', 'EMB_127_nan', 'EMB_107_nan', 'EMB_056_nan', 'EMB_075_nan', 'EMB_044_nan', 'EMB_024_nan', 'EMB_106_nan', 'EMB_023_nan', 'EMB_079_nan', 'EMB_094_nan', 'EMB_122_nan', 'EMB_115_nan', 'EMB_074_nan', 'EMB_009_nan', 'EMB_061_nan', 'EMB_048_nan', 'EMB_102_nan', 'EMB_047_nan', 'EMB_000_nan', 'EMB_011_nan', 'EMB_089_nan', 'EMB_118_nan', 'EMB_026_nan', 'EMB_050_nan', 'EMB_025_nan', 'EMB_035_nan', 'EMB_059_nan', 'EMB_017_nan', 'EMB_110_nan', 'EMB_119_nan', 'EMB_041_nan', 'EMB_005_nan', 'EMB_052_nan', 'EMB_030_nan', 'EMB_090_nan', 'EMB_126_nan', 'EMB_018_nan', 'EMB_012_nan', 'EMB_064_nan', 'EMB_101_nan', 'EMB_033_nan', 'EMB_036_nan', 'EMB_092_nan'] \n",
      "\u001b[94m[2023-11-17 06:36:23,597][ASSET][INFO][inference_pipeline][preprocess]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2023-11-17 06:36:23\n",
      "- current step      : preprocess\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'columns_map', 'preprocess'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:36:23,599][PROCESS][INFO]: ==================== Finish pipeline: inference_pipeline / step: preprocess\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_031</th>\n",
       "      <th>EMB_104</th>\n",
       "      <th>EMB_043</th>\n",
       "      <th>EMB_037</th>\n",
       "      <th>EMB_038</th>\n",
       "      <th>EMB_125</th>\n",
       "      <th>EMB_098</th>\n",
       "      <th>EMB_077</th>\n",
       "      <th>EMB_027</th>\n",
       "      <th>EMB_016</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_030_nan</th>\n",
       "      <th>EMB_090_nan</th>\n",
       "      <th>EMB_126_nan</th>\n",
       "      <th>EMB_018_nan</th>\n",
       "      <th>EMB_012_nan</th>\n",
       "      <th>EMB_064_nan</th>\n",
       "      <th>EMB_101_nan</th>\n",
       "      <th>EMB_033_nan</th>\n",
       "      <th>EMB_036_nan</th>\n",
       "      <th>EMB_092_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.008525</td>\n",
       "      <td>0.011202</td>\n",
       "      <td>0.002951</td>\n",
       "      <td>-0.014495</td>\n",
       "      <td>0.038221</td>\n",
       "      <td>0.019535</td>\n",
       "      <td>0.006818</td>\n",
       "      <td>0.003573</td>\n",
       "      <td>0.029008</td>\n",
       "      <td>-0.001390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032742</td>\n",
       "      <td>0.005224</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>-0.022976</td>\n",
       "      <td>0.025663</td>\n",
       "      <td>0.036108</td>\n",
       "      <td>-0.010685</td>\n",
       "      <td>-0.009478</td>\n",
       "      <td>-0.018460</td>\n",
       "      <td>-0.022039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011305</td>\n",
       "      <td>-0.002355</td>\n",
       "      <td>0.011688</td>\n",
       "      <td>0.001708</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>-0.009514</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>-0.004147</td>\n",
       "      <td>-0.017649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010245</td>\n",
       "      <td>0.014657</td>\n",
       "      <td>0.002380</td>\n",
       "      <td>-0.023908</td>\n",
       "      <td>-0.007041</td>\n",
       "      <td>0.005529</td>\n",
       "      <td>0.012320</td>\n",
       "      <td>0.006187</td>\n",
       "      <td>0.008182</td>\n",
       "      <td>-0.012942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.002716</td>\n",
       "      <td>-0.016088</td>\n",
       "      <td>-0.007759</td>\n",
       "      <td>0.012123</td>\n",
       "      <td>0.003149</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.008807</td>\n",
       "      <td>0.015945</td>\n",
       "      <td>-0.008076</td>\n",
       "      <td>-0.011258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>-0.013325</td>\n",
       "      <td>-0.003784</td>\n",
       "      <td>0.003302</td>\n",
       "      <td>0.016016</td>\n",
       "      <td>0.002450</td>\n",
       "      <td>0.008070</td>\n",
       "      <td>-0.029511</td>\n",
       "      <td>0.010089</td>\n",
       "      <td>0.013205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018034</td>\n",
       "      <td>0.003324</td>\n",
       "      <td>-0.002535</td>\n",
       "      <td>0.025256</td>\n",
       "      <td>-0.001771</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>-0.000827</td>\n",
       "      <td>-0.006499</td>\n",
       "      <td>-0.002078</td>\n",
       "      <td>-0.007425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007055</td>\n",
       "      <td>-0.012081</td>\n",
       "      <td>0.007710</td>\n",
       "      <td>-0.008706</td>\n",
       "      <td>0.003291</td>\n",
       "      <td>0.004677</td>\n",
       "      <td>0.028044</td>\n",
       "      <td>0.020258</td>\n",
       "      <td>-0.002945</td>\n",
       "      <td>-0.003443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.005031</td>\n",
       "      <td>-0.003910</td>\n",
       "      <td>-0.005302</td>\n",
       "      <td>0.009657</td>\n",
       "      <td>-0.002793</td>\n",
       "      <td>-0.008670</td>\n",
       "      <td>0.011824</td>\n",
       "      <td>0.012488</td>\n",
       "      <td>0.019366</td>\n",
       "      <td>-0.000175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004665</td>\n",
       "      <td>-0.013626</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>-0.001067</td>\n",
       "      <td>0.009883</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.006841</td>\n",
       "      <td>0.001502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.002576</td>\n",
       "      <td>-0.019350</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.016711</td>\n",
       "      <td>-0.006706</td>\n",
       "      <td>0.011058</td>\n",
       "      <td>-0.000500</td>\n",
       "      <td>-0.002399</td>\n",
       "      <td>-0.038739</td>\n",
       "      <td>0.031725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029427</td>\n",
       "      <td>0.014949</td>\n",
       "      <td>0.008464</td>\n",
       "      <td>-0.012945</td>\n",
       "      <td>-0.021204</td>\n",
       "      <td>-0.015321</td>\n",
       "      <td>0.013478</td>\n",
       "      <td>0.010060</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>-0.031791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000781</td>\n",
       "      <td>-0.016125</td>\n",
       "      <td>-0.001955</td>\n",
       "      <td>-0.008803</td>\n",
       "      <td>-0.007304</td>\n",
       "      <td>-0.047739</td>\n",
       "      <td>0.014152</td>\n",
       "      <td>-0.046066</td>\n",
       "      <td>-0.013409</td>\n",
       "      <td>-0.036250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028579</td>\n",
       "      <td>-0.005489</td>\n",
       "      <td>0.007006</td>\n",
       "      <td>-0.005084</td>\n",
       "      <td>0.018506</td>\n",
       "      <td>-0.010945</td>\n",
       "      <td>-0.019512</td>\n",
       "      <td>-0.002033</td>\n",
       "      <td>-0.004061</td>\n",
       "      <td>0.012846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.011470</td>\n",
       "      <td>-0.024099</td>\n",
       "      <td>0.035796</td>\n",
       "      <td>-0.002648</td>\n",
       "      <td>0.017315</td>\n",
       "      <td>-0.014082</td>\n",
       "      <td>-0.003605</td>\n",
       "      <td>-0.016682</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>-0.026923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005431</td>\n",
       "      <td>-0.007190</td>\n",
       "      <td>-0.005783</td>\n",
       "      <td>-0.022636</td>\n",
       "      <td>-0.026467</td>\n",
       "      <td>0.010841</td>\n",
       "      <td>0.006084</td>\n",
       "      <td>0.040247</td>\n",
       "      <td>0.005505</td>\n",
       "      <td>-0.013749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001083</td>\n",
       "      <td>-0.003770</td>\n",
       "      <td>0.013284</td>\n",
       "      <td>-0.021991</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>-0.011800</td>\n",
       "      <td>0.010231</td>\n",
       "      <td>0.020301</td>\n",
       "      <td>0.004736</td>\n",
       "      <td>-0.008520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003203</td>\n",
       "      <td>-0.003019</td>\n",
       "      <td>0.010786</td>\n",
       "      <td>0.007349</td>\n",
       "      <td>0.010961</td>\n",
       "      <td>0.014701</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.005039</td>\n",
       "      <td>0.004748</td>\n",
       "      <td>-0.001888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.008386</td>\n",
       "      <td>0.012499</td>\n",
       "      <td>-0.004206</td>\n",
       "      <td>-0.009606</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>-0.006232</td>\n",
       "      <td>-0.016146</td>\n",
       "      <td>0.011126</td>\n",
       "      <td>-0.006216</td>\n",
       "      <td>-0.002985</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014605</td>\n",
       "      <td>0.016407</td>\n",
       "      <td>-0.010216</td>\n",
       "      <td>-0.009930</td>\n",
       "      <td>0.007944</td>\n",
       "      <td>-0.004671</td>\n",
       "      <td>-0.013224</td>\n",
       "      <td>-0.012117</td>\n",
       "      <td>0.003376</td>\n",
       "      <td>-0.008876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    EMB_031   EMB_104   EMB_043   EMB_037   EMB_038   EMB_125   EMB_098  \\\n",
       "0  0.008525  0.011202  0.002951 -0.014495  0.038221  0.019535  0.006818   \n",
       "1  0.011305 -0.002355  0.011688  0.001708  0.012991 -0.009514  0.000210   \n",
       "2 -0.002716 -0.016088 -0.007759  0.012123  0.003149  0.001103  0.008807   \n",
       "3  0.018034  0.003324 -0.002535  0.025256 -0.001771  0.000677 -0.000827   \n",
       "4 -0.005031 -0.003910 -0.005302  0.009657 -0.002793 -0.008670  0.011824   \n",
       "5 -0.002576 -0.019350  0.001317  0.016711 -0.006706  0.011058 -0.000500   \n",
       "6  0.000781 -0.016125 -0.001955 -0.008803 -0.007304 -0.047739  0.014152   \n",
       "7  0.011470 -0.024099  0.035796 -0.002648  0.017315 -0.014082 -0.003605   \n",
       "8  0.001083 -0.003770  0.013284 -0.021991  0.005800 -0.011800  0.010231   \n",
       "9 -0.008386  0.012499 -0.004206 -0.009606  0.008500 -0.006232 -0.016146   \n",
       "\n",
       "    EMB_077   EMB_027   EMB_016  ...  EMB_030_nan  EMB_090_nan  EMB_126_nan  \\\n",
       "0  0.003573  0.029008 -0.001390  ...     0.032742     0.005224     0.002308   \n",
       "1 -0.000300 -0.004147 -0.017649  ...     0.010245     0.014657     0.002380   \n",
       "2  0.015945 -0.008076 -0.011258  ...     0.001252    -0.013325    -0.003784   \n",
       "3 -0.006499 -0.002078 -0.007425  ...     0.007055    -0.012081     0.007710   \n",
       "4  0.012488  0.019366 -0.000175  ...     0.004665    -0.013626     0.015686   \n",
       "5 -0.002399 -0.038739  0.031725  ...     0.029427     0.014949     0.008464   \n",
       "6 -0.046066 -0.013409 -0.036250  ...     0.028579    -0.005489     0.007006   \n",
       "7 -0.016682  0.000704 -0.026923  ...     0.005431    -0.007190    -0.005783   \n",
       "8  0.020301  0.004736 -0.008520  ...     0.003203    -0.003019     0.010786   \n",
       "9  0.011126 -0.006216 -0.002985  ...    -0.014605     0.016407    -0.010216   \n",
       "\n",
       "   EMB_018_nan  EMB_012_nan  EMB_064_nan  EMB_101_nan  EMB_033_nan  \\\n",
       "0    -0.022976     0.025663     0.036108    -0.010685    -0.009478   \n",
       "1    -0.023908    -0.007041     0.005529     0.012320     0.006187   \n",
       "2     0.003302     0.016016     0.002450     0.008070    -0.029511   \n",
       "3    -0.008706     0.003291     0.004677     0.028044     0.020258   \n",
       "4    -0.001067     0.009883     0.001509     0.002696     0.000680   \n",
       "5    -0.012945    -0.021204    -0.015321     0.013478     0.010060   \n",
       "6    -0.005084     0.018506    -0.010945    -0.019512    -0.002033   \n",
       "7    -0.022636    -0.026467     0.010841     0.006084     0.040247   \n",
       "8     0.007349     0.010961     0.014701     0.000328     0.005039   \n",
       "9    -0.009930     0.007944    -0.004671    -0.013224    -0.012117   \n",
       "\n",
       "   EMB_036_nan  EMB_092_nan  \n",
       "0    -0.018460    -0.022039  \n",
       "1     0.008182    -0.012942  \n",
       "2     0.010089     0.013205  \n",
       "3    -0.002945    -0.003443  \n",
       "4     0.006841     0.001502  \n",
       "5     0.000286    -0.031791  \n",
       "6    -0.004061     0.012846  \n",
       "7     0.005505    -0.013749  \n",
       "8     0.004748    -0.001888  \n",
       "9     0.003376    -0.008876  \n",
       "\n",
       "[10 rows x 256 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asset 실행\n",
    "preprocess_asset_structure=run(step, pipeline, asset_structure)\n",
    "\n",
    "# preprocess asset의 결과 dataframe은 preprocess_asset_structure.data['dataframe']으로 확인할 수 있습니다.  \n",
    "preprocess_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c107ab-179a-4ad9-b3ec-ba5195910eb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [2] Inference asset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a66a6da-7efd-4c9f-92b4-b91366365db7",
   "metadata": {},
   "source": [
    "#### 주요 Parameter\n",
    "- model_type: Train workflow의 Train asset과 동일하게 classification/regression 중 설정하면 됩니다. [*classification / regression*]\n",
    "- run_shapley: shapley 실행 여부를 선택합니다. [*True / False*]\n",
    "\n",
    "#### Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13b21092-9b65-4de1-874e-46e8cbd96075",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_type': 'classification', 'run_shapley': False}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - preprocess(1) - inference(2) - result(3))\n",
    "step = 2 \n",
    "asset_structure = copy.deepcopy(preprocess_asset_structure)\n",
    "asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 inference asset argument를 원하는 값으로 수정합니다.\n",
    "# asset_structure.args['x_columns'] = ['']\n",
    "asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5544d75-1e2f-4009-9e24-7cb91bc18ced",
   "metadata": {},
   "source": [
    "#### Inference asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf9857de-2615-41ec-829f-cddb0808aab2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ################################### inference_init (sec):  0.00013399124145507812 ################################### \n",
      "\n",
      "\u001b[94m[2023-11-17 06:36:23,633][ASSET][INFO][inference_pipeline][inference]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2023-11-17 06:36:23\n",
      "- current step      : inference\n",
      "- asset branch.     : tcr_v1.1.2\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : release-2.1\n",
      "- load envs. keys   : dict_keys(['solution_metadata_version', 'project_home', 'pipeline', 'step', 'num_step', 'artifacts', 'alo_version', 'asset_branch', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['model_type', 'run_shapley'])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'columns_map', 'preprocess'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[92m[2023-11-17 06:36:23,637][ASSET][INFO][inference_pipeline][inference]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr/alo/.train_artifacts/models/train/\u001b[0m\n",
      "\u001b[92m[2023-11-17 06:36:23,639][ASSET][INFO][inference_pipeline][inference]: Successfully got << output path >> for saving your data into csv or jpg file: \n",
      " /home/jovyan/gcr/alo/.inference_artifacts/output/inference/ \n",
      " - [NOTE] The names of output file must be fixed as << output.csv, output.jpg >> \u001b[0m\n",
      "해당 column 은 Training 과정에 사용되지 않습니다. (column_name: ['EMB_031', 'EMB_104', 'EMB_043', 'EMB_037', 'EMB_038', 'EMB_125', 'EMB_098', 'EMB_077', 'EMB_027', 'EMB_016', 'EMB_081', 'EMB_100', 'EMB_120', 'EMB_062', 'EMB_096', 'EMB_111', 'EMB_071', 'EMB_084', 'EMB_008', 'EMB_014', 'EMB_040', 'EMB_069', 'EMB_032', 'EMB_051', 'EMB_067', 'EMB_034', 'EMB_068', 'EMB_113', 'EMB_013', 'EMB_123', 'EMB_087', 'EMB_112', 'EMB_053', 'EMB_007', 'EMB_099', 'EMB_065', 'EMB_121', 'EMB_083', 'EMB_076', 'EMB_019', 'EMB_055', 'EMB_020', 'EMB_028', 'EMB_039', 'EMB_022', 'EMB_078', 'EMB_117', 'EMB_114', 'EMB_085', 'EMB_091', 'EMB_015', 'EMB_070', 'EMB_072', 'EMB_049', 'EMB_109', 'EMB_116', 'EMB_103', 'EMB_108', 'EMB_105', 'EMB_003', 'EMB_095', 'EMB_054', 'EMB_042', 'EMB_060', 'EMB_046', 'EMB_073', 'EMB_088', 'EMB_057', 'EMB_002', 'EMB_082', 'EMB_080', 'EMB_058', 'EMB_124', 'EMB_063', 'EMB_045', 'EMB_093', 'EMB_066', 'EMB_021', 'EMB_086', 'EMB_010', 'EMB_029', 'EMB_004', 'EMB_001', 'EMB_006', 'EMB_097', 'EMB_127', 'EMB_107', 'EMB_056', 'EMB_075', 'EMB_044', 'EMB_024', 'EMB_106', 'EMB_023', 'EMB_079', 'EMB_094', 'EMB_122', 'EMB_115', 'EMB_074', 'EMB_009', 'EMB_061', 'EMB_048', 'EMB_102', 'EMB_047', 'EMB_000', 'EMB_011', 'EMB_089', 'EMB_118', 'EMB_026', 'EMB_050', 'EMB_025', 'EMB_035', 'EMB_059', 'EMB_017', 'EMB_110', 'EMB_119', 'EMB_041', 'EMB_005', 'EMB_052', 'EMB_030', 'EMB_090', 'EMB_126', 'EMB_018', 'EMB_012', 'EMB_064', 'EMB_101', 'EMB_033', 'EMB_036', 'EMB_092'])\n",
      "\u001b[92m[2023-11-17 06:36:23,643][ASSET][INFO][inference_pipeline][inference]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr/alo/.train_artifacts/models/train/\u001b[0m\n",
      "[INFO] XAI 분석 시, 활용할 모델을 로드합니다.\n",
      "모델을 Load 완료 하였습니다. (모델 위치: /home/jovyan/gcr/alo/.train_artifacts/models/train/best_model_top0.pkl)\n",
      "[추론 데이터에 대한 모델 성능을 저장하기 위해 model_performance.json 파일을 생성합니다.\n",
      "\n",
      " ################################### inference_user_run (sec):  0.14342474937438965 ################################### \n",
      "\n",
      "\u001b[94m[2023-11-17 06:36:23,785][ASSET][INFO][inference_pipeline][inference]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2023-11-17 06:36:23\n",
      "- current step      : inference\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'columns_map', 'preprocess'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:36:23,786][PROCESS][INFO]: ==================== Finish pipeline: inference_pipeline / step: inference\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_000</th>\n",
       "      <th>EMB_001</th>\n",
       "      <th>EMB_002</th>\n",
       "      <th>EMB_003</th>\n",
       "      <th>EMB_004</th>\n",
       "      <th>EMB_005</th>\n",
       "      <th>EMB_006</th>\n",
       "      <th>EMB_007</th>\n",
       "      <th>EMB_008</th>\n",
       "      <th>EMB_009</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_123_nan</th>\n",
       "      <th>EMB_124_nan</th>\n",
       "      <th>EMB_125_nan</th>\n",
       "      <th>EMB_126_nan</th>\n",
       "      <th>EMB_127_nan</th>\n",
       "      <th>train_test</th>\n",
       "      <th>pred_</th>\n",
       "      <th>prediction_score</th>\n",
       "      <th>prob_0</th>\n",
       "      <th>prob_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.011211</td>\n",
       "      <td>0.021192</td>\n",
       "      <td>0.013510</td>\n",
       "      <td>0.010420</td>\n",
       "      <td>0.012326</td>\n",
       "      <td>0.010024</td>\n",
       "      <td>0.022406</td>\n",
       "      <td>-0.009880</td>\n",
       "      <td>0.006558</td>\n",
       "      <td>-0.014621</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003938</td>\n",
       "      <td>-0.015009</td>\n",
       "      <td>0.019535</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>-0.016103</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6703255719885912, 0.32967442801140917]</td>\n",
       "      <td>0.670326</td>\n",
       "      <td>0.329674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008541</td>\n",
       "      <td>0.004134</td>\n",
       "      <td>-0.006659</td>\n",
       "      <td>-0.008561</td>\n",
       "      <td>-0.013276</td>\n",
       "      <td>-0.011484</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.029092</td>\n",
       "      <td>0.010130</td>\n",
       "      <td>-0.020120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005121</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>-0.009514</td>\n",
       "      <td>0.002380</td>\n",
       "      <td>0.002756</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6877239522282103, 0.3122760477717894]</td>\n",
       "      <td>0.687724</td>\n",
       "      <td>0.312276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.019315</td>\n",
       "      <td>0.004849</td>\n",
       "      <td>-0.022684</td>\n",
       "      <td>0.012220</td>\n",
       "      <td>0.007098</td>\n",
       "      <td>0.011592</td>\n",
       "      <td>0.003066</td>\n",
       "      <td>-0.007337</td>\n",
       "      <td>-0.002128</td>\n",
       "      <td>0.004139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013134</td>\n",
       "      <td>0.002680</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>-0.003784</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7261614111036329, 0.27383858889636725]</td>\n",
       "      <td>0.726161</td>\n",
       "      <td>0.273839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013164</td>\n",
       "      <td>-0.009407</td>\n",
       "      <td>0.010454</td>\n",
       "      <td>-0.017179</td>\n",
       "      <td>0.021407</td>\n",
       "      <td>-0.004327</td>\n",
       "      <td>-0.006342</td>\n",
       "      <td>-0.009666</td>\n",
       "      <td>-0.005924</td>\n",
       "      <td>0.015952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001992</td>\n",
       "      <td>0.017462</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.007710</td>\n",
       "      <td>0.016784</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6811931537682666, 0.3188068462317331]</td>\n",
       "      <td>0.681193</td>\n",
       "      <td>0.318807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.008298</td>\n",
       "      <td>-0.003845</td>\n",
       "      <td>0.022752</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.018120</td>\n",
       "      <td>-0.002713</td>\n",
       "      <td>-0.004593</td>\n",
       "      <td>-0.026710</td>\n",
       "      <td>-0.010649</td>\n",
       "      <td>0.020809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.016854</td>\n",
       "      <td>-0.008670</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>-0.012213</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7746353495589344, 0.22536465044106616]</td>\n",
       "      <td>0.774635</td>\n",
       "      <td>0.225365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.014745</td>\n",
       "      <td>-0.001136</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>-0.010555</td>\n",
       "      <td>0.016204</td>\n",
       "      <td>0.015836</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.023532</td>\n",
       "      <td>0.016864</td>\n",
       "      <td>-0.015578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024693</td>\n",
       "      <td>0.003871</td>\n",
       "      <td>0.011058</td>\n",
       "      <td>0.008464</td>\n",
       "      <td>0.024613</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6730089126955886, 0.3269910873044111]</td>\n",
       "      <td>0.673009</td>\n",
       "      <td>0.326991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.031200</td>\n",
       "      <td>-0.008771</td>\n",
       "      <td>0.021719</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>0.006874</td>\n",
       "      <td>-0.011510</td>\n",
       "      <td>-0.011052</td>\n",
       "      <td>0.013309</td>\n",
       "      <td>0.008968</td>\n",
       "      <td>0.010274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001842</td>\n",
       "      <td>-0.011835</td>\n",
       "      <td>-0.047739</td>\n",
       "      <td>0.007006</td>\n",
       "      <td>-0.005282</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7035602123104578, 0.29643978768954216]</td>\n",
       "      <td>0.703560</td>\n",
       "      <td>0.296440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.004072</td>\n",
       "      <td>0.024870</td>\n",
       "      <td>-0.015082</td>\n",
       "      <td>-0.001626</td>\n",
       "      <td>0.014011</td>\n",
       "      <td>0.003743</td>\n",
       "      <td>0.049421</td>\n",
       "      <td>0.006498</td>\n",
       "      <td>-0.016665</td>\n",
       "      <td>-0.039833</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016026</td>\n",
       "      <td>-0.021061</td>\n",
       "      <td>-0.014082</td>\n",
       "      <td>-0.005783</td>\n",
       "      <td>-0.009873</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6888266042284608, 0.3111733957715394]</td>\n",
       "      <td>0.688827</td>\n",
       "      <td>0.311173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.020969</td>\n",
       "      <td>-0.003268</td>\n",
       "      <td>0.003840</td>\n",
       "      <td>0.008835</td>\n",
       "      <td>0.008807</td>\n",
       "      <td>-0.007809</td>\n",
       "      <td>0.004840</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.017214</td>\n",
       "      <td>-0.011284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009760</td>\n",
       "      <td>-0.007533</td>\n",
       "      <td>-0.011800</td>\n",
       "      <td>0.010786</td>\n",
       "      <td>-0.015307</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7984638102528191, 0.20153618974718163]</td>\n",
       "      <td>0.798464</td>\n",
       "      <td>0.201536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.012875</td>\n",
       "      <td>-0.005404</td>\n",
       "      <td>-0.025041</td>\n",
       "      <td>0.008551</td>\n",
       "      <td>-0.009934</td>\n",
       "      <td>-0.012423</td>\n",
       "      <td>-0.001929</td>\n",
       "      <td>-0.004785</td>\n",
       "      <td>0.002376</td>\n",
       "      <td>-0.020264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>-0.003956</td>\n",
       "      <td>-0.006232</td>\n",
       "      <td>-0.010216</td>\n",
       "      <td>-0.011842</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7350719212609111, 0.26492807873908875]</td>\n",
       "      <td>0.735072</td>\n",
       "      <td>0.264928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 261 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    EMB_000   EMB_001   EMB_002   EMB_003   EMB_004   EMB_005   EMB_006  \\\n",
       "0 -0.011211  0.021192  0.013510  0.010420  0.012326  0.010024  0.022406   \n",
       "1  0.008541  0.004134 -0.006659 -0.008561 -0.013276 -0.011484  0.001329   \n",
       "2  0.019315  0.004849 -0.022684  0.012220  0.007098  0.011592  0.003066   \n",
       "3  0.013164 -0.009407  0.010454 -0.017179  0.021407 -0.004327 -0.006342   \n",
       "4 -0.008298 -0.003845  0.022752  0.004630  0.018120 -0.002713 -0.004593   \n",
       "5  0.014745 -0.001136  0.002283 -0.010555  0.016204  0.015836  0.001790   \n",
       "6 -0.031200 -0.008771  0.021719  0.036718  0.006874 -0.011510 -0.011052   \n",
       "7  0.004072  0.024870 -0.015082 -0.001626  0.014011  0.003743  0.049421   \n",
       "8 -0.020969 -0.003268  0.003840  0.008835  0.008807 -0.007809  0.004840   \n",
       "9  0.012875 -0.005404 -0.025041  0.008551 -0.009934 -0.012423 -0.001929   \n",
       "\n",
       "    EMB_007   EMB_008   EMB_009  ...  EMB_123_nan  EMB_124_nan  EMB_125_nan  \\\n",
       "0 -0.009880  0.006558 -0.014621  ...    -0.003938    -0.015009     0.019535   \n",
       "1  0.029092  0.010130 -0.020120  ...     0.005121     0.002899    -0.009514   \n",
       "2 -0.007337 -0.002128  0.004139  ...     0.013134     0.002680     0.001103   \n",
       "3 -0.009666 -0.005924  0.015952  ...    -0.001992     0.017462     0.000677   \n",
       "4 -0.026710 -0.010649  0.020809  ...     0.000817     0.016854    -0.008670   \n",
       "5  0.023532  0.016864 -0.015578  ...     0.024693     0.003871     0.011058   \n",
       "6  0.013309  0.008968  0.010274  ...     0.001842    -0.011835    -0.047739   \n",
       "7  0.006498 -0.016665 -0.039833  ...    -0.016026    -0.021061    -0.014082   \n",
       "8  0.002440  0.017214 -0.011284  ...     0.009760    -0.007533    -0.011800   \n",
       "9 -0.004785  0.002376 -0.020264  ...     0.001475    -0.003956    -0.006232   \n",
       "\n",
       "   EMB_126_nan  EMB_127_nan  train_test  pred_  \\\n",
       "0     0.002308    -0.016103        test      0   \n",
       "1     0.002380     0.002756        test      0   \n",
       "2    -0.003784     0.000100        test      0   \n",
       "3     0.007710     0.016784        test      0   \n",
       "4     0.015686    -0.012213        test      0   \n",
       "5     0.008464     0.024613        test      0   \n",
       "6     0.007006    -0.005282        test      0   \n",
       "7    -0.005783    -0.009873        test      0   \n",
       "8     0.010786    -0.015307        test      0   \n",
       "9    -0.010216    -0.011842        test      0   \n",
       "\n",
       "                            prediction_score    prob_0    prob_1  \n",
       "0  [0.6703255719885912, 0.32967442801140917]  0.670326  0.329674  \n",
       "1   [0.6877239522282103, 0.3122760477717894]  0.687724  0.312276  \n",
       "2  [0.7261614111036329, 0.27383858889636725]  0.726161  0.273839  \n",
       "3   [0.6811931537682666, 0.3188068462317331]  0.681193  0.318807  \n",
       "4  [0.7746353495589344, 0.22536465044106616]  0.774635  0.225365  \n",
       "5   [0.6730089126955886, 0.3269910873044111]  0.673009  0.326991  \n",
       "6  [0.7035602123104578, 0.29643978768954216]  0.703560  0.296440  \n",
       "7   [0.6888266042284608, 0.3111733957715394]  0.688827  0.311173  \n",
       "8  [0.7984638102528191, 0.20153618974718163]  0.798464  0.201536  \n",
       "9  [0.7350719212609111, 0.26492807873908875]  0.735072  0.264928  \n",
       "\n",
       "[10 rows x 261 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asset 실행\n",
    "inference_asset_structure=run(step, pipeline, asset_structure)\n",
    "\n",
    "# inference asset의 결과 dataframe은 inference_asset_structure.data['dataframe']으로 확인할 수 있습니다.  \n",
    "inference_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93e1b561-2352-4474-ab09-c7ae583a7f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['EMB_000', 'EMB_001', 'EMB_002', 'EMB_003', 'EMB_004', 'EMB_005',\n",
       "       'EMB_006', 'EMB_007', 'EMB_008', 'EMB_009',\n",
       "       ...\n",
       "       'EMB_123_nan', 'EMB_124_nan', 'EMB_125_nan', 'EMB_126_nan',\n",
       "       'EMB_127_nan', 'train_test', 'pred_', 'prediction_score', 'prob_0',\n",
       "       'prob_1'],\n",
       "      dtype='object', length=261)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_asset_structure.data['dataframe'].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b9e329-d954-411c-9c33-42656d29bf01",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [3] Result asset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e749a64d",
   "metadata": {},
   "source": [
    "#### 주요 Parameter\n",
    "- result_save_name: 결과 저장 파일명을 설정합니다.\n",
    "\n",
    "#### Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17905521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result_save_name': None}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - preprocess(1) - inference(2) - result(3))\n",
    "step = 3\n",
    "asset_structure = copy.deepcopy(inference_asset_structure)\n",
    "asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 result asset argument를 원하는 값으로 수정합니다.\n",
    "# asset_structure.args['x_columns'] = ['']\n",
    "asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c2f5c",
   "metadata": {},
   "source": [
    "#### Result asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c899d49e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2023-11-17 06:36:23,828][ASSET][INFO][inference_pipeline][result]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2023-11-17 06:36:23\n",
      "- current step      : result\n",
      "- asset branch.     : release-1.2\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : release-2.1\n",
      "- load envs. keys   : dict_keys(['solution_metadata_version', 'project_home', 'pipeline', 'step', 'num_step', 'artifacts', 'alo_version', 'asset_branch', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['result_save_name'])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'columns_map', 'preprocess'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "Loading Embeddings\n",
      "\u001b[92m[2023-11-17 06:36:23,831][ASSET][INFO][inference_pipeline][result]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr/alo/.train_artifacts/models/result/\u001b[0m\n",
      "Merging data\n",
      "\u001b[92m[2023-11-17 06:36:24,084][ASSET][INFO][inference_pipeline][result]: Successfully got << output path >> for saving your data into csv or jpg file: \n",
      " /home/jovyan/gcr/alo/.inference_artifacts/output/result/ \n",
      " - [NOTE] The names of output file must be fixed as << output.csv, output.jpg >> \u001b[0m\n",
      "\u001b[92m[2023-11-17 06:36:24,094][ASSET][INFO][inference_pipeline][result]: Successfully got << output path >> for saving your data into csv or jpg file: \n",
      " /home/jovyan/gcr/alo/.inference_artifacts/output/result/ \n",
      " - [NOTE] The names of output file must be fixed as << output.csv, output.jpg >> \u001b[0m\n",
      "Check Result at /home/jovyan/gcr/alo/.inference_artifacts/output/result/inference_result.csv\n",
      "\u001b[94m[2023-11-17 06:36:24,096][ASSET][INFO][inference_pipeline][result]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2023-11-17 06:36:24\n",
      "- current step      : result\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'columns_map', 'preprocess'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-17 06:36:24,097][PROCESS][INFO]: ==================== Finish pipeline: inference_pipeline / step: result\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>is_married</th>\n",
       "      <th>train_test</th>\n",
       "      <th>pred_</th>\n",
       "      <th>prediction_score</th>\n",
       "      <th>prob_0</th>\n",
       "      <th>prob_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gregory_Hull</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6703255719885912, 0.32967442801140917]</td>\n",
       "      <td>0.670326</td>\n",
       "      <td>0.329674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allison_Peterson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6877239522282103, 0.3122760477717894]</td>\n",
       "      <td>0.687724</td>\n",
       "      <td>0.312276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Daniel_Davies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7261614111036329, 0.27383858889636725]</td>\n",
       "      <td>0.726161</td>\n",
       "      <td>0.273839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alison_Fox</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6811931537682666, 0.3188068462317331]</td>\n",
       "      <td>0.681193</td>\n",
       "      <td>0.318807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Daniel_Moore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7746353495589344, 0.22536465044106616]</td>\n",
       "      <td>0.774635</td>\n",
       "      <td>0.225365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Barbara_Smith</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6730089126955886, 0.3269910873044111]</td>\n",
       "      <td>0.673009</td>\n",
       "      <td>0.326991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Paul_Terry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7035602123104578, 0.29643978768954216]</td>\n",
       "      <td>0.703560</td>\n",
       "      <td>0.296440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Christina_Salas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6888266042284608, 0.3111733957715394]</td>\n",
       "      <td>0.688827</td>\n",
       "      <td>0.311173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jose_Boyd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7984638102528191, 0.20153618974718163]</td>\n",
       "      <td>0.798464</td>\n",
       "      <td>0.201536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Zachary_Fowler</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7350719212609111, 0.26492807873908875]</td>\n",
       "      <td>0.735072</td>\n",
       "      <td>0.264928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name  is_married train_test  pred_  \\\n",
       "0      Gregory_Hull         NaN       test      0   \n",
       "1  Allison_Peterson         NaN       test      0   \n",
       "2     Daniel_Davies         NaN       test      0   \n",
       "3        Alison_Fox         NaN       test      0   \n",
       "4      Daniel_Moore         NaN       test      0   \n",
       "5     Barbara_Smith         NaN       test      0   \n",
       "6        Paul_Terry         NaN       test      0   \n",
       "7   Christina_Salas         NaN       test      0   \n",
       "8         Jose_Boyd         NaN       test      0   \n",
       "9    Zachary_Fowler         NaN       test      0   \n",
       "\n",
       "                            prediction_score    prob_0    prob_1  \n",
       "0  [0.6703255719885912, 0.32967442801140917]  0.670326  0.329674  \n",
       "1   [0.6877239522282103, 0.3122760477717894]  0.687724  0.312276  \n",
       "2  [0.7261614111036329, 0.27383858889636725]  0.726161  0.273839  \n",
       "3   [0.6811931537682666, 0.3188068462317331]  0.681193  0.318807  \n",
       "4  [0.7746353495589344, 0.22536465044106616]  0.774635  0.225365  \n",
       "5   [0.6730089126955886, 0.3269910873044111]  0.673009  0.326991  \n",
       "6  [0.7035602123104578, 0.29643978768954216]  0.703560  0.296440  \n",
       "7   [0.6888266042284608, 0.3111733957715394]  0.688827  0.311173  \n",
       "8  [0.7984638102528191, 0.20153618974718163]  0.798464  0.201536  \n",
       "9  [0.7350719212609111, 0.26492807873908875]  0.735072  0.264928  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asset 실행\n",
    "result_asset_structure=run(step, pipeline, asset_structure)\n",
    "\n",
    "# result asset의 결과 dataframe은 result_asset_structure.data['dataframe']으로 확인할 수 있습니다.\n",
    "result_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36aa7cc-9b57-48ea-bec5-2b3cb0c02946",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **4. Batch Running**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ba7a95-1c7b-4e33-8eab-3362dcd7402e",
   "metadata": {},
   "source": [
    "Asset 단위가 아닌 전체 workflows에 대해 한번에 동작 시킬 수 있습니다.\n",
    "> 1. alo 디렉토리로 이동합니다.    \n",
    ">> cd alo    \n",
    "> 2. main.py를 실행합니다. \n",
    ">> python main.py\n",
    "\n",
    "*Sample Notebook에서 반영한 parameter는 experimental_plan.yaml에 반영되지 않습니다.*   \n",
    "*config/experimental_plan.yaml을 직접 수정하여 사용하시길 바랍니다.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a35634d-a522-4903-bc86-a89bb7303d90",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **5. 문의 및 기능 개발 요청**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7446e64b-5a59-42da-bcb2-bd7ed88e0ec3",
   "metadata": {},
   "source": [
    "사용중 **Issue 발생** 또는 **기능 요청** 건이 있으실 경우 아래 CLM을 통해 문의/요청 바랍니다.   \n",
    "CLM : http://clm.lge.com/issue/projects/AICONTENTS\n",
    "\n",
    "담당자: 공성우 선임, 김정원 연구원\n",
    "\n",
    "*긴급한 건에 대해서는 담당자에게 연락 바랍니다.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf86e25-352f-434e-88b7-4a4d7deff069",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **6. References**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0296eed9-6dc4-4de8-bb8d-d2f81c8014d7",
   "metadata": {},
   "source": [
    "GCR Release Note : http://collab.lge.com/main/x/iIjdgQ\n",
    "\n",
    "User Guide : http://collab.lge.com/main/x/Owo8gg\n",
    "\n",
    "데이터 명세서 : http://collab.lge.com/main/x/QAo8gg\n",
    "\n",
    "알고리즘 설명서 : http://collab.lge.com/main/x/Zgo8gg\n",
    "\n",
    "GCR Contents Git : http://mod.lge.com/hub/dxadvtech/aicontents/gcr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcr",
   "language": "python",
   "name": "gcr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
