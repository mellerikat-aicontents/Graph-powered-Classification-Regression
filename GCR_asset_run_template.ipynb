{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "194fe113-d67a-476d-ae96-7f72605a4af1",
   "metadata": {},
   "source": [
    "# Welcome to Jupyter Notebook for GCR\n",
    "\n",
    "간편 설명서: http://mod.lge.com/hub/dxadvtech/aicontents/gcr\n",
    "\n",
    "User Guide(상세): http://collab.lge.com/main/x/Owo8gg\n",
    "\n",
    "본 노트북은 Asset 별 parameter를 바꿔가며 각 asset의 결과가 어떻게 달라지는지 사용자가 확인하는 용도로 활용 가능합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3858a5e5-db47-49aa-bcf6-08fcb4561ed2",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "## 데이터 명세\n",
    "링크 : http://collab.lge.com/main/x/QAo8gg\n",
    "\n",
    "### 데이터 준비\n",
    "1. Train, Inference 두 개의 데이터셋을 준비합니다.\n",
    "2. 각 데이터에 FLAG_TRAIN_INFERENCE 컬럼을 추가합니다. \n",
    "   - 각 'Train', 'Inference'가 flag로 들어가야 합니다.\n",
    "3. 두 데이터를 합쳐 하나의 데이터셋으로 구성합니다.\n",
    "\n",
    "***GCR은 결측치와 범주형 데이터에 대한 전처리가 필요하지 않습니다***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b6b5c-8668-46b6-b5d8-00888c5580fa",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "## ALO Setup\n",
    "라이브러리 설치 및 컨텐츠 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "137cd365-9ed9-4941-aa2e-67fca06e1a4f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "os.chdir(os.path.abspath(os.path.join('./alo')))\n",
    "from src.alo import ALO\n",
    "from src.alo import AssetStructure\n",
    "alo = ALO(); alo.preset(); pipelines = list(alo.asset_source.keys())\n",
    "from src.external import external_load_data, external_save_artifacts\n",
    "\n",
    "def run(step, pipeline, asset_structure):\n",
    "    # 반복되는 작업을 함수로 변환\n",
    "    asset_config = alo.asset_source[pipeline]\n",
    "    return alo.process_asset_step(asset_config[step], step, pipeline, asset_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171f4e9-d318-47ec-8374-f49fb6b30356",
   "metadata": {},
   "source": [
    "## Train Workflow\n",
    "GCR의 Train Workflow 구성은 다음과 같습니다.\n",
    "1. Input : 사용자가 지정한 경로로부터 데이터를 Import\n",
    "2. Graph : 데이터를 토대로 그래프를 구성하고 필요한 임베딩 추출\n",
    "3. Preprocess : (필요시) 결측치 처리 및 라벨 인코딩\n",
    "4. Sampling : (필요시) Imbalance 데이터의 Undersampling\n",
    "5. Train : ML 모델 학습\n",
    "\n",
    "아래 코드를 실행하여 Train Workflow에 필요한 라이브러리를 먼저 설치 해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b7d0f4d-05da-40d0-9b5d-75c496b5794a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2023-11-15 10:36:33,890][PROCESS][INFO]: You did not write any << s3_private_key_file >> in the config yaml file. When you wanna get data from s3 storage, \n",
      "                                 you have to write the s3_private_key_file path or set << ACCESS_KEY, SECRET_KEY >> in your os environment. \n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,895][PROCESS][INFO]:  Start loading external data. << /nas001/users/seongwoo.kong/gcr_test_data/sample/ >>  \n",
      " << sample >> does not exist in << /home/jovyan/gcr_dev/alo/input/ >>. \n",
      " & << get_external_data >> is set as << once >>. \n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,908][PROCESS][INFO]: Start setting-up << input >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,910][PROCESS][INFO]: << input >> asset had already been created at 2023-11-15 10:14:14.470202\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,913][PROCESS][INFO]: Start setting-up << graph >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,916][PROCESS][INFO]: << graph >> asset had already been created at 2023-11-15 10:14:15.835214\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,918][PROCESS][INFO]: Start setting-up << preprocess >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,921][PROCESS][INFO]: << preprocess >> asset had already been created at 2023-11-15 10:14:37.441404\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,925][PROCESS][INFO]: Start setting-up << sampling >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,928][PROCESS][INFO]: << sampling >> asset had already been created at 2023-11-15 10:14:39.913426\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,930][PROCESS][INFO]: Start setting-up << train >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,933][PROCESS][INFO]: << train >> asset had already been created at 2023-11-15 10:14:45.281474\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,937][PROCESS][INFO]: >>> Ignored installing << torch==2.0.0 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,940][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,942][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,944][PROCESS][INFO]: >>> Ignored installing << numpy==1.25.2 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,946][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,949][PROCESS][INFO]: >>> Ignored installing << scikit-learn >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,952][PROCESS][INFO]: >>> Ignored installing << matplotlib >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,955][PROCESS][INFO]: ======================================== Start dependency installation : << input >> \u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,957][PROCESS][INFO]: Start checking existence & installing package - pandas==1.5.3 | Progress: ( 1 / 14 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-15 10:36:33,959][PROCESS][INFO]: [OK] << pandas==1.5.3 >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,962][PROCESS][INFO]: ======================================== Start dependency installation : << graph >> \u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,964][PROCESS][INFO]: Start checking existence & installing package - torch==2.0.0 | Progress: ( 2 / 14 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-15 10:36:33,968][PROCESS][INFO]: [OK] << torch==2.0.0 >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,970][PROCESS][INFO]: Start checking existence & installing package - torchbiggraph@git+https://github.com/facebookresearch/PyTorch-BigGraph.git | Progress: ( 3 / 14 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-15 10:36:33,973][PROCESS][INFO]: [OK] << torchbiggraph@git+https://github.com/facebookresearch/PyTorch-BigGraph.git >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,975][PROCESS][INFO]: ======================================== Start dependency installation : << preprocess >> \u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,978][PROCESS][INFO]: Start checking existence & installing package - category_encoders | Progress: ( 4 / 14 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-15 10:36:33,980][PROCESS][INFO]: [OK] << category_encoders >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,983][PROCESS][INFO]: ======================================== Start dependency installation : << sampling >> \u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,985][PROCESS][INFO]: Start checking existence & installing package - numpy==1.25.2 | Progress: ( 5 / 14 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-15 10:36:33,988][PROCESS][INFO]: [OK] << numpy==1.25.2 >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,990][PROCESS][INFO]: Start checking existence & installing package - scikit-learn | Progress: ( 6 / 14 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-15 10:36:33,993][PROCESS][INFO]: [OK] << scikit-learn >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:33,995][PROCESS][INFO]: Start checking existence & installing package - umap-learn | Progress: ( 7 / 14 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-15 10:36:33,998][PROCESS][INFO]: [OK] << umap-learn >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:34,000][PROCESS][INFO]: Start checking existence & installing package - matplotlib | Progress: ( 8 / 14 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-15 10:36:34,003][PROCESS][INFO]: [OK] << matplotlib >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:34,005][PROCESS][INFO]: ======================================== Start dependency installation : << train >> \u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:34,007][PROCESS][INFO]: Start checking existence & installing package - seaborn | Progress: ( 9 / 14 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-15 10:36:34,010][PROCESS][INFO]: [OK] << seaborn >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:34,012][PROCESS][INFO]: Start checking existence & installing package - shap | Progress: ( 10 / 14 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-15 10:36:34,015][PROCESS][INFO]: [OK] << shap >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:34,017][PROCESS][INFO]: Start checking existence & installing package - lightgbm | Progress: ( 11 / 14 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-15 10:36:34,020][PROCESS][INFO]: [OK] << lightgbm >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:34,022][PROCESS][INFO]: Start checking existence & installing package - catboost | Progress: ( 12 / 14 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-15 10:36:34,025][PROCESS][INFO]: [OK] << catboost >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:34,027][PROCESS][INFO]: Start checking existence & installing package - ngboost | Progress: ( 13 / 14 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-15 10:36:34,030][PROCESS][INFO]: [OK] << ngboost >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:34,032][PROCESS][INFO]: ======================================== Start dependency installation : << force-reinstall >> \u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:34,035][PROCESS][INFO]: Start checking existence & installing package - numpy==1.25.2 --force-reinstall | Progress: ( 14 / 14 total packages ) \u001b[0m\n",
      "\u001b[94m[2023-11-15 10:36:34,038][PROCESS][INFO]: >>> Start installing package - numpy==1.25.2 --force-reinstall\u001b[0m\n",
      "Collecting numpy==1.25.2\n",
      "  Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.2\n",
      "    Uninstalling numpy-1.25.2:\n",
      "      Successfully uninstalled numpy-1.25.2\n",
      "Successfully installed numpy-1.25.2\n",
      "\u001b[94m[2023-11-15 10:36:43,058][PROCESS][INFO]: ======================================== Finish dependency installation \n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "external_load_data(pipelines[0], alo.external_path, alo.external_path_permission, alo.control['get_external_data'])\n",
    "pipeline = pipelines[0]\n",
    "alo.install_steps(pipeline, alo.control[\"get_asset_source\"])\n",
    "\n",
    "# 초기 data structure 구성\n",
    "envs, args, data, config = {}, {}, {}, {}\n",
    "init_asset_structure = AssetStructure(envs, args, data, config)\n",
    "# logger init\n",
    "alo.set_proc_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9f36d6-84e0-440e-80d3-b462254b006a",
   "metadata": {
    "tags": []
   },
   "source": [
    "</br>\n",
    "\n",
    "### 0. Input asset \n",
    "#### 주요 Parameter\n",
    "- *input_path : Extenal Path에서 받은 데이터는 alo/input/train에 저장됩니다. 이 중에서 사용할 데이터가 저장된 디렉터리 명을 작성해주시면 됩니다.\n",
    "- x_columns : 데이터의 모든 컬럼을 활용하지 않는 경우엔 직접 선택해서 사용할 수 있습니다. 모두 사용하는 경우 use_all_x를 True로 설정하고 사용하면 됩니다.\n",
    "- *y_column : Classification, Regression을 위해서는 Label이 있어야 합니다. Label에 해당하는 컬럼을 작성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90e2c80e-0a14-4e19-b50f-2aa621aec2da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_path': 'sample',\n",
       " 'x_columns': None,\n",
       " 'use_all_x': True,\n",
       " 'y_column': 'is_married',\n",
       " 'groupkey_columns': None,\n",
       " 'drop_columns': None,\n",
       " 'time_column': None,\n",
       " 'concat_dataframes': None,\n",
       " 'encoding': None}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - graph(1) - preprocess(2) - sampling(3) - train(4))\n",
    "step = 0 \n",
    "asset_structure = copy.deepcopy(init_asset_structure)\n",
    "asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 input asset argument를 원하는 값으로 수정합니다. \n",
    "# asset_structure.args['x_columns'] = ['']\n",
    "asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017bbb5c-b15e-4b8b-98c9-9b355da55263",
   "metadata": {},
   "source": [
    "#### Input asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ac5a67f-02e5-49f5-9d44-34f24b08baaf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-11-15 10:37:22,583][USER][INFO][train_pipeline][input]: >> Load path : ['/home/jovyan/gcr_dev/alo/input/train/sample/']\n",
      "[2023-11-15 10:37:22,598][USER][INFO][train_pipeline][input]: >> The file for batch data has been loaded. (File name: /home/jovyan/gcr_dev/alo/input/train/sample/customers.csv)\n",
      "[2023-11-15 10:37:22,601][USER][INFO][train_pipeline][input]: You set the << use_all_x >> as << True >> in the yaml file. So skip checking dataframe columns existence.\n",
      "[2023-11-15 10:37:22,604][USER][INFO][train_pipeline][input]: ==================== Success loading dataframe ====================\n",
      "[2023-11-15 10:37:22,607][USER][INFO][train_pipeline][input]: >> Start processing ignore columns & drop columns: ['/home/jovyan/gcr_dev/alo/input/train/sample/customers.csv']\n",
      "[2023-11-15 10:37:22,610][USER][INFO][train_pipeline][input]: >> You set the << use_all_x >> parameter as << True >> in your config yaml. (So, these x_columns are used: ['age', 'name', 'job', 'gender', 'FLAG_TRAIN_INFERENCE', 'address', 'orders', 'spent', 'hobbies'] )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2023-11-15 10:37:22,578][ASSET][INFO][train_pipeline][input]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2023-11-15 10:37:22\n",
      "- current step      : input\n",
      "- asset branch.     : tabular_2.0\n",
      "- alolib ver.       : 2.0\n",
      "- alo ver.          : release-2.0\n",
      "- load envs. keys   : dict_keys(['project_home', 'pipeline', 'step', 'num_step', 'artifacts', 'alo_version', 'asset_branch', 'interface_mode', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path'])\n",
      "- load args. keys   : dict_keys(['input_path', 'x_columns', 'use_all_x', 'y_column', 'groupkey_columns', 'drop_columns', 'time_column', 'concat_dataframes', 'encoding'])\n",
      "- load config. keys : dict_keys(['meta'])\n",
      "- load data keys    : dict_keys([])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:37:22,611][ASSET][INFO][train_pipeline][input]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2023-11-15 10:37:22\n",
      "- current step      : input\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:37:22,613][PROCESS][INFO]: ==================== Finish pipeline: train_pipeline / step: input\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>orders</th>\n",
       "      <th>spent</th>\n",
       "      <th>job</th>\n",
       "      <th>hobbies</th>\n",
       "      <th>is_married</th>\n",
       "      <th>FLAG_TRAIN_INFERENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jasmine_Young</td>\n",
       "      <td>TN17745</td>\n",
       "      <td>female</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>233.44</td>\n",
       "      <td>Receptionist</td>\n",
       "      <td>Photography</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jeffery_Robinson</td>\n",
       "      <td>CT69980</td>\n",
       "      <td>male</td>\n",
       "      <td>42</td>\n",
       "      <td>15</td>\n",
       "      <td>264.70</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>Fishing</td>\n",
       "      <td>True</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Steven_Sullivan</td>\n",
       "      <td>CT13314</td>\n",
       "      <td>male</td>\n",
       "      <td>70</td>\n",
       "      <td>13</td>\n",
       "      <td>339.10</td>\n",
       "      <td>Janitor</td>\n",
       "      <td>Hiking</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jay_Williams</td>\n",
       "      <td>TN68283</td>\n",
       "      <td>male</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>70.61</td>\n",
       "      <td>Waitress</td>\n",
       "      <td>Playing musical instruments</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Benjamin_Beck</td>\n",
       "      <td>AE11377</td>\n",
       "      <td>male</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>748.94</td>\n",
       "      <td>Farmer</td>\n",
       "      <td>Playing sports</td>\n",
       "      <td>True</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gregory_Gomez</td>\n",
       "      <td>FM04887</td>\n",
       "      <td>male</td>\n",
       "      <td>75</td>\n",
       "      <td>10</td>\n",
       "      <td>937.97</td>\n",
       "      <td>Unkown</td>\n",
       "      <td>Running</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mary_Harris</td>\n",
       "      <td>KS55063</td>\n",
       "      <td>female</td>\n",
       "      <td>60</td>\n",
       "      <td>12</td>\n",
       "      <td>60.97</td>\n",
       "      <td>Librarian</td>\n",
       "      <td>Reading</td>\n",
       "      <td>True</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jimmy_Smith</td>\n",
       "      <td>AL47190</td>\n",
       "      <td>male</td>\n",
       "      <td>72</td>\n",
       "      <td>5</td>\n",
       "      <td>468.64</td>\n",
       "      <td>Waitress</td>\n",
       "      <td>Sewing</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kenneth_Rubio</td>\n",
       "      <td>RI07301</td>\n",
       "      <td>male</td>\n",
       "      <td>74</td>\n",
       "      <td>15</td>\n",
       "      <td>482.72</td>\n",
       "      <td>Polic</td>\n",
       "      <td>Dancing</td>\n",
       "      <td>True</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jordan_Simmons</td>\n",
       "      <td>AA06497</td>\n",
       "      <td>female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>156.16</td>\n",
       "      <td>Cashier</td>\n",
       "      <td>Baking</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name  address  gender  age  orders   spent           job  \\\n",
       "0     Jasmine_Young  TN17745  female   80       0  233.44  Receptionist   \n",
       "1  Jeffery_Robinson  CT69980    male   42      15  264.70       Teacher   \n",
       "2   Steven_Sullivan  CT13314    male   70      13  339.10       Janitor   \n",
       "3      Jay_Williams  TN68283    male   27       7   70.61      Waitress   \n",
       "4     Benjamin_Beck  AE11377    male   21       9  748.94        Farmer   \n",
       "5     Gregory_Gomez  FM04887    male   75      10  937.97        Unkown   \n",
       "6       Mary_Harris  KS55063  female   60      12   60.97     Librarian   \n",
       "7       Jimmy_Smith  AL47190    male   72       5  468.64      Waitress   \n",
       "8     Kenneth_Rubio  RI07301    male   74      15  482.72         Polic   \n",
       "9    Jordan_Simmons  AA06497  female   41       1  156.16       Cashier   \n",
       "\n",
       "                       hobbies is_married FLAG_TRAIN_INFERENCE  \n",
       "0                  Photography      False                TRAIN  \n",
       "1                      Fishing       True                TRAIN  \n",
       "2                       Hiking      False                TRAIN  \n",
       "3  Playing musical instruments      False                TRAIN  \n",
       "4               Playing sports       True                TRAIN  \n",
       "5                      Running      False                TRAIN  \n",
       "6                      Reading       True                TRAIN  \n",
       "7                       Sewing      False                TRAIN  \n",
       "8                      Dancing       True                TRAIN  \n",
       "9                       Baking      False                TRAIN  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_asset_structure=run(step, pipeline, asset_structure)\n",
    "\n",
    "# input asset의 결과 dataframe은 input_asset_structure.data['dataframe']으로 확인할 수 있습니다. \n",
    "input_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b53121",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### 1. Graph asset \n",
    "GCR의 가장 핵심이 되는 asset입니다. Raw Data를 임베딩으로 모두 변환하여 별도의 전처리 없이 더 높은 성능의 ML모델을 획득하기 위한 Tool이 됩니다.\n",
    "#### 주요 Parameter\n",
    "- graph_type : 그래프의 구조를 선택할 수 있습니다. radial은 방사형 구조, relational은 관계형 구조입니다.\n",
    "- center_node_column : 방사형 그래프는 중심 컬럼을 기준으로 그래프를 구성합니다. 데이터의 대표가 되는 컬럼을 선택하면 됩니다.(ex. ID, Name 등)\n",
    "- embedding_column : 임베딩의 대상이 되는 컬럼을 선택합니다. 라벨이 center_node_column과 대응하지 않는 경우가 있을 수 있습니다. 이때는 label이 대응하는 컬럼을 embedding column으로 설정합니다.\n",
    "- drop_columns : 그래프 구성에서 제외할 컬럼을 선택합니다.\n",
    "- num_epochs: Embedding 학습을 위한 epoch을 설정합니다.\n",
    "- workers, num_partitions : 주어진 리소스와 데이터의 크기에 따라 필요한 경우 설정하여 사용합니다.\n",
    "- extra_columns_for_ml : ml 모델 학습에 임베딩 외에 사용될 컬럼을 지정합니다. 도메인 지식에 기반하여 선택할 필요가 있습니다.\n",
    "- custom_connection : 그래프를 활용할 줄 안다면 직접 relation을 정의할 수 있습니다. 자세한 내용은 manual을 참고."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05226c4c-981b-46e6-a187-72e3d47433f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'graph_type': None,\n",
       " 'center_node_column': 'name',\n",
       " 'embedding_column': 'name',\n",
       " 'train_inference_column': 'FLAG_TRAIN_INFERENCE',\n",
       " 'drop_columns': [],\n",
       " 'dimension': 64,\n",
       " 'num_epochs': 1,\n",
       " 'workers': None,\n",
       " 'num_partitions': None,\n",
       " 'extra_columns_for_ml': [],\n",
       " 'custom_connection': []}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - graph(1) - preprocess(2) - sampling(3) - train(4))\n",
    "step = 1 \n",
    "asset_structure = copy.deepcopy(input_asset_structure)\n",
    "asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 graph asset argument를 원하는 값으로 수정합니다. \n",
    "#asset_structure.args['dimension'] = 128\n",
    "asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2084ad",
   "metadata": {},
   "source": [
    "##### Graph asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be198a60-4793-4639-a709-3cb74263d1a4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[2023-11-15 10:38:31,931][ASSET][INFO][train_pipeline][graph]: Successfully got << output path >> for saving your data into csv or jpg file: \n",
      " /home/jovyan/gcr_dev/alo/.train_artifacts/output/graph/ \n",
      " - [NOTE] The names of output file must be fixed as << output.csv, output.jpg >> \u001b[0m\n",
      "\u001b[94m[2023-11-15 10:38:31,934][ASSET][INFO][train_pipeline][graph]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2023-11-15 10:38:31\n",
      "- current step      : graph\n",
      "- asset branch.     : release-1.2\n",
      "- alolib ver.       : 2.0\n",
      "- alo ver.          : release-2.0\n",
      "- load envs. keys   : dict_keys(['project_home', 'pipeline', 'step', 'num_step', 'artifacts', 'alo_version', 'asset_branch', 'interface_mode', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['graph_type', 'center_node_column', 'embedding_column', 'train_inference_column', 'drop_columns', 'dimension', 'num_epochs', 'workers', 'num_partitions', 'extra_columns_for_ml', 'custom_connection'])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "preprocessing blank space...\n",
      "In __init__: pbg ready\n",
      "[2023-11-15 10:38:32.050165] Using the 7 relation types given in the config\n",
      "[2023-11-15 10:38:32.051079] Searching for the entities in the edge files...\n",
      "[2023-11-15 10:38:32.074623] Entity type address:\n",
      "[2023-11-15 10:38:32.075430] - Found 1000 entities\n",
      "[2023-11-15 10:38:32.076169] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-15 10:38:32.077139] - Left with 1000 entities\n",
      "[2023-11-15 10:38:32.077869] - Shuffling them...\n",
      "[2023-11-15 10:38:32.079477] Entity type name:\n",
      "[2023-11-15 10:38:32.080228] - Found 985 entities\n",
      "[2023-11-15 10:38:32.080938] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-15 10:38:32.081997] - Left with 985 entities\n",
      "[2023-11-15 10:38:32.084631] - Shuffling them...\n",
      "[2023-11-15 10:38:32.086501] Entity type age:\n",
      "[2023-11-15 10:38:32.087688] - Found 63 entities\n",
      "[2023-11-15 10:38:32.088483] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-15 10:38:32.089179] - Left with 63 entities\n",
      "[2023-11-15 10:38:32.090007] - Shuffling them...\n",
      "[2023-11-15 10:38:32.090766] Entity type job:\n",
      "[2023-11-15 10:38:32.091556] - Found 37 entities\n",
      "[2023-11-15 10:38:32.092120] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-15 10:38:32.092833] - Left with 37 entities\n",
      "[2023-11-15 10:38:32.093413] - Shuffling them...\n",
      "[2023-11-15 10:38:32.094108] Entity type hobbies:\n",
      "[2023-11-15 10:38:32.094745] - Found 27 entities\n",
      "[2023-11-15 10:38:32.095404] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-15 10:38:32.096023] - Left with 27 entities\n",
      "[2023-11-15 10:38:32.096670] - Shuffling them...\n",
      "[2023-11-15 10:38:32.097304] Entity type orders:\n",
      "[2023-11-15 10:38:32.097972] - Found 21 entities\n",
      "[2023-11-15 10:38:32.098570] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-15 10:38:32.099258] - Left with 21 entities\n",
      "[2023-11-15 10:38:32.099848] - Shuffling them...\n",
      "[2023-11-15 10:38:32.100562] Entity type gender:\n",
      "[2023-11-15 10:38:32.101141] - Found 2 entities\n",
      "[2023-11-15 10:38:32.101802] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-15 10:38:32.102446] - Left with 2 entities\n",
      "[2023-11-15 10:38:32.102971] - Shuffling them...\n",
      "[2023-11-15 10:38:32.103458] Entity type spent:\n",
      "[2023-11-15 10:38:32.103976] - Found 360 entities\n",
      "[2023-11-15 10:38:32.104441] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-15 10:38:32.105063] - Left with 360 entities\n",
      "[2023-11-15 10:38:32.105530] - Shuffling them...\n",
      "[2023-11-15 10:38:32.106499] Preparing counts and dictionaries for entities and relation types:\n",
      "[2023-11-15 10:38:32.110927] - Writing count of entity type address and partition 0\n",
      "[2023-11-15 10:38:32.114506] - Writing count of entity type name and partition 0\n",
      "[2023-11-15 10:38:32.117979] - Writing count of entity type age and partition 0\n",
      "[2023-11-15 10:38:32.120192] - Writing count of entity type job and partition 0\n",
      "[2023-11-15 10:38:32.122091] - Writing count of entity type hobbies and partition 0\n",
      "[2023-11-15 10:38:32.124002] - Writing count of entity type orders and partition 0\n",
      "[2023-11-15 10:38:32.126062] - Writing count of entity type gender and partition 0\n",
      "[2023-11-15 10:38:32.128016] - Writing count of entity type spent and partition 0\n",
      "[2023-11-15 10:38:32.130837] Preparing edge path /home/jovyan/gcr_dev/alo/.train_artifacts/output/graph/partitions/edges_partitioned_rel_3, out of the edges found in /home/jovyan/gcr_dev/alo/.train_artifacts/output/graph/tsvs/rel_3.tsv\n",
      "[2023-11-15 10:38:32.132272] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-11-15 10:38:33.672886] - Processed 999 edges in total\n",
      "[2023-11-15 10:38:33.675033] Preparing edge path /home/jovyan/gcr_dev/alo/.train_artifacts/output/graph/partitions/edges_partitioned_rel_0, out of the edges found in /home/jovyan/gcr_dev/alo/.train_artifacts/output/graph/tsvs/rel_0.tsv\n",
      "[2023-11-15 10:38:33.676963] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-11-15 10:38:35.230287] - Processed 1000 edges in total\n",
      "[2023-11-15 10:38:35.232257] Preparing edge path /home/jovyan/gcr_dev/alo/.train_artifacts/output/graph/partitions/edges_partitioned_rel_5, out of the edges found in /home/jovyan/gcr_dev/alo/.train_artifacts/output/graph/tsvs/rel_5.tsv\n",
      "[2023-11-15 10:38:35.234261] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-11-15 10:38:36.811779] - Processed 1000 edges in total\n",
      "[2023-11-15 10:38:36.813711] Preparing edge path /home/jovyan/gcr_dev/alo/.train_artifacts/output/graph/partitions/edges_partitioned_rel_8, out of the edges found in /home/jovyan/gcr_dev/alo/.train_artifacts/output/graph/tsvs/rel_8.tsv\n",
      "[2023-11-15 10:38:36.815697] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-11-15 10:38:38.367974] - Processed 1000 edges in total\n",
      "[2023-11-15 10:38:38.369942] Preparing edge path /home/jovyan/gcr_dev/alo/.train_artifacts/output/graph/partitions/edges_partitioned_rel_4, out of the edges found in /home/jovyan/gcr_dev/alo/.train_artifacts/output/graph/tsvs/rel_4.tsv\n",
      "[2023-11-15 10:38:38.371951] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-11-15 10:38:39.924190] - Processed 1000 edges in total\n",
      "[2023-11-15 10:38:39.926131] Preparing edge path /home/jovyan/gcr_dev/alo/.train_artifacts/output/graph/partitions/edges_partitioned_rel_7, out of the edges found in /home/jovyan/gcr_dev/alo/.train_artifacts/output/graph/tsvs/rel_7.tsv\n",
      "[2023-11-15 10:38:39.928031] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-11-15 10:38:41.510363] - Processed 985 edges in total\n",
      "[2023-11-15 10:38:41.512330] Preparing edge path /home/jovyan/gcr_dev/alo/.train_artifacts/output/graph/partitions/edges_partitioned_rel_2, out of the edges found in /home/jovyan/gcr_dev/alo/.train_artifacts/output/graph/tsvs/rel_2.tsv\n",
      "[2023-11-15 10:38:41.514318] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-11-15 10:38:43.071811] - Processed 1000 edges in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Embedding Complete]\n",
      "[Embeddig result saved at /home/jovyan/gcr_dev/alo/.train_artifacts/output/graph/RESULT]\n",
      "\u001b[92m[2023-11-15 10:38:49,011][ASSET][INFO][train_pipeline][graph]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr_dev/alo/.train_artifacts/models/graph/\u001b[0m\n",
      "In __del__: pbg deleted\n",
      "\u001b[94m[2023-11-15 10:38:49,045][ASSET][INFO][train_pipeline][graph]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2023-11-15 10:38:49\n",
      "- current step      : graph\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:38:49,047][PROCESS][INFO]: ==================== Finish pipeline: train_pipeline / step: graph\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>is_married</th>\n",
       "      <th>EMB_00</th>\n",
       "      <th>EMB_01</th>\n",
       "      <th>EMB_02</th>\n",
       "      <th>EMB_03</th>\n",
       "      <th>EMB_04</th>\n",
       "      <th>EMB_05</th>\n",
       "      <th>EMB_06</th>\n",
       "      <th>EMB_07</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_54</th>\n",
       "      <th>EMB_55</th>\n",
       "      <th>EMB_56</th>\n",
       "      <th>EMB_57</th>\n",
       "      <th>EMB_58</th>\n",
       "      <th>EMB_59</th>\n",
       "      <th>EMB_60</th>\n",
       "      <th>EMB_61</th>\n",
       "      <th>EMB_62</th>\n",
       "      <th>EMB_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jasmine_Young</td>\n",
       "      <td>False</td>\n",
       "      <td>0.007213</td>\n",
       "      <td>-0.008291</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>0.019367</td>\n",
       "      <td>0.006562</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>-0.008466</td>\n",
       "      <td>-0.008785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008855</td>\n",
       "      <td>0.017830</td>\n",
       "      <td>0.016781</td>\n",
       "      <td>0.001959</td>\n",
       "      <td>0.004641</td>\n",
       "      <td>0.007173</td>\n",
       "      <td>0.018189</td>\n",
       "      <td>-0.015447</td>\n",
       "      <td>0.006227</td>\n",
       "      <td>-0.000144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jeffery_Robinson</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.014511</td>\n",
       "      <td>0.002363</td>\n",
       "      <td>-0.007719</td>\n",
       "      <td>0.008299</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>-0.017137</td>\n",
       "      <td>0.001935</td>\n",
       "      <td>-0.000809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002311</td>\n",
       "      <td>-0.011557</td>\n",
       "      <td>-0.015904</td>\n",
       "      <td>-0.002983</td>\n",
       "      <td>-0.006523</td>\n",
       "      <td>-0.004575</td>\n",
       "      <td>-0.019567</td>\n",
       "      <td>0.004261</td>\n",
       "      <td>0.007740</td>\n",
       "      <td>0.012818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Steven_Sullivan</td>\n",
       "      <td>False</td>\n",
       "      <td>0.002854</td>\n",
       "      <td>-0.001622</td>\n",
       "      <td>0.002289</td>\n",
       "      <td>0.003368</td>\n",
       "      <td>0.006124</td>\n",
       "      <td>0.012722</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>-0.008109</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014884</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>-0.004040</td>\n",
       "      <td>-0.010065</td>\n",
       "      <td>0.004344</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>-0.015204</td>\n",
       "      <td>-0.000611</td>\n",
       "      <td>-0.012606</td>\n",
       "      <td>-0.003830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jay_Williams</td>\n",
       "      <td>False</td>\n",
       "      <td>0.010741</td>\n",
       "      <td>-0.013256</td>\n",
       "      <td>-0.007893</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>0.006228</td>\n",
       "      <td>0.008158</td>\n",
       "      <td>0.003921</td>\n",
       "      <td>0.006279</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004801</td>\n",
       "      <td>0.012620</td>\n",
       "      <td>0.006935</td>\n",
       "      <td>0.004563</td>\n",
       "      <td>0.008849</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.004126</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>0.002276</td>\n",
       "      <td>-0.012924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Benjamin_Beck</td>\n",
       "      <td>True</td>\n",
       "      <td>0.004758</td>\n",
       "      <td>0.007196</td>\n",
       "      <td>-0.003562</td>\n",
       "      <td>0.012136</td>\n",
       "      <td>-0.013484</td>\n",
       "      <td>-0.007808</td>\n",
       "      <td>-0.014584</td>\n",
       "      <td>-0.005335</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009329</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>0.003512</td>\n",
       "      <td>-0.005578</td>\n",
       "      <td>-0.001362</td>\n",
       "      <td>-0.020555</td>\n",
       "      <td>-0.005366</td>\n",
       "      <td>-0.002047</td>\n",
       "      <td>-0.007756</td>\n",
       "      <td>0.009132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gregory_Gomez</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.001550</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>-0.011048</td>\n",
       "      <td>0.008184</td>\n",
       "      <td>0.011128</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>-0.005889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006647</td>\n",
       "      <td>0.009438</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>-0.000443</td>\n",
       "      <td>-0.002673</td>\n",
       "      <td>0.009028</td>\n",
       "      <td>-0.011084</td>\n",
       "      <td>-0.004736</td>\n",
       "      <td>-0.005573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mary_Harris</td>\n",
       "      <td>True</td>\n",
       "      <td>0.004203</td>\n",
       "      <td>0.011715</td>\n",
       "      <td>-0.009070</td>\n",
       "      <td>0.011303</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.018733</td>\n",
       "      <td>-0.024618</td>\n",
       "      <td>-0.004518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011672</td>\n",
       "      <td>-0.008868</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.005001</td>\n",
       "      <td>-0.007693</td>\n",
       "      <td>-0.001968</td>\n",
       "      <td>-0.005505</td>\n",
       "      <td>-0.013968</td>\n",
       "      <td>0.010933</td>\n",
       "      <td>-0.004017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jimmy_Smith</td>\n",
       "      <td>False</td>\n",
       "      <td>0.008707</td>\n",
       "      <td>-0.008234</td>\n",
       "      <td>-0.005993</td>\n",
       "      <td>0.008595</td>\n",
       "      <td>0.011104</td>\n",
       "      <td>0.006216</td>\n",
       "      <td>0.003735</td>\n",
       "      <td>0.008235</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007515</td>\n",
       "      <td>0.007262</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>0.004152</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>-0.004755</td>\n",
       "      <td>-0.013660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kenneth_Rubio</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.000956</td>\n",
       "      <td>0.008619</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.005250</td>\n",
       "      <td>-0.002141</td>\n",
       "      <td>-0.006218</td>\n",
       "      <td>0.015131</td>\n",
       "      <td>-0.019853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.005475</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.014425</td>\n",
       "      <td>-0.006796</td>\n",
       "      <td>-0.005826</td>\n",
       "      <td>0.014773</td>\n",
       "      <td>0.023770</td>\n",
       "      <td>-0.008801</td>\n",
       "      <td>-0.009318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jordan_Simmons</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.015955</td>\n",
       "      <td>0.011642</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>-0.009329</td>\n",
       "      <td>-0.004091</td>\n",
       "      <td>0.002172</td>\n",
       "      <td>0.008689</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019861</td>\n",
       "      <td>-0.003924</td>\n",
       "      <td>-0.016654</td>\n",
       "      <td>-0.007646</td>\n",
       "      <td>0.023083</td>\n",
       "      <td>0.007611</td>\n",
       "      <td>-0.001119</td>\n",
       "      <td>-0.022306</td>\n",
       "      <td>-0.015598</td>\n",
       "      <td>0.005756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               name is_married    EMB_00    EMB_01    EMB_02    EMB_03  \\\n",
       "0     Jasmine_Young      False  0.007213 -0.008291  0.002010  0.019367   \n",
       "1  Jeffery_Robinson       True -0.014511  0.002363 -0.007719  0.008299   \n",
       "2   Steven_Sullivan      False  0.002854 -0.001622  0.002289  0.003368   \n",
       "3      Jay_Williams      False  0.010741 -0.013256 -0.007893  0.008797   \n",
       "4     Benjamin_Beck       True  0.004758  0.007196 -0.003562  0.012136   \n",
       "5     Gregory_Gomez      False  0.000456  0.001550  0.004035 -0.011048   \n",
       "6       Mary_Harris       True  0.004203  0.011715 -0.009070  0.011303   \n",
       "7       Jimmy_Smith      False  0.008707 -0.008234 -0.005993  0.008595   \n",
       "8     Kenneth_Rubio       True -0.000956  0.008619 -0.000025 -0.005250   \n",
       "9    Jordan_Simmons      False -0.015955  0.011642  0.000087 -0.009329   \n",
       "\n",
       "     EMB_04    EMB_05    EMB_06    EMB_07  ...    EMB_54    EMB_55    EMB_56  \\\n",
       "0  0.006562  0.000120 -0.008466 -0.008785  ...  0.008855  0.017830  0.016781   \n",
       "1  0.000370 -0.017137  0.001935 -0.000809  ... -0.002311 -0.011557 -0.015904   \n",
       "2  0.006124  0.012722  0.003609 -0.008109  ... -0.014884  0.020110 -0.004040   \n",
       "3  0.006228  0.008158  0.003921  0.006279  ... -0.004801  0.012620  0.006935   \n",
       "4 -0.013484 -0.007808 -0.014584 -0.005335  ... -0.009329  0.003168  0.003512   \n",
       "5  0.008184  0.011128  0.000683 -0.005889  ...  0.006647  0.009438  0.006800   \n",
       "6  0.002564  0.018733 -0.024618 -0.004518  ...  0.011672 -0.008868  0.000706   \n",
       "7  0.011104  0.006216  0.003735  0.008235  ... -0.007515  0.007262  0.006920   \n",
       "8 -0.002141 -0.006218  0.015131 -0.019853  ...  0.002192  0.005475  0.001773   \n",
       "9 -0.004091  0.002172  0.008689  0.001707  ...  0.019861 -0.003924 -0.016654   \n",
       "\n",
       "     EMB_57    EMB_58    EMB_59    EMB_60    EMB_61    EMB_62    EMB_63  \n",
       "0  0.001959  0.004641  0.007173  0.018189 -0.015447  0.006227 -0.000144  \n",
       "1 -0.002983 -0.006523 -0.004575 -0.019567  0.004261  0.007740  0.012818  \n",
       "2 -0.010065  0.004344  0.005975 -0.015204 -0.000611 -0.012606 -0.003830  \n",
       "3  0.004563  0.008849  0.003670  0.004126  0.001602  0.002276 -0.012924  \n",
       "4 -0.005578 -0.001362 -0.020555 -0.005366 -0.002047 -0.007756  0.009132  \n",
       "5 -0.000200 -0.000443 -0.002673  0.009028 -0.011084 -0.004736 -0.005573  \n",
       "6  0.005001 -0.007693 -0.001968 -0.005505 -0.013968  0.010933 -0.004017  \n",
       "7  0.000579  0.006274  0.004152  0.002790  0.003433 -0.004755 -0.013660  \n",
       "8  0.014425 -0.006796 -0.005826  0.014773  0.023770 -0.008801 -0.009318  \n",
       "9 -0.007646  0.023083  0.007611 -0.001119 -0.022306 -0.015598  0.005756  \n",
       "\n",
       "[10 rows x 66 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asset 실행\n",
    "graph_asset_structure=run(step, pipeline, asset_structure)\n",
    "\n",
    "# graph asset의 결과 dataframe은 graph_asset_structure.data['dataframe']으로 확인할 수 있습니다. \n",
    "graph_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7698dd0-9153-46c0-97d8-6971ab8f3b2c",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### 2. Preprocess asset \n",
    "GCR은 데이터 전처리가 불필요하기 때문에 Preprocess asset의 역할은 크지 않습니다. 다만 사용자가 임베딩 외에 raw data를 학습에 사용하는 경우 (즉, extra_columns_for_ml 설정시) 결측치를 처리하기 위한 용도입니다.\n",
    "#### 주요 Parameter\n",
    "- handling_missing: 결측치 처리 방식을 지정합니다. 'interpolation' 또는 'fill_number' 중에 선택할 수 있으며 GCR에서는 'interpolation'을 권장합니다.\n",
    "- *handling_encoding_y_column: input asset의 y_column과 동일하게 설정합니다. (필수)\n",
    "- handling_encoding_y: y_column의 인코딩 방식을 설정합니다. GCR에서는 'label'로 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91e75b62-dd0c-441a-a8db-eb6d15a4345b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'handling_missing': 'interpolation',\n",
       " 'handling_encoding_y_column': 'is_married',\n",
       " 'handling_encoding_y': 'label',\n",
       " 'handling_scaling_x': 'none',\n",
       " 'load_train_preprocess': False}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - graph(1) - preprocess(2) - sampling(3) - train(4))\n",
    "step = 2 \n",
    "asset_structure = copy.deepcopy(graph_asset_structure)\n",
    "asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 preprocess asset argument를 원하는 값으로 수정합니다. \n",
    "# asset_structure.args['handling_missing'] = dropna\n",
    "asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d78a8-0a32-4f54-aac7-bff907e2ae94",
   "metadata": {},
   "source": [
    "##### Preprocess asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a116e5ee-0eaa-4cad-9e8b-b7452f7068be",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[2023-11-15 10:38:09,757][ASSET][INFO][train_pipeline][preprocess]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr_dev/alo/.train_artifacts/models/preprocess/\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:38:09,759][ASSET][INFO][train_pipeline][preprocess]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2023-11-15 10:38:09\n",
      "- current step      : preprocess\n",
      "- asset branch.     : release-1.2\n",
      "- alolib ver.       : 2.0\n",
      "- alo ver.          : release-2.0\n",
      "- load envs. keys   : dict_keys(['project_home', 'pipeline', 'step', 'num_step', 'artifacts', 'alo_version', 'asset_branch', 'interface_mode', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['handling_missing', 'handling_encoding_y_column', 'handling_encoding_y', 'handling_scaling_x', 'load_train_preprocess'])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "is_married column : label Encoder saved : /home/jovyan/gcr_dev/alo/.train_artifacts/models/preprocess/\n",
      "['EMB_00_nan', 'EMB_01_nan', 'EMB_02_nan', 'EMB_03_nan', 'EMB_04_nan', 'EMB_05_nan', 'EMB_06_nan', 'EMB_07_nan', 'EMB_08_nan', 'EMB_09_nan', 'EMB_10_nan', 'EMB_11_nan', 'EMB_12_nan', 'EMB_13_nan', 'EMB_14_nan', 'EMB_15_nan', 'EMB_16_nan', 'EMB_17_nan', 'EMB_18_nan', 'EMB_19_nan', 'EMB_20_nan', 'EMB_21_nan', 'EMB_22_nan', 'EMB_23_nan', 'EMB_24_nan', 'EMB_25_nan', 'EMB_26_nan', 'EMB_27_nan', 'EMB_28_nan', 'EMB_29_nan', 'EMB_30_nan', 'EMB_31_nan', 'EMB_32_nan', 'EMB_33_nan', 'EMB_34_nan', 'EMB_35_nan', 'EMB_36_nan', 'EMB_37_nan', 'EMB_38_nan', 'EMB_39_nan', 'EMB_40_nan', 'EMB_41_nan', 'EMB_42_nan', 'EMB_43_nan', 'EMB_44_nan', 'EMB_45_nan', 'EMB_46_nan', 'EMB_47_nan', 'EMB_48_nan', 'EMB_49_nan', 'EMB_50_nan', 'EMB_51_nan', 'EMB_52_nan', 'EMB_53_nan', 'EMB_54_nan', 'EMB_55_nan', 'EMB_56_nan', 'EMB_57_nan', 'EMB_58_nan', 'EMB_59_nan', 'EMB_60_nan', 'EMB_61_nan', 'EMB_62_nan', 'EMB_63_nan'] is_married_encoded_nan\n",
      "\u001b[94m[2023-11-15 10:38:09,773][ASSET][INFO][train_pipeline][preprocess]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2023-11-15 10:38:09\n",
      "- current step      : preprocess\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'columns_map', 'preprocess'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:38:09,775][PROCESS][INFO]: ==================== Finish pipeline: train_pipeline / step: preprocess\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_00</th>\n",
       "      <th>EMB_01</th>\n",
       "      <th>EMB_02</th>\n",
       "      <th>EMB_03</th>\n",
       "      <th>EMB_04</th>\n",
       "      <th>EMB_05</th>\n",
       "      <th>EMB_06</th>\n",
       "      <th>EMB_07</th>\n",
       "      <th>EMB_08</th>\n",
       "      <th>EMB_09</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_55_nan</th>\n",
       "      <th>EMB_56_nan</th>\n",
       "      <th>EMB_57_nan</th>\n",
       "      <th>EMB_58_nan</th>\n",
       "      <th>EMB_59_nan</th>\n",
       "      <th>EMB_60_nan</th>\n",
       "      <th>EMB_61_nan</th>\n",
       "      <th>EMB_62_nan</th>\n",
       "      <th>EMB_63_nan</th>\n",
       "      <th>is_married_encoded_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.015890</td>\n",
       "      <td>0.010359</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>-0.006694</td>\n",
       "      <td>0.010317</td>\n",
       "      <td>-0.006505</td>\n",
       "      <td>0.012308</td>\n",
       "      <td>0.008132</td>\n",
       "      <td>0.007925</td>\n",
       "      <td>0.007160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010587</td>\n",
       "      <td>-0.002108</td>\n",
       "      <td>0.015666</td>\n",
       "      <td>-0.016077</td>\n",
       "      <td>-0.021133</td>\n",
       "      <td>-0.007121</td>\n",
       "      <td>0.002302</td>\n",
       "      <td>-0.008441</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008279</td>\n",
       "      <td>-0.018507</td>\n",
       "      <td>-0.007342</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>-0.009598</td>\n",
       "      <td>0.002298</td>\n",
       "      <td>-0.005190</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>-0.004184</td>\n",
       "      <td>-0.002219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005197</td>\n",
       "      <td>-0.001207</td>\n",
       "      <td>0.005212</td>\n",
       "      <td>-0.003690</td>\n",
       "      <td>0.022665</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>-0.001580</td>\n",
       "      <td>-0.004223</td>\n",
       "      <td>0.010824</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015221</td>\n",
       "      <td>-0.012392</td>\n",
       "      <td>-0.010896</td>\n",
       "      <td>-0.009008</td>\n",
       "      <td>-0.010588</td>\n",
       "      <td>-0.011452</td>\n",
       "      <td>0.006627</td>\n",
       "      <td>-0.001878</td>\n",
       "      <td>-0.011367</td>\n",
       "      <td>-0.010615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020656</td>\n",
       "      <td>-0.000387</td>\n",
       "      <td>0.015497</td>\n",
       "      <td>-0.002362</td>\n",
       "      <td>-0.007223</td>\n",
       "      <td>0.009587</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>0.009188</td>\n",
       "      <td>-0.005805</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.010194</td>\n",
       "      <td>-0.006735</td>\n",
       "      <td>-0.017681</td>\n",
       "      <td>-0.003619</td>\n",
       "      <td>0.018746</td>\n",
       "      <td>0.004364</td>\n",
       "      <td>-0.011543</td>\n",
       "      <td>-0.007596</td>\n",
       "      <td>0.013683</td>\n",
       "      <td>-0.000686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004636</td>\n",
       "      <td>0.012836</td>\n",
       "      <td>0.020173</td>\n",
       "      <td>0.005142</td>\n",
       "      <td>-0.000881</td>\n",
       "      <td>-0.007676</td>\n",
       "      <td>-0.016363</td>\n",
       "      <td>0.005906</td>\n",
       "      <td>0.007571</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012226</td>\n",
       "      <td>-0.009684</td>\n",
       "      <td>-0.018724</td>\n",
       "      <td>-0.001956</td>\n",
       "      <td>-0.015494</td>\n",
       "      <td>-0.002130</td>\n",
       "      <td>-0.006006</td>\n",
       "      <td>0.005657</td>\n",
       "      <td>0.011365</td>\n",
       "      <td>0.014810</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001128</td>\n",
       "      <td>-0.015768</td>\n",
       "      <td>0.007924</td>\n",
       "      <td>0.013127</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>-0.017789</td>\n",
       "      <td>0.004032</td>\n",
       "      <td>0.010447</td>\n",
       "      <td>0.018297</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.007849</td>\n",
       "      <td>-0.001687</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.013847</td>\n",
       "      <td>-0.000497</td>\n",
       "      <td>0.005966</td>\n",
       "      <td>-0.006251</td>\n",
       "      <td>0.016704</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.009915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004866</td>\n",
       "      <td>-0.006914</td>\n",
       "      <td>0.003020</td>\n",
       "      <td>-0.007241</td>\n",
       "      <td>-0.012763</td>\n",
       "      <td>0.003972</td>\n",
       "      <td>0.014606</td>\n",
       "      <td>-0.007405</td>\n",
       "      <td>-0.010723</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.008334</td>\n",
       "      <td>-0.008122</td>\n",
       "      <td>-0.006053</td>\n",
       "      <td>-0.006221</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>-0.009049</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>-0.001330</td>\n",
       "      <td>0.003310</td>\n",
       "      <td>-0.020413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023664</td>\n",
       "      <td>0.020635</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.016953</td>\n",
       "      <td>0.018372</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>-0.006554</td>\n",
       "      <td>-0.011326</td>\n",
       "      <td>-0.006034</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.017376</td>\n",
       "      <td>-0.001523</td>\n",
       "      <td>-0.015615</td>\n",
       "      <td>-0.001115</td>\n",
       "      <td>0.015108</td>\n",
       "      <td>-0.000604</td>\n",
       "      <td>-0.013333</td>\n",
       "      <td>-0.013561</td>\n",
       "      <td>0.015115</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005647</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.021941</td>\n",
       "      <td>0.012231</td>\n",
       "      <td>-0.002708</td>\n",
       "      <td>-0.006992</td>\n",
       "      <td>-0.017490</td>\n",
       "      <td>0.005981</td>\n",
       "      <td>0.007980</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.001262</td>\n",
       "      <td>-0.003000</td>\n",
       "      <td>-0.008630</td>\n",
       "      <td>-0.006827</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>-0.011381</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.009943</td>\n",
       "      <td>0.011611</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011726</td>\n",
       "      <td>-0.007660</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>0.004771</td>\n",
       "      <td>-0.008471</td>\n",
       "      <td>-0.003395</td>\n",
       "      <td>-0.002090</td>\n",
       "      <td>0.014159</td>\n",
       "      <td>0.012365</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.010268</td>\n",
       "      <td>-0.013989</td>\n",
       "      <td>0.009881</td>\n",
       "      <td>-0.007917</td>\n",
       "      <td>-0.003422</td>\n",
       "      <td>0.006176</td>\n",
       "      <td>-0.006915</td>\n",
       "      <td>-0.003551</td>\n",
       "      <td>-0.007780</td>\n",
       "      <td>-0.004452</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000581</td>\n",
       "      <td>0.020297</td>\n",
       "      <td>-0.006151</td>\n",
       "      <td>0.017107</td>\n",
       "      <td>0.016024</td>\n",
       "      <td>-0.002685</td>\n",
       "      <td>0.005236</td>\n",
       "      <td>-0.015980</td>\n",
       "      <td>-0.006860</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     EMB_00    EMB_01    EMB_02    EMB_03    EMB_04    EMB_05    EMB_06  \\\n",
       "0 -0.015890  0.010359 -0.000132 -0.006694  0.010317 -0.006505  0.012308   \n",
       "1  0.008279 -0.018507 -0.007342  0.000324 -0.009598  0.002298 -0.005190   \n",
       "2  0.015221 -0.012392 -0.010896 -0.009008 -0.010588 -0.011452  0.006627   \n",
       "3 -0.010194 -0.006735 -0.017681 -0.003619  0.018746  0.004364 -0.011543   \n",
       "4  0.012226 -0.009684 -0.018724 -0.001956 -0.015494 -0.002130 -0.006006   \n",
       "5 -0.007849 -0.001687  0.001183  0.013847 -0.000497  0.005966 -0.006251   \n",
       "6 -0.008334 -0.008122 -0.006053 -0.006221  0.000171 -0.009049  0.002058   \n",
       "7 -0.017376 -0.001523 -0.015615 -0.001115  0.015108 -0.000604 -0.013333   \n",
       "8 -0.001262 -0.003000 -0.008630 -0.006827  0.001260 -0.011381  0.001131   \n",
       "9  0.010268 -0.013989  0.009881 -0.007917 -0.003422  0.006176 -0.006915   \n",
       "\n",
       "     EMB_07    EMB_08    EMB_09  ...  EMB_55_nan  EMB_56_nan  EMB_57_nan  \\\n",
       "0  0.008132  0.007925  0.007160  ...    0.010587   -0.002108    0.015666   \n",
       "1  0.002527 -0.004184 -0.002219  ...    0.005197   -0.001207    0.005212   \n",
       "2 -0.001878 -0.011367 -0.010615  ...    0.020656   -0.000387    0.015497   \n",
       "3 -0.007596  0.013683 -0.000686  ...   -0.004636    0.012836    0.020173   \n",
       "4  0.005657  0.011365  0.014810  ...   -0.001128   -0.015768    0.007924   \n",
       "5  0.016704  0.000538  0.009915  ...    0.004866   -0.006914    0.003020   \n",
       "6 -0.001330  0.003310 -0.020413  ...   -0.023664    0.020635    0.003903   \n",
       "7 -0.013561  0.015115  0.001751  ...   -0.005647    0.001130    0.021941   \n",
       "8  0.001910  0.009943  0.011611  ...   -0.011726   -0.007660    0.004988   \n",
       "9 -0.003551 -0.007780 -0.004452  ...   -0.000581    0.020297   -0.006151   \n",
       "\n",
       "   EMB_58_nan  EMB_59_nan  EMB_60_nan  EMB_61_nan  EMB_62_nan  EMB_63_nan  \\\n",
       "0   -0.016077   -0.021133   -0.007121    0.002302   -0.008441    0.001901   \n",
       "1   -0.003690    0.022665    0.000971   -0.001580   -0.004223    0.010824   \n",
       "2   -0.002362   -0.007223    0.009587    0.002415    0.009188   -0.005805   \n",
       "3    0.005142   -0.000881   -0.007676   -0.016363    0.005906    0.007571   \n",
       "4    0.013127    0.000869   -0.017789    0.004032    0.010447    0.018297   \n",
       "5   -0.007241   -0.012763    0.003972    0.014606   -0.007405   -0.010723   \n",
       "6    0.016953    0.018372    0.000199   -0.006554   -0.011326   -0.006034   \n",
       "7    0.012231   -0.002708   -0.006992   -0.017490    0.005981    0.007980   \n",
       "8    0.004771   -0.008471   -0.003395   -0.002090    0.014159    0.012365   \n",
       "9    0.017107    0.016024   -0.002685    0.005236   -0.015980   -0.006860   \n",
       "\n",
       "   is_married_encoded_nan  \n",
       "0                       0  \n",
       "1                       1  \n",
       "2                       0  \n",
       "3                       0  \n",
       "4                       1  \n",
       "5                       0  \n",
       "6                       1  \n",
       "7                       0  \n",
       "8                       1  \n",
       "9                       0  \n",
       "\n",
       "[10 rows x 131 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asset 실행\n",
    "preprocess_asset_structure=run(step, pipeline, asset_structure)\n",
    "\n",
    "# preprocess asset의 결과 dataframe은 preprocess_asset_structure.data['dataframe']으로 확인할 수 있습니다.  \n",
    "preprocess_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd463df-8ac2-4eeb-a020-49f89a05c5a1",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### 3. Sampling asset  \n",
    "데이터 imbalance가 심한경우엔 sampling이 필요할 수 있습니다. 현재는 undersampling만 제공됩니다.\n",
    "#### 주요 Parameter\n",
    "- sampling_type: sampling이 필요한 경우 설정합니다. 필요없는 경우엔 'none'으로 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a20b2f60-8fb3-4115-9006-8bee0b3c5ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sampling_type': 'none',\n",
       " 'sampling_method': 'negative',\n",
       " 'label_sampling': True,\n",
       " 'ignore_label_class': 1,\n",
       " 'negative_target_class': None,\n",
       " 'label_sampling_num_type': 'compare',\n",
       " 'label_sampling_num': {1: 1, 0: 25},\n",
       " 'sampling_groupkey_columns': None,\n",
       " 'sampling_num_type': None,\n",
       " 'sampling_num': None}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - graph(1) - preprocess(2) - sampling(3) - train(4))\n",
    "step = 3 \n",
    "asset_structure = copy.deepcopy(preprocess_asset_structure)\n",
    "asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 sampling asset argument를 원하는 값으로 수정합니다. \n",
    "# asset_structure.args['sampling_type'] = 'under'\n",
    "asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4230191c-ec22-4e85-9a18-82817a90e2fb",
   "metadata": {},
   "source": [
    "##### Sampling asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "025f4f8c-f430-4c28-b8e0-e27e7fe1a0db",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2023-11-15 11:49:31,387][ASSET][INFO][train_pipeline][sampling]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2023-11-15 11:49:31\n",
      "- current step      : sampling\n",
      "- asset branch.     : release-1.2\n",
      "- alolib ver.       : 2.0\n",
      "- alo ver.          : release-2.0\n",
      "- load envs. keys   : dict_keys(['project_home', 'pipeline', 'step', 'num_step', 'artifacts', 'alo_version', 'asset_branch', 'interface_mode', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['sampling_type', 'sampling_method', 'label_sampling', 'ignore_label_class', 'negative_target_class', 'label_sampling_num_type', 'label_sampling_num', 'sampling_groupkey_columns', 'sampling_num_type', 'sampling_num'])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'columns_map', 'preprocess'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[92m[2023-11-15 11:49:31,391][ASSET][INFO][train_pipeline][sampling]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr_dev/alo/.train_artifacts/models/sampling/\u001b[0m\n",
      "\u001b[94m[2023-11-15 11:49:31,393][ASSET][INFO][train_pipeline][sampling]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2023-11-15 11:49:31\n",
      "- current step      : sampling\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'columns_map', 'preprocess', 'sampling_type'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-15 11:49:31,396][PROCESS][INFO]: ==================== Finish pipeline: train_pipeline / step: sampling\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_00</th>\n",
       "      <th>EMB_01</th>\n",
       "      <th>EMB_02</th>\n",
       "      <th>EMB_03</th>\n",
       "      <th>EMB_04</th>\n",
       "      <th>EMB_05</th>\n",
       "      <th>EMB_06</th>\n",
       "      <th>EMB_07</th>\n",
       "      <th>EMB_08</th>\n",
       "      <th>EMB_09</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_55_nan</th>\n",
       "      <th>EMB_56_nan</th>\n",
       "      <th>EMB_57_nan</th>\n",
       "      <th>EMB_58_nan</th>\n",
       "      <th>EMB_59_nan</th>\n",
       "      <th>EMB_60_nan</th>\n",
       "      <th>EMB_61_nan</th>\n",
       "      <th>EMB_62_nan</th>\n",
       "      <th>EMB_63_nan</th>\n",
       "      <th>is_married_encoded_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.015890</td>\n",
       "      <td>0.010359</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>-0.006694</td>\n",
       "      <td>0.010317</td>\n",
       "      <td>-0.006505</td>\n",
       "      <td>0.012308</td>\n",
       "      <td>0.008132</td>\n",
       "      <td>0.007925</td>\n",
       "      <td>0.007160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010587</td>\n",
       "      <td>-0.002108</td>\n",
       "      <td>0.015666</td>\n",
       "      <td>-0.016077</td>\n",
       "      <td>-0.021133</td>\n",
       "      <td>-0.007121</td>\n",
       "      <td>0.002302</td>\n",
       "      <td>-0.008441</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008279</td>\n",
       "      <td>-0.018507</td>\n",
       "      <td>-0.007342</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>-0.009598</td>\n",
       "      <td>0.002298</td>\n",
       "      <td>-0.005190</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>-0.004184</td>\n",
       "      <td>-0.002219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005197</td>\n",
       "      <td>-0.001207</td>\n",
       "      <td>0.005212</td>\n",
       "      <td>-0.003690</td>\n",
       "      <td>0.022665</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>-0.001580</td>\n",
       "      <td>-0.004223</td>\n",
       "      <td>0.010824</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015221</td>\n",
       "      <td>-0.012392</td>\n",
       "      <td>-0.010896</td>\n",
       "      <td>-0.009008</td>\n",
       "      <td>-0.010588</td>\n",
       "      <td>-0.011452</td>\n",
       "      <td>0.006627</td>\n",
       "      <td>-0.001878</td>\n",
       "      <td>-0.011367</td>\n",
       "      <td>-0.010615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020656</td>\n",
       "      <td>-0.000387</td>\n",
       "      <td>0.015497</td>\n",
       "      <td>-0.002362</td>\n",
       "      <td>-0.007223</td>\n",
       "      <td>0.009587</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>0.009188</td>\n",
       "      <td>-0.005805</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.010194</td>\n",
       "      <td>-0.006735</td>\n",
       "      <td>-0.017681</td>\n",
       "      <td>-0.003619</td>\n",
       "      <td>0.018746</td>\n",
       "      <td>0.004364</td>\n",
       "      <td>-0.011543</td>\n",
       "      <td>-0.007596</td>\n",
       "      <td>0.013683</td>\n",
       "      <td>-0.000686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004636</td>\n",
       "      <td>0.012836</td>\n",
       "      <td>0.020173</td>\n",
       "      <td>0.005142</td>\n",
       "      <td>-0.000881</td>\n",
       "      <td>-0.007676</td>\n",
       "      <td>-0.016363</td>\n",
       "      <td>0.005906</td>\n",
       "      <td>0.007571</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012226</td>\n",
       "      <td>-0.009684</td>\n",
       "      <td>-0.018724</td>\n",
       "      <td>-0.001956</td>\n",
       "      <td>-0.015494</td>\n",
       "      <td>-0.002130</td>\n",
       "      <td>-0.006006</td>\n",
       "      <td>0.005657</td>\n",
       "      <td>0.011365</td>\n",
       "      <td>0.014810</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001128</td>\n",
       "      <td>-0.015768</td>\n",
       "      <td>0.007924</td>\n",
       "      <td>0.013127</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>-0.017789</td>\n",
       "      <td>0.004032</td>\n",
       "      <td>0.010447</td>\n",
       "      <td>0.018297</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.007849</td>\n",
       "      <td>-0.001687</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.013847</td>\n",
       "      <td>-0.000497</td>\n",
       "      <td>0.005966</td>\n",
       "      <td>-0.006251</td>\n",
       "      <td>0.016704</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.009915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004866</td>\n",
       "      <td>-0.006914</td>\n",
       "      <td>0.003020</td>\n",
       "      <td>-0.007241</td>\n",
       "      <td>-0.012763</td>\n",
       "      <td>0.003972</td>\n",
       "      <td>0.014606</td>\n",
       "      <td>-0.007405</td>\n",
       "      <td>-0.010723</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.008334</td>\n",
       "      <td>-0.008122</td>\n",
       "      <td>-0.006053</td>\n",
       "      <td>-0.006221</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>-0.009049</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>-0.001330</td>\n",
       "      <td>0.003310</td>\n",
       "      <td>-0.020413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023664</td>\n",
       "      <td>0.020635</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.016953</td>\n",
       "      <td>0.018372</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>-0.006554</td>\n",
       "      <td>-0.011326</td>\n",
       "      <td>-0.006034</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.017376</td>\n",
       "      <td>-0.001523</td>\n",
       "      <td>-0.015615</td>\n",
       "      <td>-0.001115</td>\n",
       "      <td>0.015108</td>\n",
       "      <td>-0.000604</td>\n",
       "      <td>-0.013333</td>\n",
       "      <td>-0.013561</td>\n",
       "      <td>0.015115</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005647</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.021941</td>\n",
       "      <td>0.012231</td>\n",
       "      <td>-0.002708</td>\n",
       "      <td>-0.006992</td>\n",
       "      <td>-0.017490</td>\n",
       "      <td>0.005981</td>\n",
       "      <td>0.007980</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.001262</td>\n",
       "      <td>-0.003000</td>\n",
       "      <td>-0.008630</td>\n",
       "      <td>-0.006827</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>-0.011381</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.009943</td>\n",
       "      <td>0.011611</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011726</td>\n",
       "      <td>-0.007660</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>0.004771</td>\n",
       "      <td>-0.008471</td>\n",
       "      <td>-0.003395</td>\n",
       "      <td>-0.002090</td>\n",
       "      <td>0.014159</td>\n",
       "      <td>0.012365</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.010268</td>\n",
       "      <td>-0.013989</td>\n",
       "      <td>0.009881</td>\n",
       "      <td>-0.007917</td>\n",
       "      <td>-0.003422</td>\n",
       "      <td>0.006176</td>\n",
       "      <td>-0.006915</td>\n",
       "      <td>-0.003551</td>\n",
       "      <td>-0.007780</td>\n",
       "      <td>-0.004452</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000581</td>\n",
       "      <td>0.020297</td>\n",
       "      <td>-0.006151</td>\n",
       "      <td>0.017107</td>\n",
       "      <td>0.016024</td>\n",
       "      <td>-0.002685</td>\n",
       "      <td>0.005236</td>\n",
       "      <td>-0.015980</td>\n",
       "      <td>-0.006860</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     EMB_00    EMB_01    EMB_02    EMB_03    EMB_04    EMB_05    EMB_06  \\\n",
       "0 -0.015890  0.010359 -0.000132 -0.006694  0.010317 -0.006505  0.012308   \n",
       "1  0.008279 -0.018507 -0.007342  0.000324 -0.009598  0.002298 -0.005190   \n",
       "2  0.015221 -0.012392 -0.010896 -0.009008 -0.010588 -0.011452  0.006627   \n",
       "3 -0.010194 -0.006735 -0.017681 -0.003619  0.018746  0.004364 -0.011543   \n",
       "4  0.012226 -0.009684 -0.018724 -0.001956 -0.015494 -0.002130 -0.006006   \n",
       "5 -0.007849 -0.001687  0.001183  0.013847 -0.000497  0.005966 -0.006251   \n",
       "6 -0.008334 -0.008122 -0.006053 -0.006221  0.000171 -0.009049  0.002058   \n",
       "7 -0.017376 -0.001523 -0.015615 -0.001115  0.015108 -0.000604 -0.013333   \n",
       "8 -0.001262 -0.003000 -0.008630 -0.006827  0.001260 -0.011381  0.001131   \n",
       "9  0.010268 -0.013989  0.009881 -0.007917 -0.003422  0.006176 -0.006915   \n",
       "\n",
       "     EMB_07    EMB_08    EMB_09  ...  EMB_55_nan  EMB_56_nan  EMB_57_nan  \\\n",
       "0  0.008132  0.007925  0.007160  ...    0.010587   -0.002108    0.015666   \n",
       "1  0.002527 -0.004184 -0.002219  ...    0.005197   -0.001207    0.005212   \n",
       "2 -0.001878 -0.011367 -0.010615  ...    0.020656   -0.000387    0.015497   \n",
       "3 -0.007596  0.013683 -0.000686  ...   -0.004636    0.012836    0.020173   \n",
       "4  0.005657  0.011365  0.014810  ...   -0.001128   -0.015768    0.007924   \n",
       "5  0.016704  0.000538  0.009915  ...    0.004866   -0.006914    0.003020   \n",
       "6 -0.001330  0.003310 -0.020413  ...   -0.023664    0.020635    0.003903   \n",
       "7 -0.013561  0.015115  0.001751  ...   -0.005647    0.001130    0.021941   \n",
       "8  0.001910  0.009943  0.011611  ...   -0.011726   -0.007660    0.004988   \n",
       "9 -0.003551 -0.007780 -0.004452  ...   -0.000581    0.020297   -0.006151   \n",
       "\n",
       "   EMB_58_nan  EMB_59_nan  EMB_60_nan  EMB_61_nan  EMB_62_nan  EMB_63_nan  \\\n",
       "0   -0.016077   -0.021133   -0.007121    0.002302   -0.008441    0.001901   \n",
       "1   -0.003690    0.022665    0.000971   -0.001580   -0.004223    0.010824   \n",
       "2   -0.002362   -0.007223    0.009587    0.002415    0.009188   -0.005805   \n",
       "3    0.005142   -0.000881   -0.007676   -0.016363    0.005906    0.007571   \n",
       "4    0.013127    0.000869   -0.017789    0.004032    0.010447    0.018297   \n",
       "5   -0.007241   -0.012763    0.003972    0.014606   -0.007405   -0.010723   \n",
       "6    0.016953    0.018372    0.000199   -0.006554   -0.011326   -0.006034   \n",
       "7    0.012231   -0.002708   -0.006992   -0.017490    0.005981    0.007980   \n",
       "8    0.004771   -0.008471   -0.003395   -0.002090    0.014159    0.012365   \n",
       "9    0.017107    0.016024   -0.002685    0.005236   -0.015980   -0.006860   \n",
       "\n",
       "   is_married_encoded_nan  \n",
       "0                       0  \n",
       "1                       1  \n",
       "2                       0  \n",
       "3                       0  \n",
       "4                       1  \n",
       "5                       0  \n",
       "6                       1  \n",
       "7                       0  \n",
       "8                       1  \n",
       "9                       0  \n",
       "\n",
       "[10 rows x 131 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asset 실행\n",
    "sampling_asset_structure=run(step, pipeline, asset_structure)\n",
    "\n",
    "# sampling asset의 결과 dataframe은 sampling_asset_structure.data['dataframe']으로 확인할 수 있습니다.\n",
    "sampling_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6a3a7-2204-42b5-8cc8-a5b5f740f457",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### 4. Train asset   \n",
    "추출된 임베딩을 활용하여 ML모델을 학습합니다.\n",
    "#### 주요 Parameter\n",
    "- model_type: 목적에 맞는 학습 방식을 선택합니다. (classification/regression)\n",
    "- data_split_method: HPO를 위한 데이터 분할 방식을 선택합니다. (cross_validate/train_test_split)\n",
    "- evaluation_metric: classification의 경우 accuracy, precision, recall, f1-score / regression의 경우 mse, r2, mae, rmse 중 선택합니다.\n",
    "- model_list: lightgbm, random-forest, gbm, Catboost 중 복수 선택 가능합니다.\n",
    "- num_hpo: 설정 범위 내 hpo 횟수를 결정합니다.\n",
    "- param_range: Search 범위를 지정합니다.\n",
    "- shap_ratio: shap value 뽑을 데이터를 sampling 하는 비율을 결정합니다.\n",
    "- evaluation_report: Evaluation Report 출력 여부를 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39f0149f-bfb4-4223-835a-769fd57594bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_type': 'classification',\n",
       " 'data_split_method': 'cross_validate',\n",
       " 'evaluation_metric': 'accuracy',\n",
       " 'model_list': ['lgb', 'rf', 'cb'],\n",
       " 'num_hpo': 3,\n",
       " 'param_range': {'rf': {'max_depth': 6, 'n_estimators': [300, 500]},\n",
       "  'gbm': {'max_depth': [5, 7], 'n_estimators': [300, 500]},\n",
       "  'ngb': {'col_sample': [0.6, 0.8], 'n_estimators': [100, 300]},\n",
       "  'lgb': {'max_depth': [5, 9], 'n_estimators': [300, 500]},\n",
       "  'cb': {'max_depth': [5, 9], 'n_estimators': [100, 500]}},\n",
       " 'shap_ratio': 1.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - graph(1) - preprocess(2) - sampling(3) - train(4))\n",
    "step = 4 \n",
    "asset_structure = copy.deepcopy(sampling_asset_structure)\n",
    "asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 train asset argument를 원하는 값으로 수정합니다.\n",
    "# asset_structure.args['num_hpo'] = 1\n",
    "asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c0b28-561d-4815-b4fc-f2078e1a0f59",
   "metadata": {},
   "source": [
    "##### Train asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c2a5dcd-b0b8-455d-8b68-3483238a5aae",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[2023-11-15 10:14:46,169][ASSET][INFO][train_pipeline][train]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr_dev/alo/.train_artifacts/models/train/\u001b[0m\n",
      "\u001b[92m[2023-11-15 10:14:46,174][ASSET][INFO][train_pipeline][train]: Successfully got << output path >> for saving your data into csv or jpg file: \n",
      " /home/jovyan/gcr_dev/alo/.train_artifacts/output/train/ \n",
      " - [NOTE] The names of output file must be fixed as << output.csv, output.jpg >> \u001b[0m\n",
      "\u001b[94m[2023-11-15 10:14:46,176][ASSET][INFO][train_pipeline][train]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2023-11-15 10:14:46\n",
      "- current step      : train\n",
      "- asset branch.     : tcr_v1.1.2\n",
      "- alolib ver.       : 2.0\n",
      "- alo ver.          : release-2.0\n",
      "- load envs. keys   : dict_keys(['project_home', 'pipeline', 'step', 'num_step', 'artifacts', 'alo_version', 'asset_branch', 'interface_mode', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['model_type', 'data_split_method', 'evaluation_metric', 'model_list', 'num_hpo', 'param_range', 'shap_ratio'])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'columns_map', 'preprocess', 'sampling_type'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[92m[2023-11-15 10:14:46,180][ASSET][INFO][train_pipeline][train]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr_dev/alo/.train_artifacts/models/train/\u001b[0m\n",
      "\u001b[92m[2023-11-15 10:14:46,182][ASSET][INFO][train_pipeline][train]: Successfully got << output path >> for saving your data into csv or jpg file: \n",
      " /home/jovyan/gcr_dev/alo/.train_artifacts/output/train/ \n",
      " - [NOTE] The names of output file must be fixed as << output.csv, output.jpg >> \u001b[0m\n",
      "\u001b[92m[2023-11-15 10:14:46,184][ASSET][INFO][train_pipeline][train]: Successfully got << report path >> for saving your << report.html >> file: \n",
      " /home/jovyan/gcr_dev/alo/.train_artifacts/report/\u001b[0m\n",
      "해당 column 은 Training 과정에 사용되지 않습니다. (column_name: ['EMB_62', 'EMB_43', 'EMB_14', 'EMB_31', 'EMB_38', 'EMB_54', 'EMB_50', 'EMB_60', 'EMB_51', 'EMB_02', 'EMB_26', 'EMB_15', 'EMB_34', 'EMB_56', 'EMB_29', 'EMB_33', 'EMB_24', 'EMB_30', 'EMB_16', 'EMB_49', 'EMB_19', 'is_married_encoded_nan', 'EMB_12', 'EMB_42', 'EMB_36', 'EMB_25', 'EMB_57', 'EMB_48', 'EMB_07', 'EMB_21', 'EMB_37', 'EMB_40', 'EMB_10', 'EMB_53', 'EMB_03', 'EMB_39', 'EMB_28', 'EMB_46', 'EMB_58', 'EMB_32', 'EMB_17', 'EMB_44', 'EMB_52', 'EMB_11', 'EMB_55', 'EMB_09', 'EMB_13', 'EMB_22', 'EMB_23', 'EMB_45', 'is_married_encoded', 'EMB_20', 'EMB_47', 'is_married', 'EMB_61', 'EMB_05', 'EMB_00', 'EMB_04', 'EMB_06', 'EMB_27', 'EMB_08', 'EMB_41', 'EMB_35', 'EMB_01', 'EMB_63', 'EMB_18', 'EMB_59'])\n",
      "[INFO] 모델 학습을 시작합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 0th-fold RandomForestClassifier_set0 모델을 학습합니다.(1/36)\n",
      "[INFO] 1th-fold RandomForestClassifier_set0 모델을 학습합니다.(2/36)\n",
      "[INFO] 2th-fold RandomForestClassifier_set0 모델을 학습합니다.(3/36)\n",
      "[INFO] 3th-fold RandomForestClassifier_set0 모델을 학습합니다.(4/36)\n",
      "[INFO] 0th-fold RandomForestClassifier_set1 모델을 학습합니다.(5/36)\n",
      "[INFO] 1th-fold RandomForestClassifier_set1 모델을 학습합니다.(6/36)\n",
      "[INFO] 2th-fold RandomForestClassifier_set1 모델을 학습합니다.(7/36)\n",
      "[INFO] 3th-fold RandomForestClassifier_set1 모델을 학습합니다.(8/36)\n",
      "[INFO] 0th-fold RandomForestClassifier_set2 모델을 학습합니다.(9/36)\n",
      "[INFO] 1th-fold RandomForestClassifier_set2 모델을 학습합니다.(10/36)\n",
      "[INFO] 2th-fold RandomForestClassifier_set2 모델을 학습합니다.(11/36)\n",
      "[INFO] 3th-fold RandomForestClassifier_set2 모델을 학습합니다.(12/36)\n",
      "[INFO] 0th-fold LGBMClassifier_set0 모델을 학습합니다.(13/36)\n",
      "[INFO] 1th-fold LGBMClassifier_set0 모델을 학습합니다.(14/36)\n",
      "[INFO] 2th-fold LGBMClassifier_set0 모델을 학습합니다.(15/36)\n",
      "[INFO] 3th-fold LGBMClassifier_set0 모델을 학습합니다.(16/36)\n",
      "[INFO] 0th-fold LGBMClassifier_set1 모델을 학습합니다.(17/36)\n",
      "[INFO] 1th-fold LGBMClassifier_set1 모델을 학습합니다.(18/36)\n",
      "[INFO] 2th-fold LGBMClassifier_set1 모델을 학습합니다.(19/36)\n",
      "[INFO] 3th-fold LGBMClassifier_set1 모델을 학습합니다.(20/36)\n",
      "[INFO] 0th-fold LGBMClassifier_set2 모델을 학습합니다.(21/36)\n",
      "[INFO] 1th-fold LGBMClassifier_set2 모델을 학습합니다.(22/36)\n",
      "[INFO] 2th-fold LGBMClassifier_set2 모델을 학습합니다.(23/36)\n",
      "[INFO] 3th-fold LGBMClassifier_set2 모델을 학습합니다.(24/36)\n",
      "[INFO] 0th-fold CatBoostClassifier_set0 모델을 학습합니다.(25/36)\n",
      "[INFO] 1th-fold CatBoostClassifier_set0 모델을 학습합니다.(26/36)\n",
      "[INFO] 2th-fold CatBoostClassifier_set0 모델을 학습합니다.(27/36)\n",
      "[INFO] 3th-fold CatBoostClassifier_set0 모델을 학습합니다.(28/36)\n",
      "[INFO] 0th-fold CatBoostClassifier_set1 모델을 학습합니다.(29/36)\n",
      "[INFO] 1th-fold CatBoostClassifier_set1 모델을 학습합니다.(30/36)\n",
      "[INFO] 2th-fold CatBoostClassifier_set1 모델을 학습합니다.(31/36)\n",
      "[INFO] 3th-fold CatBoostClassifier_set1 모델을 학습합니다.(32/36)\n",
      "[INFO] 0th-fold CatBoostClassifier_set2 모델을 학습합니다.(33/36)\n",
      "[INFO] 1th-fold CatBoostClassifier_set2 모델을 학습합니다.(34/36)\n",
      "[INFO] 2th-fold CatBoostClassifier_set2 모델을 학습합니다.(35/36)\n",
      "[INFO] 3th-fold CatBoostClassifier_set2 모델을 학습합니다.(36/36)\n",
      "@scoring_classification func. - label list: @scoring_classification func. - label list: @scoring_classification func. - label list: @scoring_classification func. - label list: @scoring_classification func. - label list: @scoring_classification func. - label list:  @scoring_classification func. - label list:   @scoring_classification func. - label list:  @scoring_classification func. - label list: {0, 1}  {0, 1}{0, 1} {0, 1} \n",
      "{0, 1} \n",
      "{0, 1}\n",
      "{0, 1}\n",
      "{0, 1}\n",
      "{0, 1}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[INFO] 평가 지표는 ( accuracy ) 를 사용합니다. \n",
      "모델 정보 로그를 저장합니다. (저장위치: /home/jovyan/gcr_dev/alo/.train_artifacts/models/train/model_selection.json)\n",
      "\n",
      "Top 1 model file is saved: /home/jovyan/gcr_dev/alo/.train_artifacts/models/train/best_model_top0.pkl\n",
      "[Score] accuracy: 0.7066\n",
      "[Hyper-parameters] n_estimators: 300, n_jobs: 1, random_state: 1234, max_depth: 6, \n",
      "\n",
      "Top 2 model file is saved: /home/jovyan/gcr_dev/alo/.train_artifacts/models/train/best_model_top1.pkl\n",
      "[Score] accuracy: 0.7066\n",
      "[Hyper-parameters] n_estimators: 500, n_jobs: 1, random_state: 1234, max_depth: 6, \n",
      "\n",
      "Top 3 model file is saved: /home/jovyan/gcr_dev/alo/.train_artifacts/models/train/best_model_top2.pkl\n",
      "[Score] accuracy: 0.7066\n",
      "[Hyper-parameters] n_estimators: 400, n_jobs: 1, random_state: 1234, max_depth: 6, \n",
      "\n",
      "Following model is the best: RandomForestClassifier_set0 / accuracy:0.7066\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[INFO] Summary_plot for Train data 를 저장했습니다.\n",
      "\n",
      "ignore columns와 X로 지정한 데이터 프레임을 합치는 과정중에 에러가 발생했습니다. 확인 부탁드립니다.\n",
      "\u001b[94m[2023-11-15 10:16:24,347][ASSET][INFO][train_pipeline][train]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2023-11-15 10:16:24\n",
      "- current step      : train\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'columns_map', 'preprocess', 'sampling_type', 'feature_dict'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:16:24,350][PROCESS][INFO]: ==================== Finish pipeline: train_pipeline / step: train\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_00_nan</th>\n",
       "      <th>EMB_01_nan</th>\n",
       "      <th>EMB_02_nan</th>\n",
       "      <th>EMB_03_nan</th>\n",
       "      <th>EMB_04_nan</th>\n",
       "      <th>EMB_05_nan</th>\n",
       "      <th>EMB_06_nan</th>\n",
       "      <th>EMB_07_nan</th>\n",
       "      <th>EMB_08_nan</th>\n",
       "      <th>EMB_09_nan</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_62_nan_shapley</th>\n",
       "      <th>EMB_63_nan_shapley</th>\n",
       "      <th>is_married_encoded_nan</th>\n",
       "      <th>pred_is_married_encoded_nan</th>\n",
       "      <th>pred_is_married_encoded_nan_best0</th>\n",
       "      <th>pred_is_married_encoded_nan_best1</th>\n",
       "      <th>pred_is_married_encoded_nan_best2</th>\n",
       "      <th>prob_0</th>\n",
       "      <th>prob_1</th>\n",
       "      <th>train_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013973</td>\n",
       "      <td>-0.001572</td>\n",
       "      <td>0.010948</td>\n",
       "      <td>0.004840</td>\n",
       "      <td>0.017348</td>\n",
       "      <td>-0.020388</td>\n",
       "      <td>-0.032298</td>\n",
       "      <td>-0.005983</td>\n",
       "      <td>0.019362</td>\n",
       "      <td>-0.000380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.771603</td>\n",
       "      <td>0.228397</td>\n",
       "      <td>0th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.020313</td>\n",
       "      <td>0.024985</td>\n",
       "      <td>0.007497</td>\n",
       "      <td>0.014886</td>\n",
       "      <td>-0.003417</td>\n",
       "      <td>-0.020612</td>\n",
       "      <td>-0.022077</td>\n",
       "      <td>-0.005566</td>\n",
       "      <td>0.002318</td>\n",
       "      <td>0.009703</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004207</td>\n",
       "      <td>-0.001410</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.747783</td>\n",
       "      <td>0.252217</td>\n",
       "      <td>2th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010512</td>\n",
       "      <td>0.011726</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>-0.009885</td>\n",
       "      <td>-0.006145</td>\n",
       "      <td>0.002035</td>\n",
       "      <td>0.009448</td>\n",
       "      <td>-0.002692</td>\n",
       "      <td>0.013878</td>\n",
       "      <td>0.023849</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003978</td>\n",
       "      <td>0.006021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.734291</td>\n",
       "      <td>0.265709</td>\n",
       "      <td>1th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.012100</td>\n",
       "      <td>0.005590</td>\n",
       "      <td>-0.003737</td>\n",
       "      <td>-0.002317</td>\n",
       "      <td>-0.001216</td>\n",
       "      <td>-0.000904</td>\n",
       "      <td>0.003089</td>\n",
       "      <td>0.011738</td>\n",
       "      <td>0.006569</td>\n",
       "      <td>-0.017615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001580</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.512179</td>\n",
       "      <td>0.487821</td>\n",
       "      <td>3th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.001300</td>\n",
       "      <td>-0.016159</td>\n",
       "      <td>0.007699</td>\n",
       "      <td>-0.004790</td>\n",
       "      <td>-0.018002</td>\n",
       "      <td>-0.007718</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>-0.005991</td>\n",
       "      <td>-0.013442</td>\n",
       "      <td>-0.006249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.003667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.830293</td>\n",
       "      <td>0.169707</td>\n",
       "      <td>3th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.010009</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>-0.004439</td>\n",
       "      <td>-0.004102</td>\n",
       "      <td>0.017531</td>\n",
       "      <td>-0.017588</td>\n",
       "      <td>0.009399</td>\n",
       "      <td>-0.010795</td>\n",
       "      <td>-0.000925</td>\n",
       "      <td>0.016055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>-0.004946</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.679965</td>\n",
       "      <td>0.320035</td>\n",
       "      <td>3th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.007973</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>-0.015081</td>\n",
       "      <td>0.006139</td>\n",
       "      <td>0.010574</td>\n",
       "      <td>-0.012273</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.009835</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>-0.024582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.003637</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.748791</td>\n",
       "      <td>0.251209</td>\n",
       "      <td>3th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.009722</td>\n",
       "      <td>0.026881</td>\n",
       "      <td>-0.010141</td>\n",
       "      <td>-0.013379</td>\n",
       "      <td>-0.003388</td>\n",
       "      <td>-0.016693</td>\n",
       "      <td>-0.009568</td>\n",
       "      <td>-0.009944</td>\n",
       "      <td>-0.003763</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.693393</td>\n",
       "      <td>0.306607</td>\n",
       "      <td>3th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.004425</td>\n",
       "      <td>-0.015092</td>\n",
       "      <td>-0.001902</td>\n",
       "      <td>0.016308</td>\n",
       "      <td>-0.003122</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>-0.009649</td>\n",
       "      <td>-0.003224</td>\n",
       "      <td>-0.010696</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002423</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.590100</td>\n",
       "      <td>0.409900</td>\n",
       "      <td>3th_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.003163</td>\n",
       "      <td>-0.002716</td>\n",
       "      <td>-0.003907</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>0.025343</td>\n",
       "      <td>0.003111</td>\n",
       "      <td>-0.014874</td>\n",
       "      <td>-0.019971</td>\n",
       "      <td>0.008084</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.001960</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.753959</td>\n",
       "      <td>0.246041</td>\n",
       "      <td>2th_test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 136 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EMB_00_nan  EMB_01_nan  EMB_02_nan  EMB_03_nan  EMB_04_nan  EMB_05_nan  \\\n",
       "0    0.013973   -0.001572    0.010948    0.004840    0.017348   -0.020388   \n",
       "1    0.020313    0.024985    0.007497    0.014886   -0.003417   -0.020612   \n",
       "2    0.010512    0.011726    0.000498   -0.009885   -0.006145    0.002035   \n",
       "3   -0.012100    0.005590   -0.003737   -0.002317   -0.001216   -0.000904   \n",
       "4   -0.001300   -0.016159    0.007699   -0.004790   -0.018002   -0.007718   \n",
       "5   -0.010009    0.002221   -0.004439   -0.004102    0.017531   -0.017588   \n",
       "6    0.007973    0.004696   -0.015081    0.006139    0.010574   -0.012273   \n",
       "7    0.009722    0.026881   -0.010141   -0.013379   -0.003388   -0.016693   \n",
       "8    0.004425   -0.015092   -0.001902    0.016308   -0.003122    0.002001   \n",
       "9   -0.003163   -0.002716   -0.003907    0.004145    0.025343    0.003111   \n",
       "\n",
       "   EMB_06_nan  EMB_07_nan  EMB_08_nan  EMB_09_nan  ...  EMB_62_nan_shapley  \\\n",
       "0   -0.032298   -0.005983    0.019362   -0.000380  ...            0.003099   \n",
       "1   -0.022077   -0.005566    0.002318    0.009703  ...           -0.004207   \n",
       "2    0.009448   -0.002692    0.013878    0.023849  ...           -0.003978   \n",
       "3    0.003089    0.011738    0.006569   -0.017615  ...            0.001580   \n",
       "4    0.001093   -0.005991   -0.013442   -0.006249  ...            0.002130   \n",
       "5    0.009399   -0.010795   -0.000925    0.016055  ...            0.001118   \n",
       "6    0.000237    0.009835    0.003575   -0.024582  ...            0.000330   \n",
       "7   -0.009568   -0.009944   -0.003763   -0.000140  ...            0.000450   \n",
       "8    0.002096   -0.009649   -0.003224   -0.010696  ...           -0.002423   \n",
       "9   -0.014874   -0.019971    0.008084    0.001732  ...            0.002083   \n",
       "\n",
       "   EMB_63_nan_shapley  is_married_encoded_nan  pred_is_married_encoded_nan  \\\n",
       "0            0.002000                       0                            0   \n",
       "1           -0.001410                       0                            0   \n",
       "2            0.006021                       0                            0   \n",
       "3            0.002248                       0                            0   \n",
       "4            0.003667                       0                            0   \n",
       "5           -0.004946                       1                            0   \n",
       "6            0.003637                       1                            0   \n",
       "7            0.001239                       1                            0   \n",
       "8            0.002060                       0                            0   \n",
       "9            0.001960                       0                            0   \n",
       "\n",
       "   pred_is_married_encoded_nan_best0  pred_is_married_encoded_nan_best1  \\\n",
       "0                                  0                                  0   \n",
       "1                                  0                                  0   \n",
       "2                                  0                                  0   \n",
       "3                                  0                                  0   \n",
       "4                                  0                                  0   \n",
       "5                                  0                                  0   \n",
       "6                                  0                                  0   \n",
       "7                                  0                                  0   \n",
       "8                                  0                                  0   \n",
       "9                                  0                                  0   \n",
       "\n",
       "   pred_is_married_encoded_nan_best2    prob_0    prob_1  train_test  \n",
       "0                                  0  0.771603  0.228397    0th_test  \n",
       "1                                  0  0.747783  0.252217    2th_test  \n",
       "2                                  0  0.734291  0.265709    1th_test  \n",
       "3                                  0  0.512179  0.487821    3th_test  \n",
       "4                                  0  0.830293  0.169707    3th_test  \n",
       "5                                  0  0.679965  0.320035    3th_test  \n",
       "6                                  0  0.748791  0.251209    3th_test  \n",
       "7                                  0  0.693393  0.306607    3th_test  \n",
       "8                                  0  0.590100  0.409900    3th_test  \n",
       "9                                  0  0.753959  0.246041    2th_test  \n",
       "\n",
       "[10 rows x 136 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# asset 실행\n",
    "train_asset_structure=run(step, pipeline, asset_structure)\n",
    "\n",
    "# train asset의 결과 dataframe은 train_asset_structure.data['dataframe']으로 확인할 수 있습니다.\n",
    "train_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f82f0-1e6c-4af5-b842-78d428c1e6f3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Inference workflow \n",
    "GCR의 Inference Workflow 구성은 다음과 같습니다.\n",
    "1. Input : 사용자가 지정한 경로로부터 데이터를 Import\n",
    "2. Preprocess : (필요시) 결측치 처리 및 라벨 인코딩\n",
    "3. Inference : Train Workflow에서 선택된 베스트 모델을 활용해 라벨 추론\n",
    "4. Result : 결과 출력\n",
    "\n",
    "아래 코드를 실행하여 Train Workflow에 필요한 라이브러리를 먼저 설치 해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e052e1b-3478-47f8-8c6d-62aa4474b591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2023-11-15 12:18:40,694][PROCESS][INFO]: You did not write any << s3_private_key_file >> in the config yaml file. When you wanna get data from s3 storage, \n",
      "                                 you have to write the s3_private_key_file path or set << ACCESS_KEY, SECRET_KEY >> in your os environment. \n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-15 12:18:40,700][PROCESS][INFO]: Start setting-up << input >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[92m[2023-11-15 12:18:40,703][PROCESS][INFO]: Now << local >> asset_source_code mode: <input> asset exists.\u001b[0m\n",
      "\u001b[94m[2023-11-15 12:18:40,706][PROCESS][INFO]: Start setting-up << preprocess >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[92m[2023-11-15 12:18:40,708][PROCESS][INFO]: Now << local >> asset_source_code mode: <preprocess> asset exists.\u001b[0m\n",
      "\u001b[94m[2023-11-15 12:18:40,710][PROCESS][INFO]: Start setting-up << inference >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-11-15 12:18:40,713][PROCESS][INFO]: << inference >> asset had already been created at 2023-11-15 10:16:25.756357\u001b[0m\n",
      "\u001b[94m[2023-11-15 12:18:40,716][PROCESS][INFO]: Start setting-up << result >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-11-15 12:18:40,719][PROCESS][INFO]: << result >> asset had already been created at 2023-11-15 10:16:25.967359\u001b[0m\n",
      "\u001b[94m[2023-11-15 12:18:40,721][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-15 12:18:40,724][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-15 12:18:40,726][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-15 12:18:40,729][PROCESS][INFO]: ======================================== Start dependency installation : << input >> \u001b[0m\n",
      "\u001b[94m[2023-11-15 12:18:40,732][PROCESS][INFO]: Start checking existence & installing package - pandas==1.5.3 | Progress: ( 1 / 1 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-15 12:18:40,735][PROCESS][INFO]: [OK] << pandas==1.5.3 >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-15 12:18:40,738][PROCESS][INFO]: ======================================== Start dependency installation : << force-reinstall >> \u001b[0m\n",
      "\u001b[94m[2023-11-15 12:18:40,740][PROCESS][INFO]: ======================================== Finish dependency installation \n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 아래는 Inference 시 필요한 라이브러리를 설치하는 코드입니다. library 설치 에러가 발생하면 아래 셀을 재실행 해주세요\n",
    "external_load_data(pipelines[1], alo.external_path, alo.external_path_permission, alo.control['get_external_data'])\n",
    "pipeline = pipelines[1]\n",
    "alo.install_steps(pipeline, alo.control[\"get_asset_source\"])\n",
    "\n",
    "# 초기 data structure 구성\n",
    "envs, args, data, config = {}, {}, {}, {}\n",
    "init_asset_structure = AssetStructure(envs, args, data, config)\n",
    "# logger init\n",
    "alo.set_proc_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f681c8a8-ffde-4f59-a694-1b3849b90dd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 0. Input asset \n",
    "#### 주요 Parameter\n",
    "- *input_path : GCR에서는 추론데이터가 'inference' 위치에 자동 저장됩니다. 따로 설정할 필요 없이 주어진 'inference'로 놓고 사용합니다.\n",
    "- x_columns : Inference workflow에서는 x_column을 따로 지정하지 않습니다. None으로 설정합니다.\n",
    "- *use_all_x : Inference workflow에서는 use_all_x를 True로 놓습니다.\n",
    "- y_column : 추론데이터는 y_column이 없습니다. None으로 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2f43e8e-5a9a-4cbc-b2b5-f548d92b3feb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_path': 'inference',\n",
       " 'x_columns': None,\n",
       " 'use_all_x': True,\n",
       " 'y_column': None,\n",
       " 'groupkey_columns': None,\n",
       " 'drop_columns': None,\n",
       " 'time_column': None,\n",
       " 'concat_dataframes': None,\n",
       " 'encoding': None}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - preprocess(1) - inference(2) - result(3))\n",
    "step = 0 \n",
    "asset_structure = copy.deepcopy(init_asset_structure)\n",
    "asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 input asset argument를 원하는 값으로 수정합니다.\n",
    "# asset_structure.args['x_columns'] = ['']\n",
    "asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705ac75-11a8-4093-9531-3b09de8fc38e",
   "metadata": {},
   "source": [
    "##### Input asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "026f2355-99c7-437f-8274-dbed7aa968d8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-11-15 10:16:25,646][USER][INFO][inference_pipeline][input]: >> Load path : ['/home/jovyan/gcr_dev/alo/input/inference/inference/']\n",
      "[2023-11-15 10:16:25,657][USER][INFO][inference_pipeline][input]: >> The file for batch data has been loaded. (File name: /home/jovyan/gcr_dev/alo/input/inference/inference/inference.csv)\n",
      "[2023-11-15 10:16:25,660][USER][INFO][inference_pipeline][input]: You set the << use_all_x >> as << True >> in the yaml file. So skip checking dataframe columns existence.\n",
      "[2023-11-15 10:16:25,663][USER][INFO][inference_pipeline][input]: ==================== Success loading dataframe ====================\n",
      "[2023-11-15 10:16:25,666][USER][INFO][inference_pipeline][input]: >> Start processing ignore columns & drop columns: ['/home/jovyan/gcr_dev/alo/input/inference/inference/inference.csv']\n",
      "[2023-11-15 10:16:25,670][USER][INFO][inference_pipeline][input]: >> You set the << use_all_x >> parameter as << True >> in your config yaml. (So, these x_columns are used: ['EMB_62', 'EMB_43', 'EMB_14', 'EMB_31', 'EMB_38', 'EMB_54', 'EMB_50', 'EMB_60', 'EMB_51', 'EMB_02', 'EMB_26', 'EMB_15', 'EMB_34', 'EMB_56', 'EMB_29', 'EMB_33', 'EMB_24', 'EMB_30', 'EMB_16', 'EMB_49', 'EMB_19', 'EMB_12', 'EMB_42', 'EMB_36', 'EMB_25', 'EMB_57', 'EMB_48', 'EMB_07', 'EMB_21', 'EMB_37', 'EMB_40', 'EMB_10', 'EMB_53', 'EMB_03', 'EMB_39', 'EMB_28', 'EMB_46', 'EMB_58', 'EMB_32', 'EMB_17', 'EMB_44', 'EMB_52', 'EMB_11', 'EMB_55', 'EMB_09', 'EMB_13', 'EMB_22', 'EMB_23', 'EMB_45', 'EMB_20', 'EMB_47', 'EMB_61', 'EMB_05', 'EMB_00', 'EMB_04', 'EMB_06', 'EMB_27', 'EMB_08', 'EMB_41', 'EMB_35', 'EMB_01', 'EMB_63', 'EMB_18', 'EMB_59'] )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2023-11-15 10:16:25,642][ASSET][INFO][inference_pipeline][input]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2023-11-15 10:16:25\n",
      "- current step      : input\n",
      "- asset branch.     : tabular_2.0\n",
      "- alolib ver.       : 2.0\n",
      "- alo ver.          : release-2.0\n",
      "- load envs. keys   : dict_keys(['project_home', 'pipeline', 'step', 'num_step', 'artifacts', 'alo_version', 'asset_branch', 'interface_mode', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path'])\n",
      "- load args. keys   : dict_keys(['input_path', 'x_columns', 'use_all_x', 'y_column', 'groupkey_columns', 'drop_columns', 'time_column', 'concat_dataframes', 'encoding'])\n",
      "- load config. keys : dict_keys(['meta'])\n",
      "- load data keys    : dict_keys([])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:16:25,671][ASSET][INFO][inference_pipeline][input]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2023-11-15 10:16:25\n",
      "- current step      : input\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:16:25,673][PROCESS][INFO]: ==================== Finish pipeline: inference_pipeline / step: input\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_00</th>\n",
       "      <th>EMB_01</th>\n",
       "      <th>EMB_02</th>\n",
       "      <th>EMB_03</th>\n",
       "      <th>EMB_04</th>\n",
       "      <th>EMB_05</th>\n",
       "      <th>EMB_06</th>\n",
       "      <th>EMB_07</th>\n",
       "      <th>EMB_08</th>\n",
       "      <th>EMB_09</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_54</th>\n",
       "      <th>EMB_55</th>\n",
       "      <th>EMB_56</th>\n",
       "      <th>EMB_57</th>\n",
       "      <th>EMB_58</th>\n",
       "      <th>EMB_59</th>\n",
       "      <th>EMB_60</th>\n",
       "      <th>EMB_61</th>\n",
       "      <th>EMB_62</th>\n",
       "      <th>EMB_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005414</td>\n",
       "      <td>-0.001414</td>\n",
       "      <td>-0.001504</td>\n",
       "      <td>0.012186</td>\n",
       "      <td>0.005871</td>\n",
       "      <td>-0.002538</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>-0.012055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>-0.009254</td>\n",
       "      <td>-0.005421</td>\n",
       "      <td>0.008268</td>\n",
       "      <td>0.011973</td>\n",
       "      <td>-0.006837</td>\n",
       "      <td>0.003715</td>\n",
       "      <td>-0.012994</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.006311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002558</td>\n",
       "      <td>0.012455</td>\n",
       "      <td>-0.002922</td>\n",
       "      <td>0.006427</td>\n",
       "      <td>0.008360</td>\n",
       "      <td>-0.007265</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>-0.008540</td>\n",
       "      <td>-0.007959</td>\n",
       "      <td>0.009944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016557</td>\n",
       "      <td>-0.021171</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.007369</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>-0.003986</td>\n",
       "      <td>0.014472</td>\n",
       "      <td>-0.002204</td>\n",
       "      <td>-0.023196</td>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.027199</td>\n",
       "      <td>-0.002865</td>\n",
       "      <td>-0.004055</td>\n",
       "      <td>-0.003229</td>\n",
       "      <td>0.027128</td>\n",
       "      <td>-0.007764</td>\n",
       "      <td>0.014579</td>\n",
       "      <td>-0.011088</td>\n",
       "      <td>-0.005998</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000122</td>\n",
       "      <td>-0.027237</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.017666</td>\n",
       "      <td>-0.020329</td>\n",
       "      <td>0.010877</td>\n",
       "      <td>0.025670</td>\n",
       "      <td>0.015895</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>-0.008467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.010353</td>\n",
       "      <td>-0.001605</td>\n",
       "      <td>-0.002123</td>\n",
       "      <td>0.004019</td>\n",
       "      <td>0.016011</td>\n",
       "      <td>-0.012714</td>\n",
       "      <td>0.009503</td>\n",
       "      <td>-0.010463</td>\n",
       "      <td>-0.008563</td>\n",
       "      <td>0.019467</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003271</td>\n",
       "      <td>-0.010024</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>0.017236</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>-0.004881</td>\n",
       "      <td>-0.002048</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>-0.006697</td>\n",
       "      <td>0.016579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015729</td>\n",
       "      <td>-0.004807</td>\n",
       "      <td>-0.000182</td>\n",
       "      <td>0.007917</td>\n",
       "      <td>0.017711</td>\n",
       "      <td>0.038704</td>\n",
       "      <td>0.005745</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.010579</td>\n",
       "      <td>0.004112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013575</td>\n",
       "      <td>-0.015005</td>\n",
       "      <td>-0.032511</td>\n",
       "      <td>-0.011125</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.021525</td>\n",
       "      <td>0.001062</td>\n",
       "      <td>-0.008668</td>\n",
       "      <td>0.020516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.017127</td>\n",
       "      <td>0.024546</td>\n",
       "      <td>-0.011373</td>\n",
       "      <td>0.011780</td>\n",
       "      <td>-0.015008</td>\n",
       "      <td>-0.035985</td>\n",
       "      <td>-0.004418</td>\n",
       "      <td>0.003619</td>\n",
       "      <td>0.017380</td>\n",
       "      <td>-0.020313</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012573</td>\n",
       "      <td>-0.017277</td>\n",
       "      <td>0.013202</td>\n",
       "      <td>0.029550</td>\n",
       "      <td>0.014711</td>\n",
       "      <td>0.026671</td>\n",
       "      <td>-0.016550</td>\n",
       "      <td>0.035736</td>\n",
       "      <td>-0.015332</td>\n",
       "      <td>-0.007625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.009031</td>\n",
       "      <td>0.012920</td>\n",
       "      <td>0.003261</td>\n",
       "      <td>-0.009855</td>\n",
       "      <td>0.012314</td>\n",
       "      <td>-0.017714</td>\n",
       "      <td>-0.024314</td>\n",
       "      <td>-0.002422</td>\n",
       "      <td>-0.014871</td>\n",
       "      <td>0.022444</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008524</td>\n",
       "      <td>0.020048</td>\n",
       "      <td>-0.015424</td>\n",
       "      <td>-0.001511</td>\n",
       "      <td>-0.002701</td>\n",
       "      <td>-0.000331</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.017088</td>\n",
       "      <td>-0.010794</td>\n",
       "      <td>0.002108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.047938</td>\n",
       "      <td>0.021478</td>\n",
       "      <td>0.014271</td>\n",
       "      <td>0.017225</td>\n",
       "      <td>-0.005911</td>\n",
       "      <td>-0.009691</td>\n",
       "      <td>-0.023102</td>\n",
       "      <td>0.002689</td>\n",
       "      <td>-0.017652</td>\n",
       "      <td>0.025938</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032887</td>\n",
       "      <td>-0.003750</td>\n",
       "      <td>0.006039</td>\n",
       "      <td>0.002796</td>\n",
       "      <td>-0.025264</td>\n",
       "      <td>0.005336</td>\n",
       "      <td>0.003164</td>\n",
       "      <td>0.012031</td>\n",
       "      <td>-0.016678</td>\n",
       "      <td>0.002014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.010277</td>\n",
       "      <td>-0.012224</td>\n",
       "      <td>0.030582</td>\n",
       "      <td>-0.008301</td>\n",
       "      <td>0.008208</td>\n",
       "      <td>0.017006</td>\n",
       "      <td>0.012763</td>\n",
       "      <td>-0.003090</td>\n",
       "      <td>0.005644</td>\n",
       "      <td>0.022367</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023069</td>\n",
       "      <td>0.015431</td>\n",
       "      <td>-0.010737</td>\n",
       "      <td>0.019173</td>\n",
       "      <td>-0.020052</td>\n",
       "      <td>-0.013143</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>-0.018340</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.027325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.013013</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>-0.006603</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.020575</td>\n",
       "      <td>-0.015420</td>\n",
       "      <td>0.010655</td>\n",
       "      <td>-0.017540</td>\n",
       "      <td>-0.005734</td>\n",
       "      <td>0.015409</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007585</td>\n",
       "      <td>-0.005575</td>\n",
       "      <td>-0.002974</td>\n",
       "      <td>0.009368</td>\n",
       "      <td>0.002496</td>\n",
       "      <td>-0.001832</td>\n",
       "      <td>-0.005721</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>-0.005290</td>\n",
       "      <td>0.014835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     EMB_00    EMB_01    EMB_02    EMB_03    EMB_04    EMB_05    EMB_06  \\\n",
       "0  0.005414 -0.001414 -0.001504  0.012186  0.005871 -0.002538  0.007031   \n",
       "1  0.002558  0.012455 -0.002922  0.006427  0.008360 -0.007265  0.002146   \n",
       "2  0.001795  0.027199 -0.002865 -0.004055 -0.003229  0.027128 -0.007764   \n",
       "3 -0.010353 -0.001605 -0.002123  0.004019  0.016011 -0.012714  0.009503   \n",
       "4  0.015729 -0.004807 -0.000182  0.007917  0.017711  0.038704  0.005745   \n",
       "5 -0.017127  0.024546 -0.011373  0.011780 -0.015008 -0.035985 -0.004418   \n",
       "6  0.009031  0.012920  0.003261 -0.009855  0.012314 -0.017714 -0.024314   \n",
       "7  0.047938  0.021478  0.014271  0.017225 -0.005911 -0.009691 -0.023102   \n",
       "8 -0.010277 -0.012224  0.030582 -0.008301  0.008208  0.017006  0.012763   \n",
       "9 -0.013013  0.000820 -0.006603  0.000225  0.020575 -0.015420  0.010655   \n",
       "\n",
       "     EMB_07    EMB_08    EMB_09  ...    EMB_54    EMB_55    EMB_56    EMB_57  \\\n",
       "0  0.001438  0.004546 -0.012055  ...  0.004200 -0.009254 -0.005421  0.008268   \n",
       "1 -0.008540 -0.007959  0.009944  ...  0.016557 -0.021171  0.001721  0.007369   \n",
       "2  0.014579 -0.011088 -0.005998  ... -0.000122 -0.027237  0.002930  0.017666   \n",
       "3 -0.010463 -0.008563  0.019467  ... -0.003271 -0.010024  0.001952  0.017236   \n",
       "4  0.000413  0.010579  0.004112  ...  0.013575 -0.015005 -0.032511 -0.011125   \n",
       "5  0.003619  0.017380 -0.020313  ... -0.012573 -0.017277  0.013202  0.029550   \n",
       "6 -0.002422 -0.014871  0.022444  ... -0.008524  0.020048 -0.015424 -0.001511   \n",
       "7  0.002689 -0.017652  0.025938  ... -0.032887 -0.003750  0.006039  0.002796   \n",
       "8 -0.003090  0.005644  0.022367  ... -0.023069  0.015431 -0.010737  0.019173   \n",
       "9 -0.017540 -0.005734  0.015409  ... -0.007585 -0.005575 -0.002974  0.009368   \n",
       "\n",
       "     EMB_58    EMB_59    EMB_60    EMB_61    EMB_62    EMB_63  \n",
       "0  0.011973 -0.006837  0.003715 -0.012994  0.000196  0.006311  \n",
       "1  0.000753 -0.003986  0.014472 -0.002204 -0.023196  0.000223  \n",
       "2 -0.020329  0.010877  0.025670  0.015895  0.001505 -0.008467  \n",
       "3  0.000463 -0.004881 -0.002048  0.002250 -0.006697  0.016579  \n",
       "4  0.001102  0.000403  0.021525  0.001062 -0.008668  0.020516  \n",
       "5  0.014711  0.026671 -0.016550  0.035736 -0.015332 -0.007625  \n",
       "6 -0.002701 -0.000331  0.002000  0.017088 -0.010794  0.002108  \n",
       "7 -0.025264  0.005336  0.003164  0.012031 -0.016678  0.002014  \n",
       "8 -0.020052 -0.013143  0.000196 -0.018340  0.000921  0.027325  \n",
       "9  0.002496 -0.001832 -0.005721  0.000963 -0.005290  0.014835  \n",
       "\n",
       "[10 rows x 64 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asset 실행\n",
    "input_asset_structure=run(step, pipeline, asset_structure)\n",
    "\n",
    "# input asset의 결과 dataframe은 input_asset_structure.data['dataframe']으로 확인할 수 있습니다.\n",
    "input_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ddf39d-af5c-48bd-9a96-a3543e030b2c",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### 1. Preprocess asset \n",
    "GCR은 데이터 전처리가 불필요하기 때문에 Preprocess asset의 역할은 크지 않습니다. 다만 사용자가 임베딩 외에 raw data를 학습에 사용하는 경우 (즉, extra_columns_for_ml 설정시) 결측치를 처리하기 위한 용도입니다.\n",
    "#### 주요 Parameter\n",
    "- handling_missing: 결측치 처리 방식을 지정합니다. 'interpolation' 또는 'fill_number' 중에 선택할 수 있으며 GCR에서는 'interpolation'을 권장합니다.\n",
    "- *handling_encoding_y_column: input asset의 y_column과 동일하게 설정합니다. (필수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b486fd42-87de-4c15-aa77-324cb77415b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'handling_missing': 'interpolation',\n",
       " 'handling_encoding_y_column': None,\n",
       " 'limit_encoding_categories': 30,\n",
       " 'load_train_preprocess': True}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - preprocess(1) - inference(2) - result(3))\n",
    "step = 1 \n",
    "asset_structure = copy.deepcopy(input_asset_structure)\n",
    "asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 preprocess asset argument를 원하는 값으로 수정합니다.\n",
    "# asset_structure.args['x_columns'] = ['']\n",
    "asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6a36f8-55c3-4e3e-8879-d8aeb7f9ecac",
   "metadata": {},
   "source": [
    "##### Preprocess asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f690c108-afc0-48e3-9b04-85a4f498c8da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[2023-11-15 10:16:25,710][ASSET][INFO][inference_pipeline][preprocess]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr_dev/alo/.train_artifacts/models/preprocess/\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:16:25,712][ASSET][INFO][inference_pipeline][preprocess]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2023-11-15 10:16:25\n",
      "- current step      : preprocess\n",
      "- asset branch.     : release-1.2\n",
      "- alolib ver.       : 2.0\n",
      "- alo ver.          : release-2.0\n",
      "- load envs. keys   : dict_keys(['project_home', 'pipeline', 'step', 'num_step', 'artifacts', 'alo_version', 'asset_branch', 'interface_mode', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['handling_missing', 'handling_encoding_y_column', 'limit_encoding_categories', 'load_train_preprocess'])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "['EMB_62_nan', 'EMB_43_nan', 'EMB_14_nan', 'EMB_31_nan', 'EMB_38_nan', 'EMB_54_nan', 'EMB_50_nan', 'EMB_60_nan', 'EMB_51_nan', 'EMB_02_nan', 'EMB_26_nan', 'EMB_15_nan', 'EMB_34_nan', 'EMB_56_nan', 'EMB_29_nan', 'EMB_33_nan', 'EMB_24_nan', 'EMB_30_nan', 'EMB_16_nan', 'EMB_49_nan', 'EMB_19_nan', 'EMB_12_nan', 'EMB_42_nan', 'EMB_36_nan', 'EMB_25_nan', 'EMB_57_nan', 'EMB_48_nan', 'EMB_07_nan', 'EMB_21_nan', 'EMB_37_nan', 'EMB_40_nan', 'EMB_10_nan', 'EMB_53_nan', 'EMB_03_nan', 'EMB_39_nan', 'EMB_28_nan', 'EMB_46_nan', 'EMB_58_nan', 'EMB_32_nan', 'EMB_17_nan', 'EMB_44_nan', 'EMB_52_nan', 'EMB_11_nan', 'EMB_55_nan', 'EMB_09_nan', 'EMB_13_nan', 'EMB_22_nan', 'EMB_23_nan', 'EMB_45_nan', 'EMB_20_nan', 'EMB_47_nan', 'EMB_61_nan', 'EMB_05_nan', 'EMB_00_nan', 'EMB_04_nan', 'EMB_06_nan', 'EMB_27_nan', 'EMB_08_nan', 'EMB_41_nan', 'EMB_35_nan', 'EMB_01_nan', 'EMB_63_nan', 'EMB_18_nan', 'EMB_59_nan'] \n",
      "\u001b[94m[2023-11-15 10:16:25,718][ASSET][INFO][inference_pipeline][preprocess]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2023-11-15 10:16:25\n",
      "- current step      : preprocess\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'columns_map', 'preprocess'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:16:25,720][PROCESS][INFO]: ==================== Finish pipeline: inference_pipeline / step: preprocess\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_62</th>\n",
       "      <th>EMB_43</th>\n",
       "      <th>EMB_14</th>\n",
       "      <th>EMB_31</th>\n",
       "      <th>EMB_38</th>\n",
       "      <th>EMB_54</th>\n",
       "      <th>EMB_50</th>\n",
       "      <th>EMB_60</th>\n",
       "      <th>EMB_51</th>\n",
       "      <th>EMB_02</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_04_nan</th>\n",
       "      <th>EMB_06_nan</th>\n",
       "      <th>EMB_27_nan</th>\n",
       "      <th>EMB_08_nan</th>\n",
       "      <th>EMB_41_nan</th>\n",
       "      <th>EMB_35_nan</th>\n",
       "      <th>EMB_01_nan</th>\n",
       "      <th>EMB_63_nan</th>\n",
       "      <th>EMB_18_nan</th>\n",
       "      <th>EMB_59_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.011611</td>\n",
       "      <td>0.010256</td>\n",
       "      <td>-0.002353</td>\n",
       "      <td>-0.006461</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>-0.009528</td>\n",
       "      <td>0.003715</td>\n",
       "      <td>-0.005716</td>\n",
       "      <td>-0.001504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005871</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>-0.018411</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.021116</td>\n",
       "      <td>-0.005868</td>\n",
       "      <td>-0.001414</td>\n",
       "      <td>0.006311</td>\n",
       "      <td>-0.007431</td>\n",
       "      <td>-0.006837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.023196</td>\n",
       "      <td>-0.001093</td>\n",
       "      <td>-0.017425</td>\n",
       "      <td>0.003496</td>\n",
       "      <td>-0.009400</td>\n",
       "      <td>0.016557</td>\n",
       "      <td>0.008686</td>\n",
       "      <td>0.014472</td>\n",
       "      <td>0.006627</td>\n",
       "      <td>-0.002922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008360</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>-0.011488</td>\n",
       "      <td>-0.007959</td>\n",
       "      <td>-0.004343</td>\n",
       "      <td>-0.002364</td>\n",
       "      <td>0.012455</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>-0.002865</td>\n",
       "      <td>-0.003986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.008165</td>\n",
       "      <td>0.012083</td>\n",
       "      <td>-0.012214</td>\n",
       "      <td>0.006523</td>\n",
       "      <td>-0.000122</td>\n",
       "      <td>-0.002292</td>\n",
       "      <td>0.025670</td>\n",
       "      <td>0.010234</td>\n",
       "      <td>-0.002865</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003229</td>\n",
       "      <td>-0.007764</td>\n",
       "      <td>-0.020970</td>\n",
       "      <td>-0.011088</td>\n",
       "      <td>-0.008415</td>\n",
       "      <td>-0.016358</td>\n",
       "      <td>0.027199</td>\n",
       "      <td>-0.008467</td>\n",
       "      <td>-0.018428</td>\n",
       "      <td>0.010877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.006697</td>\n",
       "      <td>-0.020988</td>\n",
       "      <td>-0.009581</td>\n",
       "      <td>-0.001076</td>\n",
       "      <td>-0.012396</td>\n",
       "      <td>-0.003271</td>\n",
       "      <td>0.009289</td>\n",
       "      <td>-0.002048</td>\n",
       "      <td>-0.010167</td>\n",
       "      <td>-0.002123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016011</td>\n",
       "      <td>0.009503</td>\n",
       "      <td>0.008207</td>\n",
       "      <td>-0.008563</td>\n",
       "      <td>0.017410</td>\n",
       "      <td>0.010789</td>\n",
       "      <td>-0.001605</td>\n",
       "      <td>0.016579</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>-0.004881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.008668</td>\n",
       "      <td>-0.013330</td>\n",
       "      <td>0.014066</td>\n",
       "      <td>-0.018598</td>\n",
       "      <td>0.011340</td>\n",
       "      <td>0.013575</td>\n",
       "      <td>0.009422</td>\n",
       "      <td>0.021525</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>-0.000182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017711</td>\n",
       "      <td>0.005745</td>\n",
       "      <td>0.006702</td>\n",
       "      <td>0.010579</td>\n",
       "      <td>-0.031473</td>\n",
       "      <td>0.005961</td>\n",
       "      <td>-0.004807</td>\n",
       "      <td>0.020516</td>\n",
       "      <td>-0.013745</td>\n",
       "      <td>0.000403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.015332</td>\n",
       "      <td>0.018338</td>\n",
       "      <td>0.003890</td>\n",
       "      <td>0.021785</td>\n",
       "      <td>-0.010227</td>\n",
       "      <td>-0.012573</td>\n",
       "      <td>0.006438</td>\n",
       "      <td>-0.016550</td>\n",
       "      <td>-0.025950</td>\n",
       "      <td>-0.011373</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015008</td>\n",
       "      <td>-0.004418</td>\n",
       "      <td>-0.011600</td>\n",
       "      <td>0.017380</td>\n",
       "      <td>-0.007034</td>\n",
       "      <td>0.008936</td>\n",
       "      <td>0.024546</td>\n",
       "      <td>-0.007625</td>\n",
       "      <td>-0.020536</td>\n",
       "      <td>0.026671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.010794</td>\n",
       "      <td>-0.001457</td>\n",
       "      <td>-0.004939</td>\n",
       "      <td>0.016416</td>\n",
       "      <td>0.011705</td>\n",
       "      <td>-0.008524</td>\n",
       "      <td>-0.013296</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.009023</td>\n",
       "      <td>0.003261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012314</td>\n",
       "      <td>-0.024314</td>\n",
       "      <td>-0.012192</td>\n",
       "      <td>-0.014871</td>\n",
       "      <td>-0.004951</td>\n",
       "      <td>-0.003598</td>\n",
       "      <td>0.012920</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>-0.004750</td>\n",
       "      <td>-0.000331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.016678</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>0.021897</td>\n",
       "      <td>-0.014345</td>\n",
       "      <td>-0.032887</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.003164</td>\n",
       "      <td>0.011486</td>\n",
       "      <td>0.014271</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005911</td>\n",
       "      <td>-0.023102</td>\n",
       "      <td>-0.006162</td>\n",
       "      <td>-0.017652</td>\n",
       "      <td>0.003110</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.021478</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>-0.006613</td>\n",
       "      <td>0.005336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.022632</td>\n",
       "      <td>-0.011128</td>\n",
       "      <td>-0.022184</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>-0.023069</td>\n",
       "      <td>-0.028725</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.003828</td>\n",
       "      <td>0.030582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008208</td>\n",
       "      <td>0.012763</td>\n",
       "      <td>-0.024520</td>\n",
       "      <td>0.005644</td>\n",
       "      <td>-0.043829</td>\n",
       "      <td>0.020047</td>\n",
       "      <td>-0.012224</td>\n",
       "      <td>0.027325</td>\n",
       "      <td>0.017989</td>\n",
       "      <td>-0.013143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.005290</td>\n",
       "      <td>-0.018874</td>\n",
       "      <td>-0.008214</td>\n",
       "      <td>-0.006472</td>\n",
       "      <td>-0.008163</td>\n",
       "      <td>-0.007585</td>\n",
       "      <td>0.009757</td>\n",
       "      <td>-0.005721</td>\n",
       "      <td>-0.010411</td>\n",
       "      <td>-0.006603</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020575</td>\n",
       "      <td>0.010655</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>-0.005734</td>\n",
       "      <td>0.016677</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.014835</td>\n",
       "      <td>-0.002253</td>\n",
       "      <td>-0.001832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     EMB_62    EMB_43    EMB_14    EMB_31    EMB_38    EMB_54    EMB_50  \\\n",
       "0  0.000196  0.011611  0.010256 -0.002353 -0.006461  0.004200 -0.009528   \n",
       "1 -0.023196 -0.001093 -0.017425  0.003496 -0.009400  0.016557  0.008686   \n",
       "2  0.001505  0.008165  0.012083 -0.012214  0.006523 -0.000122 -0.002292   \n",
       "3 -0.006697 -0.020988 -0.009581 -0.001076 -0.012396 -0.003271  0.009289   \n",
       "4 -0.008668 -0.013330  0.014066 -0.018598  0.011340  0.013575  0.009422   \n",
       "5 -0.015332  0.018338  0.003890  0.021785 -0.010227 -0.012573  0.006438   \n",
       "6 -0.010794 -0.001457 -0.004939  0.016416  0.011705 -0.008524 -0.013296   \n",
       "7 -0.016678  0.001108  0.014526  0.021897 -0.014345 -0.032887  0.000184   \n",
       "8  0.000921  0.022632 -0.011128 -0.022184  0.005126 -0.023069 -0.028725   \n",
       "9 -0.005290 -0.018874 -0.008214 -0.006472 -0.008163 -0.007585  0.009757   \n",
       "\n",
       "     EMB_60    EMB_51    EMB_02  ...  EMB_04_nan  EMB_06_nan  EMB_27_nan  \\\n",
       "0  0.003715 -0.005716 -0.001504  ...    0.005871    0.007031   -0.018411   \n",
       "1  0.014472  0.006627 -0.002922  ...    0.008360    0.002146   -0.011488   \n",
       "2  0.025670  0.010234 -0.002865  ...   -0.003229   -0.007764   -0.020970   \n",
       "3 -0.002048 -0.010167 -0.002123  ...    0.016011    0.009503    0.008207   \n",
       "4  0.021525  0.022600 -0.000182  ...    0.017711    0.005745    0.006702   \n",
       "5 -0.016550 -0.025950 -0.011373  ...   -0.015008   -0.004418   -0.011600   \n",
       "6  0.002000  0.009023  0.003261  ...    0.012314   -0.024314   -0.012192   \n",
       "7  0.003164  0.011486  0.014271  ...   -0.005911   -0.023102   -0.006162   \n",
       "8  0.000196  0.003828  0.030582  ...    0.008208    0.012763   -0.024520   \n",
       "9 -0.005721 -0.010411 -0.006603  ...    0.020575    0.010655    0.003212   \n",
       "\n",
       "   EMB_08_nan  EMB_41_nan  EMB_35_nan  EMB_01_nan  EMB_63_nan  EMB_18_nan  \\\n",
       "0    0.004546    0.021116   -0.005868   -0.001414    0.006311   -0.007431   \n",
       "1   -0.007959   -0.004343   -0.002364    0.012455    0.000223   -0.002865   \n",
       "2   -0.011088   -0.008415   -0.016358    0.027199   -0.008467   -0.018428   \n",
       "3   -0.008563    0.017410    0.010789   -0.001605    0.016579    0.000986   \n",
       "4    0.010579   -0.031473    0.005961   -0.004807    0.020516   -0.013745   \n",
       "5    0.017380   -0.007034    0.008936    0.024546   -0.007625   -0.020536   \n",
       "6   -0.014871   -0.004951   -0.003598    0.012920    0.002108   -0.004750   \n",
       "7   -0.017652    0.003110    0.000152    0.021478    0.002014   -0.006613   \n",
       "8    0.005644   -0.043829    0.020047   -0.012224    0.027325    0.017989   \n",
       "9   -0.005734    0.016677    0.009014    0.000820    0.014835   -0.002253   \n",
       "\n",
       "   EMB_59_nan  \n",
       "0   -0.006837  \n",
       "1   -0.003986  \n",
       "2    0.010877  \n",
       "3   -0.004881  \n",
       "4    0.000403  \n",
       "5    0.026671  \n",
       "6   -0.000331  \n",
       "7    0.005336  \n",
       "8   -0.013143  \n",
       "9   -0.001832  \n",
       "\n",
       "[10 rows x 128 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asset 실행\n",
    "preprocess_asset_structure=run(step, pipeline, asset_structure)\n",
    "\n",
    "# preprocess asset의 결과 dataframe은 preprocess_asset_structure.data['dataframe']으로 확인할 수 있습니다.  \n",
    "preprocess_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a66a6da-7efd-4c9f-92b4-b91366365db7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2. Inference asset \n",
    "\n",
    "#### 주요 Parameter\n",
    "- model_type: Train workflow의 Train asset과 동일하게 classification/regression 중 설정하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13b21092-9b65-4de1-874e-46e8cbd96075",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_type': 'classification', 'run_shapley': False}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - preprocess(1) - inference(2) - result(3))\n",
    "step = 2 \n",
    "asset_structure = copy.deepcopy(preprocess_asset_structure)\n",
    "asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 inference asset argument를 원하는 값으로 수정합니다.\n",
    "# asset_structure.args['x_columns'] = ['']\n",
    "asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5544d75-1e2f-4009-9e24-7cb91bc18ced",
   "metadata": {},
   "source": [
    "##### inference asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf9857de-2615-41ec-829f-cddb0808aab2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ################################### inference_init (sec):  0.0001595020294189453 ################################### \n",
      "\n",
      "\u001b[94m[2023-11-15 10:16:25,782][ASSET][INFO][inference_pipeline][inference]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2023-11-15 10:16:25\n",
      "- current step      : inference\n",
      "- asset branch.     : tcr_v1.1.2\n",
      "- alolib ver.       : 2.0\n",
      "- alo ver.          : release-2.0\n",
      "- load envs. keys   : dict_keys(['project_home', 'pipeline', 'step', 'num_step', 'artifacts', 'alo_version', 'asset_branch', 'interface_mode', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['model_type', 'run_shapley'])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'columns_map', 'preprocess'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[92m[2023-11-15 10:16:25,785][ASSET][INFO][inference_pipeline][inference]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr_dev/alo/.train_artifacts/models/train/\u001b[0m\n",
      "\u001b[92m[2023-11-15 10:16:25,788][ASSET][INFO][inference_pipeline][inference]: Successfully got << output path >> for saving your data into csv or jpg file: \n",
      " /home/jovyan/gcr_dev/alo/.inference_artifacts/output/inference/ \n",
      " - [NOTE] The names of output file must be fixed as << output.csv, output.jpg >> \u001b[0m\n",
      "해당 column 은 Training 과정에 사용되지 않습니다. (column_name: ['EMB_62', 'EMB_43', 'EMB_14', 'EMB_31', 'EMB_38', 'EMB_54', 'EMB_50', 'EMB_60', 'EMB_51', 'EMB_02', 'EMB_26', 'EMB_15', 'EMB_34', 'EMB_56', 'EMB_29', 'EMB_33', 'EMB_24', 'EMB_30', 'EMB_16', 'EMB_49', 'EMB_19', 'EMB_12', 'EMB_42', 'EMB_36', 'EMB_25', 'EMB_57', 'EMB_48', 'EMB_07', 'EMB_21', 'EMB_37', 'EMB_40', 'EMB_10', 'EMB_53', 'EMB_03', 'EMB_39', 'EMB_28', 'EMB_46', 'EMB_58', 'EMB_32', 'EMB_17', 'EMB_44', 'EMB_52', 'EMB_11', 'EMB_55', 'EMB_09', 'EMB_13', 'EMB_22', 'EMB_23', 'EMB_45', 'EMB_20', 'EMB_47', 'EMB_61', 'EMB_05', 'EMB_00', 'EMB_04', 'EMB_06', 'EMB_27', 'EMB_08', 'EMB_41', 'EMB_35', 'EMB_01', 'EMB_63', 'EMB_18', 'EMB_59'])\n",
      "\u001b[92m[2023-11-15 10:16:25,792][ASSET][INFO][inference_pipeline][inference]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr_dev/alo/.train_artifacts/models/train/\u001b[0m\n",
      "[INFO] XAI 분석 시, 활용할 모델을 로드합니다.\n",
      "모델을 Load 완료 하였습니다. (모델 위치: /home/jovyan/gcr_dev/alo/.train_artifacts/models/train/best_model_top0.pkl)\n",
      "[추론 데이터에 대한 모델 성능을 저장하기 위해 model_performance.json 파일을 생성합니다.\n",
      "\n",
      " ################################### inference_user_run (sec):  0.14433598518371582 ################################### \n",
      "\n",
      "\u001b[94m[2023-11-15 10:16:25,934][ASSET][INFO][inference_pipeline][inference]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2023-11-15 10:16:25\n",
      "- current step      : inference\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'columns_map', 'preprocess'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:16:25,935][PROCESS][INFO]: ==================== Finish pipeline: inference_pipeline / step: inference\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_00</th>\n",
       "      <th>EMB_01</th>\n",
       "      <th>EMB_02</th>\n",
       "      <th>EMB_03</th>\n",
       "      <th>EMB_04</th>\n",
       "      <th>EMB_05</th>\n",
       "      <th>EMB_06</th>\n",
       "      <th>EMB_07</th>\n",
       "      <th>EMB_08</th>\n",
       "      <th>EMB_09</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_59_nan</th>\n",
       "      <th>EMB_60_nan</th>\n",
       "      <th>EMB_61_nan</th>\n",
       "      <th>EMB_62_nan</th>\n",
       "      <th>EMB_63_nan</th>\n",
       "      <th>train_test</th>\n",
       "      <th>pred_</th>\n",
       "      <th>prediction_score</th>\n",
       "      <th>prob_0</th>\n",
       "      <th>prob_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005414</td>\n",
       "      <td>-0.001414</td>\n",
       "      <td>-0.001504</td>\n",
       "      <td>0.012186</td>\n",
       "      <td>0.005871</td>\n",
       "      <td>-0.002538</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>-0.012055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006837</td>\n",
       "      <td>0.003715</td>\n",
       "      <td>-0.012994</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.006311</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6719129985390637, 0.328087001460936]</td>\n",
       "      <td>0.671913</td>\n",
       "      <td>0.328087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002558</td>\n",
       "      <td>0.012455</td>\n",
       "      <td>-0.002922</td>\n",
       "      <td>0.006427</td>\n",
       "      <td>0.008360</td>\n",
       "      <td>-0.007265</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>-0.008540</td>\n",
       "      <td>-0.007959</td>\n",
       "      <td>0.009944</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003986</td>\n",
       "      <td>0.014472</td>\n",
       "      <td>-0.002204</td>\n",
       "      <td>-0.023196</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6739091426774176, 0.32609085732258253]</td>\n",
       "      <td>0.673909</td>\n",
       "      <td>0.326091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.027199</td>\n",
       "      <td>-0.002865</td>\n",
       "      <td>-0.004055</td>\n",
       "      <td>-0.003229</td>\n",
       "      <td>0.027128</td>\n",
       "      <td>-0.007764</td>\n",
       "      <td>0.014579</td>\n",
       "      <td>-0.011088</td>\n",
       "      <td>-0.005998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010877</td>\n",
       "      <td>0.025670</td>\n",
       "      <td>0.015895</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>-0.008467</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.658143786838848, 0.3418562131611526]</td>\n",
       "      <td>0.658144</td>\n",
       "      <td>0.341856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.010353</td>\n",
       "      <td>-0.001605</td>\n",
       "      <td>-0.002123</td>\n",
       "      <td>0.004019</td>\n",
       "      <td>0.016011</td>\n",
       "      <td>-0.012714</td>\n",
       "      <td>0.009503</td>\n",
       "      <td>-0.010463</td>\n",
       "      <td>-0.008563</td>\n",
       "      <td>0.019467</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004881</td>\n",
       "      <td>-0.002048</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>-0.006697</td>\n",
       "      <td>0.016579</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6681796643846966, 0.3318203356153041]</td>\n",
       "      <td>0.668180</td>\n",
       "      <td>0.331820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015729</td>\n",
       "      <td>-0.004807</td>\n",
       "      <td>-0.000182</td>\n",
       "      <td>0.007917</td>\n",
       "      <td>0.017711</td>\n",
       "      <td>0.038704</td>\n",
       "      <td>0.005745</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.010579</td>\n",
       "      <td>0.004112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.021525</td>\n",
       "      <td>0.001062</td>\n",
       "      <td>-0.008668</td>\n",
       "      <td>0.020516</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6655546987596799, 0.33444530124031996]</td>\n",
       "      <td>0.665555</td>\n",
       "      <td>0.334445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.017127</td>\n",
       "      <td>0.024546</td>\n",
       "      <td>-0.011373</td>\n",
       "      <td>0.011780</td>\n",
       "      <td>-0.015008</td>\n",
       "      <td>-0.035985</td>\n",
       "      <td>-0.004418</td>\n",
       "      <td>0.003619</td>\n",
       "      <td>0.017380</td>\n",
       "      <td>-0.020313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026671</td>\n",
       "      <td>-0.016550</td>\n",
       "      <td>0.035736</td>\n",
       "      <td>-0.015332</td>\n",
       "      <td>-0.007625</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.703166737845915, 0.29683326215408584]</td>\n",
       "      <td>0.703167</td>\n",
       "      <td>0.296833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.009031</td>\n",
       "      <td>0.012920</td>\n",
       "      <td>0.003261</td>\n",
       "      <td>-0.009855</td>\n",
       "      <td>0.012314</td>\n",
       "      <td>-0.017714</td>\n",
       "      <td>-0.024314</td>\n",
       "      <td>-0.002422</td>\n",
       "      <td>-0.014871</td>\n",
       "      <td>0.022444</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000331</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.017088</td>\n",
       "      <td>-0.010794</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.634744749008124, 0.36525525099187556]</td>\n",
       "      <td>0.634745</td>\n",
       "      <td>0.365255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.047938</td>\n",
       "      <td>0.021478</td>\n",
       "      <td>0.014271</td>\n",
       "      <td>0.017225</td>\n",
       "      <td>-0.005911</td>\n",
       "      <td>-0.009691</td>\n",
       "      <td>-0.023102</td>\n",
       "      <td>0.002689</td>\n",
       "      <td>-0.017652</td>\n",
       "      <td>0.025938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005336</td>\n",
       "      <td>0.003164</td>\n",
       "      <td>0.012031</td>\n",
       "      <td>-0.016678</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7378133431871043, 0.2621866568128956]</td>\n",
       "      <td>0.737813</td>\n",
       "      <td>0.262187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.010277</td>\n",
       "      <td>-0.012224</td>\n",
       "      <td>0.030582</td>\n",
       "      <td>-0.008301</td>\n",
       "      <td>0.008208</td>\n",
       "      <td>0.017006</td>\n",
       "      <td>0.012763</td>\n",
       "      <td>-0.003090</td>\n",
       "      <td>0.005644</td>\n",
       "      <td>0.022367</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013143</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>-0.018340</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.027325</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6468732843442787, 0.3531267156557213]</td>\n",
       "      <td>0.646873</td>\n",
       "      <td>0.353127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.013013</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>-0.006603</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.020575</td>\n",
       "      <td>-0.015420</td>\n",
       "      <td>0.010655</td>\n",
       "      <td>-0.017540</td>\n",
       "      <td>-0.005734</td>\n",
       "      <td>0.015409</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001832</td>\n",
       "      <td>-0.005721</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>-0.005290</td>\n",
       "      <td>0.014835</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6643872326971653, 0.3356127673028353]</td>\n",
       "      <td>0.664387</td>\n",
       "      <td>0.335613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     EMB_00    EMB_01    EMB_02    EMB_03    EMB_04    EMB_05    EMB_06  \\\n",
       "0  0.005414 -0.001414 -0.001504  0.012186  0.005871 -0.002538  0.007031   \n",
       "1  0.002558  0.012455 -0.002922  0.006427  0.008360 -0.007265  0.002146   \n",
       "2  0.001795  0.027199 -0.002865 -0.004055 -0.003229  0.027128 -0.007764   \n",
       "3 -0.010353 -0.001605 -0.002123  0.004019  0.016011 -0.012714  0.009503   \n",
       "4  0.015729 -0.004807 -0.000182  0.007917  0.017711  0.038704  0.005745   \n",
       "5 -0.017127  0.024546 -0.011373  0.011780 -0.015008 -0.035985 -0.004418   \n",
       "6  0.009031  0.012920  0.003261 -0.009855  0.012314 -0.017714 -0.024314   \n",
       "7  0.047938  0.021478  0.014271  0.017225 -0.005911 -0.009691 -0.023102   \n",
       "8 -0.010277 -0.012224  0.030582 -0.008301  0.008208  0.017006  0.012763   \n",
       "9 -0.013013  0.000820 -0.006603  0.000225  0.020575 -0.015420  0.010655   \n",
       "\n",
       "     EMB_07    EMB_08    EMB_09  ...  EMB_59_nan  EMB_60_nan  EMB_61_nan  \\\n",
       "0  0.001438  0.004546 -0.012055  ...   -0.006837    0.003715   -0.012994   \n",
       "1 -0.008540 -0.007959  0.009944  ...   -0.003986    0.014472   -0.002204   \n",
       "2  0.014579 -0.011088 -0.005998  ...    0.010877    0.025670    0.015895   \n",
       "3 -0.010463 -0.008563  0.019467  ...   -0.004881   -0.002048    0.002250   \n",
       "4  0.000413  0.010579  0.004112  ...    0.000403    0.021525    0.001062   \n",
       "5  0.003619  0.017380 -0.020313  ...    0.026671   -0.016550    0.035736   \n",
       "6 -0.002422 -0.014871  0.022444  ...   -0.000331    0.002000    0.017088   \n",
       "7  0.002689 -0.017652  0.025938  ...    0.005336    0.003164    0.012031   \n",
       "8 -0.003090  0.005644  0.022367  ...   -0.013143    0.000196   -0.018340   \n",
       "9 -0.017540 -0.005734  0.015409  ...   -0.001832   -0.005721    0.000963   \n",
       "\n",
       "   EMB_62_nan  EMB_63_nan  train_test  pred_  \\\n",
       "0    0.000196    0.006311        test      0   \n",
       "1   -0.023196    0.000223        test      0   \n",
       "2    0.001505   -0.008467        test      0   \n",
       "3   -0.006697    0.016579        test      0   \n",
       "4   -0.008668    0.020516        test      0   \n",
       "5   -0.015332   -0.007625        test      0   \n",
       "6   -0.010794    0.002108        test      0   \n",
       "7   -0.016678    0.002014        test      0   \n",
       "8    0.000921    0.027325        test      0   \n",
       "9   -0.005290    0.014835        test      0   \n",
       "\n",
       "                            prediction_score    prob_0    prob_1  \n",
       "0    [0.6719129985390637, 0.328087001460936]  0.671913  0.328087  \n",
       "1  [0.6739091426774176, 0.32609085732258253]  0.673909  0.326091  \n",
       "2    [0.658143786838848, 0.3418562131611526]  0.658144  0.341856  \n",
       "3   [0.6681796643846966, 0.3318203356153041]  0.668180  0.331820  \n",
       "4  [0.6655546987596799, 0.33444530124031996]  0.665555  0.334445  \n",
       "5   [0.703166737845915, 0.29683326215408584]  0.703167  0.296833  \n",
       "6   [0.634744749008124, 0.36525525099187556]  0.634745  0.365255  \n",
       "7   [0.7378133431871043, 0.2621866568128956]  0.737813  0.262187  \n",
       "8   [0.6468732843442787, 0.3531267156557213]  0.646873  0.353127  \n",
       "9   [0.6643872326971653, 0.3356127673028353]  0.664387  0.335613  \n",
       "\n",
       "[10 rows x 133 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asset 실행\n",
    "inference_asset_structure=run(step, pipeline, asset_structure)\n",
    "\n",
    "# inference asset의 결과 dataframe은 inference_asset_structure.data['dataframe']으로 확인할 수 있습니다.  \n",
    "inference_asset_structure.data['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93e1b561-2352-4474-ab09-c7ae583a7f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['EMB_00', 'EMB_01', 'EMB_02', 'EMB_03', 'EMB_04', 'EMB_05', 'EMB_06',\n",
       "       'EMB_07', 'EMB_08', 'EMB_09',\n",
       "       ...\n",
       "       'EMB_59_nan', 'EMB_60_nan', 'EMB_61_nan', 'EMB_62_nan', 'EMB_63_nan',\n",
       "       'train_test', 'pred_', 'prediction_score', 'prob_0', 'prob_1'],\n",
       "      dtype='object', length=133)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_asset_structure.data['dataframe'].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e749a64d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3. Result asset \n",
    "\n",
    "#### 주요 Parameter\n",
    "- result_save_name: 결과 저장 파일명을 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17905521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result_save_name': None}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - preprocess(1) - inference(2) - result(3))\n",
    "step = 3\n",
    "asset_structure = copy.deepcopy(inference_asset_structure)\n",
    "asset_structure.args = alo.get_args(pipeline, step)\n",
    "\n",
    "# 아래 주석을 풀어 result asset argument를 원하는 값으로 수정합니다.\n",
    "# asset_structure.args['x_columns'] = ['']\n",
    "asset_structure.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c2f5c",
   "metadata": {},
   "source": [
    "##### result asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c899d49e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2023-11-15 10:16:25,978][ASSET][INFO][inference_pipeline][result]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2023-11-15 10:16:25\n",
      "- current step      : result\n",
      "- asset branch.     : release-1.2\n",
      "- alolib ver.       : 2.0\n",
      "- alo ver.          : release-2.0\n",
      "- load envs. keys   : dict_keys(['project_home', 'pipeline', 'step', 'num_step', 'artifacts', 'alo_version', 'asset_branch', 'interface_mode', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['result_save_name'])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'columns_map', 'preprocess'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "Loading Embeddings\n",
      "\u001b[92m[2023-11-15 10:16:25,980][ASSET][INFO][inference_pipeline][result]: Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr_dev/alo/.train_artifacts/models/result/\u001b[0m\n",
      "Merging data\n",
      "\u001b[92m[2023-11-15 10:16:26,005][ASSET][INFO][inference_pipeline][result]: Successfully got << output path >> for saving your data into csv or jpg file: \n",
      " /home/jovyan/gcr_dev/alo/.inference_artifacts/output/result/ \n",
      " - [NOTE] The names of output file must be fixed as << output.csv, output.jpg >> \u001b[0m\n",
      "\u001b[92m[2023-11-15 10:16:26,014][ASSET][INFO][inference_pipeline][result]: Successfully got << output path >> for saving your data into csv or jpg file: \n",
      " /home/jovyan/gcr_dev/alo/.inference_artifacts/output/result/ \n",
      " - [NOTE] The names of output file must be fixed as << output.csv, output.jpg >> \u001b[0m\n",
      "Check Result at /home/jovyan/gcr_dev/alo/.inference_artifacts/output/result/inference_result.csv\n",
      "\u001b[94m[2023-11-15 10:16:26,015][ASSET][INFO][inference_pipeline][result]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2023-11-15 10:16:26\n",
      "- current step      : result\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'columns_map', 'preprocess'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-15 10:16:26,017][PROCESS][INFO]: ==================== Finish pipeline: inference_pipeline / step: result\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>is_married</th>\n",
       "      <th>train_test</th>\n",
       "      <th>pred_</th>\n",
       "      <th>prediction_score</th>\n",
       "      <th>prob_0</th>\n",
       "      <th>prob_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gregory_Hull</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6719129985390637, 0.328087001460936]</td>\n",
       "      <td>0.671913</td>\n",
       "      <td>0.328087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allison_Peterson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6739091426774176, 0.32609085732258253]</td>\n",
       "      <td>0.673909</td>\n",
       "      <td>0.326091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Daniel_Davies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.658143786838848, 0.3418562131611526]</td>\n",
       "      <td>0.658144</td>\n",
       "      <td>0.341856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alison_Fox</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6681796643846966, 0.3318203356153041]</td>\n",
       "      <td>0.668180</td>\n",
       "      <td>0.331820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Daniel_Moore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6655546987596799, 0.33444530124031996]</td>\n",
       "      <td>0.665555</td>\n",
       "      <td>0.334445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Barbara_Smith</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.703166737845915, 0.29683326215408584]</td>\n",
       "      <td>0.703167</td>\n",
       "      <td>0.296833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Paul_Terry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.634744749008124, 0.36525525099187556]</td>\n",
       "      <td>0.634745</td>\n",
       "      <td>0.365255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Christina_Salas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7378133431871043, 0.2621866568128956]</td>\n",
       "      <td>0.737813</td>\n",
       "      <td>0.262187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jose_Boyd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6468732843442787, 0.3531267156557213]</td>\n",
       "      <td>0.646873</td>\n",
       "      <td>0.353127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Zachary_Fowler</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6643872326971653, 0.3356127673028353]</td>\n",
       "      <td>0.664387</td>\n",
       "      <td>0.335613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name  is_married train_test  pred_  \\\n",
       "0      Gregory_Hull         NaN       test      0   \n",
       "1  Allison_Peterson         NaN       test      0   \n",
       "2     Daniel_Davies         NaN       test      0   \n",
       "3        Alison_Fox         NaN       test      0   \n",
       "4      Daniel_Moore         NaN       test      0   \n",
       "5     Barbara_Smith         NaN       test      0   \n",
       "6        Paul_Terry         NaN       test      0   \n",
       "7   Christina_Salas         NaN       test      0   \n",
       "8         Jose_Boyd         NaN       test      0   \n",
       "9    Zachary_Fowler         NaN       test      0   \n",
       "\n",
       "                            prediction_score    prob_0    prob_1  \n",
       "0    [0.6719129985390637, 0.328087001460936]  0.671913  0.328087  \n",
       "1  [0.6739091426774176, 0.32609085732258253]  0.673909  0.326091  \n",
       "2    [0.658143786838848, 0.3418562131611526]  0.658144  0.341856  \n",
       "3   [0.6681796643846966, 0.3318203356153041]  0.668180  0.331820  \n",
       "4  [0.6655546987596799, 0.33444530124031996]  0.665555  0.334445  \n",
       "5   [0.703166737845915, 0.29683326215408584]  0.703167  0.296833  \n",
       "6   [0.634744749008124, 0.36525525099187556]  0.634745  0.365255  \n",
       "7   [0.7378133431871043, 0.2621866568128956]  0.737813  0.262187  \n",
       "8   [0.6468732843442787, 0.3531267156557213]  0.646873  0.353127  \n",
       "9   [0.6643872326971653, 0.3356127673028353]  0.664387  0.335613  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asset 실행\n",
    "result_asset_structure=run(step, pipeline, asset_structure)\n",
    "\n",
    "# result asset의 결과 dataframe은 result_asset_structure.data['dataframe']으로 확인할 수 있습니다.\n",
    "result_asset_structure.data['dataframe'].head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcr",
   "language": "python",
   "name": "gcr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
