{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "362b6b5c-8668-46b6-b5d8-00888c5580fa",
   "metadata": {},
   "source": [
    "아래는 ALO 기본 설정 및 라이브러리 설치 코드입니다. library 설치 에러가 발생하면 아래 셀을 재실행 하고, 지속적으로 문제가 있을 시 문의바랍니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "137cd365-9ed9-4941-aa2e-67fca06e1a4f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m======================================== Start dependency installation : << master >> \u001b[0m\n",
      "\u001b[93m>> Start checking existence & installing package - alolib@git+http://mod.lge.com/hub/dxadvtech/aicontents-framework/alolib-source.git@release-1.1 | Progress: ( 1 / 1 total packages )\u001b[0m\n",
      "\u001b[92m- << alolib@git+http://mod.lge.com/hub/dxadvtech/aicontents-framework/alolib-source.git@release-1.1 >> already exists\u001b[0m\n",
      "\u001b[94m======================================== Start dependency installation : << force-reinstall >> \u001b[0m\n",
      "\u001b[94m======================================== Finish dependency installation \n",
      "\u001b[0m\n",
      "STTIME does not exist. instead get current time\n",
      "\u001b[92m>> << /home/jovyan/gcr/alo/assets/ >> directory already exists.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    " import argparse\n",
    "import time\n",
    "import os\n",
    "os.chdir(os.path.abspath(os.path.join('./alo')))\n",
    "from src.alo import ALO\n",
    "alo = ALO(); alo.preset(); pipelines = list(alo.asset_source.keys())\n",
    "from src.external import external_load_data, external_save_artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171f4e9-d318-47ec-8374-f49fb6b30356",
   "metadata": {},
   "source": [
    "## Train Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b7d0f4d-05da-40d0-9b5d-75c496b5794a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m[INFO] You did not write any << s3_private_key_file >> in the config yaml file. When you wanna get data from s3 storage, \n",
      " you have to write the s3_private_key_file path or set << ACCESS_KEY, SECRET_KEY >> in your os environment. \n",
      "\u001b[0m\n",
      "\u001b[93m[INFO] Skip loading external data. << /nas001/users/seongwoo.kong/gcr_test_data/sample/ >> \n",
      " << sample >> already exists in << /home/jovyan/gcr/alo/input/ >>. \n",
      " & << get_external_data >> is set as << once >>. \n",
      "\u001b[0m\n",
      "\u001b[94m>> Start setting-up << input >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[93m[NOTICE] << input >> asset had already been created at 2023-10-26 05:47:41.591910\u001b[0m\n",
      "\u001b[94m>> Start setting-up << graph >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[93m[NOTICE] << graph >> asset had already been created at 2023-10-26 05:47:42.037913\u001b[0m\n",
      "\u001b[94m>> Start setting-up << preprocess >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[93m[NOTICE] << preprocess >> asset had already been created at 2023-10-26 05:47:42.783919\u001b[0m\n",
      "\u001b[94m>> Start setting-up << sampling >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[93m[NOTICE] << sampling >> asset had already been created at 2023-10-26 05:47:43.471924\u001b[0m\n",
      "\u001b[94m>> Start setting-up << train >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[93m[NOTICE] << train >> asset had already been created at 2023-10-26 05:47:44.011927\u001b[0m\n",
      "\u001b[93m>> Ignored installing << torch==2.0.0 >>. Another version will be installed in the previous step.\u001b[0m\n",
      "\u001b[93m>> Ignored installing << pandas==1.5.3 >>. Another version will be installed in the previous step.\u001b[0m\n",
      "\u001b[93m>> Ignored installing << pandas==1.5.3 >>. Another version will be installed in the previous step.\u001b[0m\n",
      "\u001b[93m>> Ignored installing << numpy==1.25.2 >>. Another version will be installed in the previous step.\u001b[0m\n",
      "\u001b[93m>> Ignored installing << pandas==1.5.3 >>. Another version will be installed in the previous step.\u001b[0m\n",
      "\u001b[93m>> Ignored installing << scikit-learn >>. Another version will be installed in the previous step.\u001b[0m\n",
      "\u001b[93m>> Ignored installing << matplotlib >>. Another version will be installed in the previous step.\u001b[0m\n",
      "\u001b[94m======================================== Start dependency installation : << input >> \u001b[0m\n",
      "\u001b[93m>> Start checking existence & installing package - pandas==1.5.3 | Progress: ( 1 / 14 total packages )\u001b[0m\n",
      "\u001b[92m- << pandas==1.5.3 >> already exists\u001b[0m\n",
      "\u001b[94m======================================== Start dependency installation : << graph >> \u001b[0m\n",
      "\u001b[93m>> Start checking existence & installing package - torch==2.0.0 | Progress: ( 2 / 14 total packages )\u001b[0m\n",
      "\u001b[92m- << torch==2.0.0 >> already exists\u001b[0m\n",
      "\u001b[93m>> Start checking existence & installing package - torchbiggraph@git+https://github.com/facebookresearch/PyTorch-BigGraph.git | Progress: ( 3 / 14 total packages )\u001b[0m\n",
      "\u001b[92m- << torchbiggraph@git+https://github.com/facebookresearch/PyTorch-BigGraph.git >> already exists\u001b[0m\n",
      "\u001b[94m======================================== Start dependency installation : << preprocess >> \u001b[0m\n",
      "\u001b[93m>> Start checking existence & installing package - category_encoders | Progress: ( 4 / 14 total packages )\u001b[0m\n",
      "\u001b[92m- << category_encoders >> already exists\u001b[0m\n",
      "\u001b[94m======================================== Start dependency installation : << sampling >> \u001b[0m\n",
      "\u001b[93m>> Start checking existence & installing package - numpy==1.25.2 | Progress: ( 5 / 14 total packages )\u001b[0m\n",
      "\u001b[92m- << numpy==1.25.2 >> already exists\u001b[0m\n",
      "\u001b[93m>> Start checking existence & installing package - scikit-learn | Progress: ( 6 / 14 total packages )\u001b[0m\n",
      "\u001b[92m- << scikit-learn >> already exists\u001b[0m\n",
      "\u001b[93m>> Start checking existence & installing package - umap-learn | Progress: ( 7 / 14 total packages )\u001b[0m\n",
      "\u001b[92m- << umap-learn >> already exists\u001b[0m\n",
      "\u001b[93m>> Start checking existence & installing package - matplotlib | Progress: ( 8 / 14 total packages )\u001b[0m\n",
      "\u001b[92m- << matplotlib >> already exists\u001b[0m\n",
      "\u001b[94m======================================== Start dependency installation : << train >> \u001b[0m\n",
      "\u001b[93m>> Start checking existence & installing package - seaborn | Progress: ( 9 / 14 total packages )\u001b[0m\n",
      "\u001b[92m- << seaborn >> already exists\u001b[0m\n",
      "\u001b[93m>> Start checking existence & installing package - shap | Progress: ( 10 / 14 total packages )\u001b[0m\n",
      "\u001b[92m- << shap >> already exists\u001b[0m\n",
      "\u001b[93m>> Start checking existence & installing package - lightgbm | Progress: ( 11 / 14 total packages )\u001b[0m\n",
      "\u001b[92m- << lightgbm >> already exists\u001b[0m\n",
      "\u001b[93m>> Start checking existence & installing package - catboost | Progress: ( 12 / 14 total packages )\u001b[0m\n",
      "\u001b[92m- << catboost >> already exists\u001b[0m\n",
      "\u001b[93m>> Start checking existence & installing package - ngboost | Progress: ( 13 / 14 total packages )\u001b[0m\n",
      "\u001b[92m- << ngboost >> already exists\u001b[0m\n",
      "\u001b[94m======================================== Start dependency installation : << force-reinstall >> \u001b[0m\n",
      "\u001b[93m>> Start checking existence & installing package - numpy==1.25.2 --force-reinstall | Progress: ( 14 / 14 total packages )\u001b[0m\n",
      "\u001b[93m- Start installing package - numpy==1.25.2 --force-reinstall\u001b[0m\n",
      "Collecting numpy==1.25.2\n",
      "  Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.2\n",
      "    Uninstalling numpy-1.25.2:\n",
      "      Successfully uninstalled numpy-1.25.2\n",
      "Successfully installed numpy-1.25.2\n",
      "\u001b[94m======================================== Finish dependency installation \n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 아래는 Train 시 필요한 라이브러리를 설치하는 코드입니다. library 설치 에러가 발생하면 아래 셀을 재실행 해주세요\n",
    "external_load_data(pipelines[0], alo.external_path, alo.external_path_permission, alo.control['get_external_data'])\n",
    "pipeline = pipelines[0]\n",
    "alo.install_steps(pipeline, alo.control[\"get_asset_source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9f36d6-84e0-440e-80d3-b462254b006a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train workflow \n",
    "### 0. Input asset \n",
    "##### Input asset의 arguments 수정 및 확인\n",
    "- 필요한경우 input_args의 항목을 ***input_args[argument명]=value입력*** 을 통해 변경할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90e2c80e-0a14-4e19-b50f-2aa621aec2da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_path': 'sample',\n",
       " 'x_columns': None,\n",
       " 'use_all_x': True,\n",
       " 'y_column': 'is_married',\n",
       " 'groupkey_columns': None,\n",
       " 'drop_columns': None,\n",
       " 'time_column': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pipelines = 'train_pipeline'\n",
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - graph(1) - preprocess(2) - sampling(3) - train(4))\n",
    "step = 0 \n",
    "input_args = alo.user_parameters[pipeline][step]['args'][0].copy()\n",
    "\n",
    "# 아래 주석을 풀어 input_args를 원하는 값으로 수정합니다. \n",
    "# input_args['x_columns'] = ['']\n",
    "input_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017bbb5c-b15e-4b8b-98c9-9b355da55263",
   "metadata": {},
   "source": [
    "##### Input asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ac5a67f-02e5-49f5-9d44-34f24b08baaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m==================== current time : 2023-10-26 07:01:19.569988+00:00 (UTC) ====================\u001b[0m\n",
      "************************************************************\n",
      "************************************************************\n",
      "\u001b[93m>> Load path : ['/home/jovyan/gcr/alo//input/sample/']\u001b[0m\n",
      "\u001b[92m>> The file for batch data has been loaded. (File name: /home/jovyan/gcr/alo//input/sample/customers.csv)\u001b[0m\n",
      "\u001b[93mYou set the << use_all_x >> as << True >> in the yaml file. So skip checking dataframe columns existence.\u001b[0m\n",
      "\u001b[92m==================== Success loading dataframe ====================\u001b[0m\n",
      "\u001b[94m>> Start processing ignore columns & drop columns: ['/home/jovyan/gcr/alo//input/sample/customers.csv']\u001b[0m\n",
      "\u001b[93m>> You set the << use_all_x >> parameter as << True >> in your config yaml. (So, these x_columns are used: ['name', 'FLAG_TRAIN_INFERENCE', 'age', 'address', 'spent', 'hobbies', 'job', 'orders', 'gender'] )\u001b[0m\n",
      "['name', 'FLAG_TRAIN_INFERENCE', 'age', 'address', 'spent', 'hobbies', 'job', 'orders', 'gender']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>orders</th>\n",
       "      <th>spent</th>\n",
       "      <th>job</th>\n",
       "      <th>hobbies</th>\n",
       "      <th>is_married</th>\n",
       "      <th>FLAG_TRAIN_INFERENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jasmine_Young</td>\n",
       "      <td>TN17745</td>\n",
       "      <td>female</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>233.44</td>\n",
       "      <td>Receptionist</td>\n",
       "      <td>Photography</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jeffery_Robinson</td>\n",
       "      <td>CT69980</td>\n",
       "      <td>male</td>\n",
       "      <td>42</td>\n",
       "      <td>15</td>\n",
       "      <td>264.70</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>Fishing</td>\n",
       "      <td>True</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Steven_Sullivan</td>\n",
       "      <td>CT13314</td>\n",
       "      <td>male</td>\n",
       "      <td>70</td>\n",
       "      <td>13</td>\n",
       "      <td>339.10</td>\n",
       "      <td>Janitor</td>\n",
       "      <td>Hiking</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jay_Williams</td>\n",
       "      <td>TN68283</td>\n",
       "      <td>male</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>70.61</td>\n",
       "      <td>Waitress</td>\n",
       "      <td>Playing musical instruments</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Benjamin_Beck</td>\n",
       "      <td>AE11377</td>\n",
       "      <td>male</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>748.94</td>\n",
       "      <td>Farmer</td>\n",
       "      <td>Playing sports</td>\n",
       "      <td>True</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gregory_Gomez</td>\n",
       "      <td>FM04887</td>\n",
       "      <td>male</td>\n",
       "      <td>75</td>\n",
       "      <td>10</td>\n",
       "      <td>937.97</td>\n",
       "      <td>Unkown</td>\n",
       "      <td>Running</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mary_Harris</td>\n",
       "      <td>KS55063</td>\n",
       "      <td>female</td>\n",
       "      <td>60</td>\n",
       "      <td>12</td>\n",
       "      <td>60.97</td>\n",
       "      <td>Librarian</td>\n",
       "      <td>Reading</td>\n",
       "      <td>True</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jimmy_Smith</td>\n",
       "      <td>AL47190</td>\n",
       "      <td>male</td>\n",
       "      <td>72</td>\n",
       "      <td>5</td>\n",
       "      <td>468.64</td>\n",
       "      <td>Waitress</td>\n",
       "      <td>Sewing</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kenneth_Rubio</td>\n",
       "      <td>RI07301</td>\n",
       "      <td>male</td>\n",
       "      <td>74</td>\n",
       "      <td>15</td>\n",
       "      <td>482.72</td>\n",
       "      <td>Polic</td>\n",
       "      <td>Dancing</td>\n",
       "      <td>True</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jordan_Simmons</td>\n",
       "      <td>AA06497</td>\n",
       "      <td>female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>156.16</td>\n",
       "      <td>Cashier</td>\n",
       "      <td>Baking</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name  address  gender  age  orders   spent           job  \\\n",
       "0     Jasmine_Young  TN17745  female   80       0  233.44  Receptionist   \n",
       "1  Jeffery_Robinson  CT69980    male   42      15  264.70       Teacher   \n",
       "2   Steven_Sullivan  CT13314    male   70      13  339.10       Janitor   \n",
       "3      Jay_Williams  TN68283    male   27       7   70.61      Waitress   \n",
       "4     Benjamin_Beck  AE11377    male   21       9  748.94        Farmer   \n",
       "5     Gregory_Gomez  FM04887    male   75      10  937.97        Unkown   \n",
       "6       Mary_Harris  KS55063  female   60      12   60.97     Librarian   \n",
       "7       Jimmy_Smith  AL47190    male   72       5  468.64      Waitress   \n",
       "8     Kenneth_Rubio  RI07301    male   74      15  482.72         Polic   \n",
       "9    Jordan_Simmons  AA06497  female   41       1  156.16       Cashier   \n",
       "\n",
       "                       hobbies is_married FLAG_TRAIN_INFERENCE  \n",
       "0                  Photography      False                TRAIN  \n",
       "1                      Fishing       True                TRAIN  \n",
       "2                       Hiking      False                TRAIN  \n",
       "3  Playing musical instruments      False                TRAIN  \n",
       "4               Playing sports       True                TRAIN  \n",
       "5                      Running      False                TRAIN  \n",
       "6                      Reading       True                TRAIN  \n",
       "7                       Sewing      False                TRAIN  \n",
       "8                      Dancing       True                TRAIN  \n",
       "9                       Baking      False                TRAIN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pipe_val = {} # 초기 input asset process 세팅\n",
    "\n",
    "data_input, config_input = alo.process_asset_step(alo.asset_source[pipeline][step], step, pipeline, data, pipe_val, [input_args])\n",
    "# data_input: input asset의 결과물입니다. 다음 asset 실행 시 필요합니다. \n",
    "# config_input: input asset의 결과 config입니다. 다음 asset실행 시 필요합니다.\n",
    "\n",
    "# input asset의 결과 dataframe은 data_input['dataframe']으로 확인할 수 있습니다. \n",
    "data_input['dataframe'].head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b53121",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### 1. Graph asset \n",
    "##### Graph asset의 args수정 및 확인\n",
    "- 필요한경우 graph_args의 항목을 ***graph_args의[argument명]=value입력*** 을 통해 변경할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d21fe408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'graph_type': None,\n",
       " 'center_node_column': 'name',\n",
       " 'embedding_column': 'name',\n",
       " 'train_inference_column': 'FLAG_TRAIN_INFERENCE',\n",
       " 'drop_columns': [],\n",
       " 'dimension': 64,\n",
       " 'num_epochs': 1,\n",
       " 'workers': None,\n",
       " 'num_partitions': None,\n",
       " 'extra_columns_for_ml': [],\n",
       " 'custom_connection': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - graph(1) - preprocess(2) - sampling(3) - train(4))\n",
    "step = 1 \n",
    "graph_args = alo.user_parameters[pipeline][step]['args'][0].copy()\n",
    "\n",
    "# 아래 주석을 풀어 graph_args 수정합니다. \n",
    "# graph_args['graph_type'] = 'relational'\n",
    "graph_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2084ad",
   "metadata": {},
   "source": [
    "##### Graph asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c100632",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m>> Successfully got << output path >> for saving your data into csv or jpg file: \n",
      " /home/jovyan/gcr/alo//.train_artifacts/output/graph/ \n",
      " L [NOTE] The names of output file must be fixed as << output.csv, output.jpg >> \u001b[0m\n",
      "************************************************************\n",
      "************************************************************\n",
      "preprocessing blank space...\n",
      "In __init__: pbg ready\n",
      "[2023-10-26 07:13:33.587077] Using the 7 relation types given in the config\n",
      "[2023-10-26 07:13:33.588337] Searching for the entities in the edge files...\n",
      "[2023-10-26 07:13:33.680755] Entity type address:\n",
      "[2023-10-26 07:13:33.681810] - Found 1000 entities\n",
      "[2023-10-26 07:13:33.682820] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-10-26 07:13:33.683989] - Left with 1000 entities\n",
      "[2023-10-26 07:13:33.684985] - Shuffling them...\n",
      "[2023-10-26 07:13:33.686736] Entity type name:\n",
      "[2023-10-26 07:13:33.687902] - Found 985 entities\n",
      "[2023-10-26 07:13:33.688926] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-10-26 07:13:33.690124] - Left with 985 entities\n",
      "[2023-10-26 07:13:33.692774] - Shuffling them...\n",
      "[2023-10-26 07:13:33.694520] Entity type age:\n",
      "[2023-10-26 07:13:33.695446] - Found 63 entities\n",
      "[2023-10-26 07:13:33.696357] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-10-26 07:13:33.697339] - Left with 63 entities\n",
      "[2023-10-26 07:13:33.698251] - Shuffling them...\n",
      "[2023-10-26 07:13:33.699209] Entity type job:\n",
      "[2023-10-26 07:13:33.700128] - Found 37 entities\n",
      "[2023-10-26 07:13:33.700922] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-10-26 07:13:33.701521] - Left with 37 entities\n",
      "[2023-10-26 07:13:33.702102] - Shuffling them...\n",
      "[2023-10-26 07:13:33.702682] Entity type hobbies:\n",
      "[2023-10-26 07:13:33.703262] - Found 27 entities\n",
      "[2023-10-26 07:13:33.703767] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-10-26 07:13:33.704358] - Left with 27 entities\n",
      "[2023-10-26 07:13:33.704905] - Shuffling them...\n",
      "[2023-10-26 07:13:33.705528] Entity type orders:\n",
      "[2023-10-26 07:13:33.706066] - Found 21 entities\n",
      "[2023-10-26 07:13:33.706635] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-10-26 07:13:33.707189] - Left with 21 entities\n",
      "[2023-10-26 07:13:33.707752] - Shuffling them...\n",
      "[2023-10-26 07:13:33.708316] Entity type gender:\n",
      "[2023-10-26 07:13:33.708911] - Found 2 entities\n",
      "[2023-10-26 07:13:33.709445] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-10-26 07:13:33.710053] - Left with 2 entities\n",
      "[2023-10-26 07:13:33.710586] - Shuffling them...\n",
      "[2023-10-26 07:13:33.711129] Entity type spent:\n",
      "[2023-10-26 07:13:33.711642] - Found 360 entities\n",
      "[2023-10-26 07:13:33.712158] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-10-26 07:13:33.712750] - Left with 360 entities\n",
      "[2023-10-26 07:13:33.713262] - Shuffling them...\n",
      "[2023-10-26 07:13:33.714110] Preparing counts and dictionaries for entities and relation types:\n",
      "[2023-10-26 07:13:33.721015] - Writing count of entity type address and partition 0\n",
      "[2023-10-26 07:13:33.732695] - Writing count of entity type name and partition 0\n",
      "[2023-10-26 07:13:33.764936] - Writing count of entity type age and partition 0\n",
      "[2023-10-26 07:13:33.798615] - Writing count of entity type job and partition 0\n",
      "[2023-10-26 07:13:33.821957] - Writing count of entity type hobbies and partition 0\n",
      "[2023-10-26 07:13:33.853320] - Writing count of entity type orders and partition 0\n",
      "[2023-10-26 07:13:34.059179] - Writing count of entity type gender and partition 0\n",
      "[2023-10-26 07:13:34.063270] - Writing count of entity type spent and partition 0\n",
      "[2023-10-26 07:13:34.068826] Preparing edge path /home/jovyan/gcr/alo//.train_artifacts/output/graph/partitions/edges_partitioned_rel_3, out of the edges found in /home/jovyan/gcr/alo/.train_artifacts/output/graph/tsvs/rel_3.tsv\n",
      "[2023-10-26 07:13:34.171517] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-10-26 07:14:57.123920] - Processed 999 edges in total\n",
      "[2023-10-26 07:14:57.126042] Preparing edge path /home/jovyan/gcr/alo//.train_artifacts/output/graph/partitions/edges_partitioned_rel_0, out of the edges found in /home/jovyan/gcr/alo/.train_artifacts/output/graph/tsvs/rel_0.tsv\n",
      "[2023-10-26 07:14:57.128036] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-10-26 07:15:29.658929] - Processed 1000 edges in total\n",
      "[2023-10-26 07:15:29.661080] Preparing edge path /home/jovyan/gcr/alo//.train_artifacts/output/graph/partitions/edges_partitioned_rel_5, out of the edges found in /home/jovyan/gcr/alo/.train_artifacts/output/graph/tsvs/rel_5.tsv\n",
      "[2023-10-26 07:15:29.663502] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-10-26 07:15:34.147026] - Processed 1000 edges in total\n",
      "[2023-10-26 07:15:34.149106] Preparing edge path /home/jovyan/gcr/alo//.train_artifacts/output/graph/partitions/edges_partitioned_rel_8, out of the edges found in /home/jovyan/gcr/alo/.train_artifacts/output/graph/tsvs/rel_8.tsv\n",
      "[2023-10-26 07:15:34.157017] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-10-26 07:15:36.087310] - Processed 1000 edges in total\n",
      "[2023-10-26 07:15:36.089455] Preparing edge path /home/jovyan/gcr/alo//.train_artifacts/output/graph/partitions/edges_partitioned_rel_4, out of the edges found in /home/jovyan/gcr/alo/.train_artifacts/output/graph/tsvs/rel_4.tsv\n",
      "[2023-10-26 07:15:36.091543] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-10-26 07:15:38.059200] - Processed 1000 edges in total\n",
      "[2023-10-26 07:15:38.060593] Preparing edge path /home/jovyan/gcr/alo//.train_artifacts/output/graph/partitions/edges_partitioned_rel_7, out of the edges found in /home/jovyan/gcr/alo/.train_artifacts/output/graph/tsvs/rel_7.tsv\n",
      "[2023-10-26 07:15:38.068612] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-10-26 07:15:41.441773] - Processed 985 edges in total\n",
      "[2023-10-26 07:15:41.443447] Preparing edge path /home/jovyan/gcr/alo//.train_artifacts/output/graph/partitions/edges_partitioned_rel_2, out of the edges found in /home/jovyan/gcr/alo/.train_artifacts/output/graph/tsvs/rel_2.tsv\n",
      "[2023-10-26 07:15:41.447152] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-10-26 07:15:43.648876] - Processed 1000 edges in total\n",
      "2023-10-26 07:15:43,651   [Trainer-0] Loading entity counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torchbiggraph/util.py:222: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = tensor.storage_type()._new_shared(size.numel())\n",
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torch/storage.py:959: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if self.device.type not in ['cpu', 'cuda']:\n",
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torch/storage.py:962: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  module = torch if self.device.type == 'cpu' else torch.cuda\n",
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torch/storage.py:985: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  untyped_storage = torch.UntypedStorage._new_shared(size * cls()._element_size())\n",
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torch/storage.py:986: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return cls(wrap_storage=untyped_storage)\n",
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-26 07:15:44,521   [Trainer-0] Creating workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torchbiggraph/train_cpu.py:304: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  ).storage()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-26 07:15:48,727   [Trainer-0] Initializing global model...\n",
      "2023-10-26 07:15:49,528   [Trainer-0] Starting epoch 1 / 1, edge path 1 / 7, edge chunk 1 / 2\n",
      "2023-10-26 07:15:49,531   [Trainer-0] Edge path: /home/jovyan/gcr/alo//.train_artifacts/output/graph/partitions/edges_partitioned_rel_3\n",
      "2023-10-26 07:15:49,532   [Trainer-0] still in queue: 0\n",
      "2023-10-26 07:15:49,533   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
      "2023-10-26 07:15:49,534   [Trainer-0] Loading partitioned embeddings from checkpoint\n",
      "2023-10-26 07:16:34,443   [Trainer-0] ( 0 , 0 ): Training stats: loss:  10.0362 , reg:  0.00110181 , violators_lhs:  73.962 , violators_rhs:  74.264 , count:  500\n",
      "2023-10-26 07:16:34,445   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Trained 500 edges in 42.47 s ( 1.2e-05 M/sec ); Eval 2*0 edges in 0.00 s ( 0 M/sec ); io: 2.44 s for 650,720 bytes ( 0.27 MB/sec )\n",
      "2023-10-26 07:16:34,446   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
      "2023-10-26 07:16:34,447   [Trainer-0] Saving partitioned embeddings to checkpoint\n",
      "2023-10-26 07:16:34,534   [Trainer-0] Finished epoch 1 / 1, edge path 1 / 7, edge chunk 1 / 2\n",
      "2023-10-26 07:16:34,535   [Trainer-0] Writing the metadata\n",
      "2023-10-26 07:16:34,553   [Trainer-0] Writing the training stats\n",
      "2023-10-26 07:16:34,555   [Trainer-0] Writing the checkpoint\n",
      "2023-10-26 07:16:34,556   [Trainer-0] Switching to the new checkpoint version\n",
      "2023-10-26 07:16:34,559   [Trainer-0] Starting epoch 1 / 1, edge path 1 / 7, edge chunk 2 / 2\n",
      "2023-10-26 07:16:34,560   [Trainer-0] Edge path: /home/jovyan/gcr/alo//.train_artifacts/output/graph/partitions/edges_partitioned_rel_3\n",
      "2023-10-26 07:16:34,561   [Trainer-0] still in queue: 0\n",
      "2023-10-26 07:16:34,562   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
      "2023-10-26 07:16:34,563   [Trainer-0] Loading partitioned embeddings from checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/torchbiggraph/train_cpu.py:821: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  self.embedding_storage_freelist[entity].add(embs.storage())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-26 07:16:34,718   [Trainer-0] ( 0 , 0 ): Training stats: loss:  10.0395 , reg:  0.00110294 , violators_lhs:  75.2545 , violators_rhs:  75.503 , count:  499\n",
      "2023-10-26 07:16:34,719   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Trained 499 edges in 0.06 s ( 0.0083 M/sec ); Eval 2*0 edges in 0.00 s ( 0 M/sec ); io: 0.10 s for 650,696 bytes ( 6.75 MB/sec )\n",
      "2023-10-26 07:16:34,720   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
      "2023-10-26 07:16:34,721   [Trainer-0] Saving partitioned embeddings to checkpoint\n",
      "2023-10-26 07:16:34,780   [Trainer-0] Finished epoch 1 / 1, edge path 1 / 7, edge chunk 2 / 2\n",
      "2023-10-26 07:16:34,781   [Trainer-0] Writing the metadata\n",
      "2023-10-26 07:16:35,401   [Trainer-0] Writing the training stats\n",
      "2023-10-26 07:16:35,404   [Trainer-0] Writing the checkpoint\n",
      "2023-10-26 07:16:35,405   [Trainer-0] Switching to the new checkpoint version\n",
      "2023-10-26 07:16:35,414   [Trainer-0] Starting epoch 1 / 1, edge path 2 / 7, edge chunk 1 / 2\n",
      "2023-10-26 07:16:35,417   [Trainer-0] Edge path: /home/jovyan/gcr/alo//.train_artifacts/output/graph/partitions/edges_partitioned_rel_0\n",
      "2023-10-26 07:16:35,418   [Trainer-0] still in queue: 0\n",
      "2023-10-26 07:16:35,419   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
      "2023-10-26 07:16:35,420   [Trainer-0] Loading partitioned embeddings from checkpoint\n",
      "2023-10-26 07:16:35,606   [Trainer-0] ( 0 , 0 ): Training stats: loss:  10.0414 , reg:  0.0011009 , violators_lhs:  75.79 , violators_rhs:  75.888 , count:  500\n",
      "2023-10-26 07:16:35,608   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Trained 500 edges in 0.05 s ( 0.0094 M/sec ); Eval 2*0 edges in 0.00 s ( 0 M/sec ); io: 0.14 s for 650,720 bytes ( 4.80 MB/sec )\n",
      "2023-10-26 07:16:35,609   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
      "2023-10-26 07:16:35,610   [Trainer-0] Saving partitioned embeddings to checkpoint\n",
      "2023-10-26 07:16:35,669   [Trainer-0] Finished epoch 1 / 1, edge path 2 / 7, edge chunk 1 / 2\n",
      "2023-10-26 07:16:35,670   [Trainer-0] Writing the metadata\n",
      "2023-10-26 07:16:35,689   [Trainer-0] Writing the training stats\n",
      "2023-10-26 07:16:35,691   [Trainer-0] Writing the checkpoint\n",
      "2023-10-26 07:16:35,692   [Trainer-0] Switching to the new checkpoint version\n",
      "2023-10-26 07:16:35,699   [Trainer-0] Starting epoch 1 / 1, edge path 2 / 7, edge chunk 2 / 2\n",
      "2023-10-26 07:16:35,701   [Trainer-0] Edge path: /home/jovyan/gcr/alo//.train_artifacts/output/graph/partitions/edges_partitioned_rel_0\n",
      "2023-10-26 07:16:35,702   [Trainer-0] still in queue: 0\n",
      "2023-10-26 07:16:35,703   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
      "2023-10-26 07:16:35,704   [Trainer-0] Loading partitioned embeddings from checkpoint\n",
      "2023-10-26 07:16:35,809   [Trainer-0] ( 0 , 0 ): Training stats: loss:  9.99368 , reg:  0.00110357 , violators_lhs:  67.388 , violators_rhs:  69.968 , count:  500\n",
      "2023-10-26 07:16:35,811   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Trained 500 edges in 0.05 s ( 0.0099 M/sec ); Eval 2*0 edges in 0.00 s ( 0 M/sec ); io: 0.06 s for 650,720 bytes ( 11.38 MB/sec )\n",
      "2023-10-26 07:16:35,812   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
      "2023-10-26 07:16:35,813   [Trainer-0] Saving partitioned embeddings to checkpoint\n",
      "2023-10-26 07:16:35,875   [Trainer-0] Finished epoch 1 / 1, edge path 2 / 7, edge chunk 2 / 2\n",
      "2023-10-26 07:16:35,876   [Trainer-0] Writing the metadata\n",
      "2023-10-26 07:16:35,894   [Trainer-0] Writing the training stats\n",
      "2023-10-26 07:16:35,897   [Trainer-0] Writing the checkpoint\n",
      "2023-10-26 07:16:35,898   [Trainer-0] Switching to the new checkpoint version\n",
      "2023-10-26 07:16:38,513   [Trainer-0] Starting epoch 1 / 1, edge path 3 / 7, edge chunk 1 / 2\n",
      "2023-10-26 07:16:38,516   [Trainer-0] Edge path: /home/jovyan/gcr/alo//.train_artifacts/output/graph/partitions/edges_partitioned_rel_5\n",
      "2023-10-26 07:16:38,518   [Trainer-0] still in queue: 0\n",
      "2023-10-26 07:16:38,519   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
      "2023-10-26 07:16:38,520   [Trainer-0] Loading partitioned embeddings from checkpoint\n",
      "2023-10-26 07:16:38,839   [Trainer-0] ( 0 , 0 ): Training stats: loss:  10.0452 , reg:  0.00110168 , violators_lhs:  73.554 , violators_rhs:  76.71 , count:  500\n",
      "2023-10-26 07:16:38,840   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Trained 500 edges in 0.06 s ( 0.0085 M/sec ); Eval 2*0 edges in 0.00 s ( 0 M/sec ); io: 0.26 s for 650,720 bytes ( 2.49 MB/sec )\n",
      "2023-10-26 07:16:38,841   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
      "2023-10-26 07:16:38,842   [Trainer-0] Saving partitioned embeddings to checkpoint\n",
      "2023-10-26 07:16:43,707   [Trainer-0] Finished epoch 1 / 1, edge path 3 / 7, edge chunk 1 / 2\n",
      "2023-10-26 07:16:43,708   [Trainer-0] Writing the metadata\n",
      "2023-10-26 07:16:43,726   [Trainer-0] Writing the training stats\n",
      "2023-10-26 07:16:43,729   [Trainer-0] Writing the checkpoint\n",
      "2023-10-26 07:16:43,730   [Trainer-0] Switching to the new checkpoint version\n",
      "2023-10-26 07:16:43,739   [Trainer-0] Starting epoch 1 / 1, edge path 3 / 7, edge chunk 2 / 2\n",
      "2023-10-26 07:16:43,740   [Trainer-0] Edge path: /home/jovyan/gcr/alo//.train_artifacts/output/graph/partitions/edges_partitioned_rel_5\n",
      "2023-10-26 07:16:43,742   [Trainer-0] still in queue: 0\n",
      "2023-10-26 07:16:43,743   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
      "2023-10-26 07:16:43,744   [Trainer-0] Loading partitioned embeddings from checkpoint\n",
      "2023-10-26 07:16:43,847   [Trainer-0] ( 0 , 0 ): Training stats: loss:  10.0704 , reg:  0.00109734 , violators_lhs:  72.152 , violators_rhs:  80.244 , count:  500\n",
      "2023-10-26 07:16:43,848   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Trained 500 edges in 0.05 s ( 0.011 M/sec ); Eval 2*0 edges in 0.00 s ( 0 M/sec ); io: 0.06 s for 650,720 bytes ( 11.18 MB/sec )\n",
      "2023-10-26 07:16:43,849   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
      "2023-10-26 07:16:43,850   [Trainer-0] Saving partitioned embeddings to checkpoint\n",
      "2023-10-26 07:16:43,912   [Trainer-0] Finished epoch 1 / 1, edge path 3 / 7, edge chunk 2 / 2\n",
      "2023-10-26 07:16:43,913   [Trainer-0] Writing the metadata\n",
      "2023-10-26 07:16:45,330   [Trainer-0] Writing the training stats\n",
      "2023-10-26 07:16:45,760   [Trainer-0] Writing the checkpoint\n",
      "2023-10-26 07:16:45,761   [Trainer-0] Switching to the new checkpoint version\n",
      "2023-10-26 07:16:45,820   [Trainer-0] Starting epoch 1 / 1, edge path 4 / 7, edge chunk 1 / 2\n",
      "2023-10-26 07:16:45,822   [Trainer-0] Edge path: /home/jovyan/gcr/alo//.train_artifacts/output/graph/partitions/edges_partitioned_rel_8\n",
      "2023-10-26 07:16:45,824   [Trainer-0] still in queue: 0\n",
      "2023-10-26 07:16:45,825   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
      "2023-10-26 07:16:45,827   [Trainer-0] Loading partitioned embeddings from checkpoint\n",
      "2023-10-26 07:16:45,967   [Trainer-0] ( 0 , 0 ): Training stats: loss:  10.0362 , reg:  0.00110152 , violators_lhs:  74.91 , violators_rhs:  74.406 , count:  500\n",
      "2023-10-26 07:16:45,969   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Trained 500 edges in 0.05 s ( 0.0094 M/sec ); Eval 2*0 edges in 0.00 s ( 0 M/sec ); io: 0.09 s for 650,720 bytes ( 7.19 MB/sec )\n",
      "2023-10-26 07:16:45,970   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
      "2023-10-26 07:16:45,971   [Trainer-0] Saving partitioned embeddings to checkpoint\n",
      "2023-10-26 07:16:46,834   [Trainer-0] Finished epoch 1 / 1, edge path 4 / 7, edge chunk 1 / 2\n",
      "2023-10-26 07:16:46,835   [Trainer-0] Writing the metadata\n",
      "2023-10-26 07:16:46,852   [Trainer-0] Writing the training stats\n",
      "2023-10-26 07:16:46,853   [Trainer-0] Writing the checkpoint\n",
      "2023-10-26 07:16:46,854   [Trainer-0] Switching to the new checkpoint version\n",
      "2023-10-26 07:16:47,062   [Trainer-0] Starting epoch 1 / 1, edge path 4 / 7, edge chunk 2 / 2\n",
      "2023-10-26 07:16:47,064   [Trainer-0] Edge path: /home/jovyan/gcr/alo//.train_artifacts/output/graph/partitions/edges_partitioned_rel_8\n",
      "2023-10-26 07:16:47,066   [Trainer-0] still in queue: 0\n",
      "2023-10-26 07:16:47,067   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
      "2023-10-26 07:16:47,068   [Trainer-0] Loading partitioned embeddings from checkpoint\n",
      "2023-10-26 07:16:47,181   [Trainer-0] ( 0 , 0 ): Training stats: loss:  10.034 , reg:  0.0011025 , violators_lhs:  71.06 , violators_rhs:  72.752 , count:  500\n",
      "2023-10-26 07:16:47,183   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Trained 500 edges in 0.06 s ( 0.0091 M/sec ); Eval 2*0 edges in 0.00 s ( 0 M/sec ); io: 0.06 s for 650,720 bytes ( 10.86 MB/sec )\n",
      "2023-10-26 07:16:47,184   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
      "2023-10-26 07:16:47,185   [Trainer-0] Saving partitioned embeddings to checkpoint\n",
      "2023-10-26 07:17:01,379   [Trainer-0] Finished epoch 1 / 1, edge path 4 / 7, edge chunk 2 / 2\n",
      "2023-10-26 07:17:01,380   [Trainer-0] Writing the metadata\n",
      "2023-10-26 07:17:01,402   [Trainer-0] Writing the training stats\n",
      "2023-10-26 07:17:01,405   [Trainer-0] Writing the checkpoint\n",
      "2023-10-26 07:17:01,406   [Trainer-0] Switching to the new checkpoint version\n",
      "2023-10-26 07:17:01,616   [Trainer-0] Starting epoch 1 / 1, edge path 5 / 7, edge chunk 1 / 2\n",
      "2023-10-26 07:17:01,618   [Trainer-0] Edge path: /home/jovyan/gcr/alo//.train_artifacts/output/graph/partitions/edges_partitioned_rel_4\n",
      "2023-10-26 07:17:01,620   [Trainer-0] still in queue: 0\n",
      "2023-10-26 07:17:01,621   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
      "2023-10-26 07:17:01,622   [Trainer-0] Loading partitioned embeddings from checkpoint\n",
      "2023-10-26 07:17:01,998   [Trainer-0] ( 0 , 0 ): Training stats: loss:  10.0427 , reg:  0.00110292 , violators_lhs:  74.166 , violators_rhs:  76.072 , count:  500\n",
      "2023-10-26 07:17:02,000   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Trained 500 edges in 0.05 s ( 0.0092 M/sec ); Eval 2*0 edges in 0.00 s ( 0 M/sec ); io: 0.32 s for 650,720 bytes ( 2.01 MB/sec )\n",
      "2023-10-26 07:17:02,001   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
      "2023-10-26 07:17:02,002   [Trainer-0] Saving partitioned embeddings to checkpoint\n",
      "2023-10-26 07:17:02,068   [Trainer-0] Finished epoch 1 / 1, edge path 5 / 7, edge chunk 1 / 2\n",
      "2023-10-26 07:17:02,070   [Trainer-0] Writing the metadata\n",
      "2023-10-26 07:17:02,092   [Trainer-0] Writing the training stats\n",
      "2023-10-26 07:17:02,094   [Trainer-0] Writing the checkpoint\n",
      "2023-10-26 07:17:02,095   [Trainer-0] Switching to the new checkpoint version\n",
      "2023-10-26 07:17:02,103   [Trainer-0] Starting epoch 1 / 1, edge path 5 / 7, edge chunk 2 / 2\n",
      "2023-10-26 07:17:02,104   [Trainer-0] Edge path: /home/jovyan/gcr/alo//.train_artifacts/output/graph/partitions/edges_partitioned_rel_4\n",
      "2023-10-26 07:17:02,105   [Trainer-0] still in queue: 0\n",
      "2023-10-26 07:17:02,106   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
      "2023-10-26 07:17:02,106   [Trainer-0] Loading partitioned embeddings from checkpoint\n",
      "2023-10-26 07:17:02,212   [Trainer-0] ( 0 , 0 ): Training stats: loss:  10.0873 , reg:  0.00110059 , violators_lhs:  75.512 , violators_rhs:  82.236 , count:  500\n",
      "2023-10-26 07:17:02,213   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Trained 500 edges in 0.05 s ( 0.0093 M/sec ); Eval 2*0 edges in 0.00 s ( 0 M/sec ); io: 0.05 s for 650,720 bytes ( 12.05 MB/sec )\n",
      "2023-10-26 07:17:02,215   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
      "2023-10-26 07:17:02,216   [Trainer-0] Saving partitioned embeddings to checkpoint\n",
      "2023-10-26 07:17:09,887   [Trainer-0] Finished epoch 1 / 1, edge path 5 / 7, edge chunk 2 / 2\n",
      "2023-10-26 07:17:09,888   [Trainer-0] Writing the metadata\n",
      "2023-10-26 07:17:09,905   [Trainer-0] Writing the training stats\n",
      "2023-10-26 07:17:09,907   [Trainer-0] Writing the checkpoint\n",
      "2023-10-26 07:17:09,907   [Trainer-0] Switching to the new checkpoint version\n",
      "2023-10-26 07:17:09,916   [Trainer-0] Starting epoch 1 / 1, edge path 6 / 7, edge chunk 1 / 2\n",
      "2023-10-26 07:17:09,917   [Trainer-0] Edge path: /home/jovyan/gcr/alo//.train_artifacts/output/graph/partitions/edges_partitioned_rel_7\n",
      "2023-10-26 07:17:09,919   [Trainer-0] still in queue: 0\n",
      "2023-10-26 07:17:09,920   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
      "2023-10-26 07:17:09,920   [Trainer-0] Loading partitioned embeddings from checkpoint\n",
      "2023-10-26 07:17:10,209   [Trainer-0] ( 0 , 0 ): Training stats: loss:  10.0395 , reg:  0.0011095 , violators_lhs:  69.4057 , violators_rhs:  80.1136 , count:  493\n",
      "2023-10-26 07:17:10,210   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Trained 493 edges in 0.06 s ( 0.0087 M/sec ); Eval 2*0 edges in 0.00 s ( 0 M/sec ); io: 0.23 s for 650,552 bytes ( 2.78 MB/sec )\n",
      "2023-10-26 07:17:10,211   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
      "2023-10-26 07:17:10,212   [Trainer-0] Saving partitioned embeddings to checkpoint\n",
      "2023-10-26 07:17:10,479   [Trainer-0] Finished epoch 1 / 1, edge path 6 / 7, edge chunk 1 / 2\n",
      "2023-10-26 07:17:10,480   [Trainer-0] Writing the metadata\n",
      "2023-10-26 07:17:10,898   [Trainer-0] Writing the training stats\n",
      "2023-10-26 07:17:10,901   [Trainer-0] Writing the checkpoint\n",
      "2023-10-26 07:17:10,903   [Trainer-0] Switching to the new checkpoint version\n",
      "2023-10-26 07:17:11,245   [Trainer-0] Starting epoch 1 / 1, edge path 6 / 7, edge chunk 2 / 2\n",
      "2023-10-26 07:17:11,248   [Trainer-0] Edge path: /home/jovyan/gcr/alo//.train_artifacts/output/graph/partitions/edges_partitioned_rel_7\n",
      "2023-10-26 07:17:11,249   [Trainer-0] still in queue: 0\n",
      "2023-10-26 07:17:11,250   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
      "2023-10-26 07:17:11,252   [Trainer-0] Loading partitioned embeddings from checkpoint\n",
      "2023-10-26 07:17:11,360   [Trainer-0] ( 0 , 0 ): Training stats: loss:  10.1123 , reg:  0.00110397 , violators_lhs:  62.2622 , violators_rhs:  88.7012 , count:  492\n",
      "2023-10-26 07:17:11,362   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Trained 492 edges in 0.06 s ( 0.0084 M/sec ); Eval 2*0 edges in 0.00 s ( 0 M/sec ); io: 0.05 s for 650,528 bytes ( 12.27 MB/sec )\n",
      "2023-10-26 07:17:11,363   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
      "2023-10-26 07:17:11,365   [Trainer-0] Saving partitioned embeddings to checkpoint\n",
      "2023-10-26 07:17:11,417   [Trainer-0] Finished epoch 1 / 1, edge path 6 / 7, edge chunk 2 / 2\n",
      "2023-10-26 07:17:11,418   [Trainer-0] Writing the metadata\n",
      "2023-10-26 07:17:11,634   [Trainer-0] Writing the training stats\n",
      "2023-10-26 07:17:11,637   [Trainer-0] Writing the checkpoint\n",
      "2023-10-26 07:17:11,638   [Trainer-0] Switching to the new checkpoint version\n",
      "2023-10-26 07:17:11,649   [Trainer-0] Starting epoch 1 / 1, edge path 7 / 7, edge chunk 1 / 2\n",
      "2023-10-26 07:17:11,651   [Trainer-0] Edge path: /home/jovyan/gcr/alo//.train_artifacts/output/graph/partitions/edges_partitioned_rel_2\n",
      "2023-10-26 07:17:11,653   [Trainer-0] still in queue: 0\n",
      "2023-10-26 07:17:11,654   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
      "2023-10-26 07:17:11,655   [Trainer-0] Loading partitioned embeddings from checkpoint\n",
      "2023-10-26 07:17:12,021   [Trainer-0] ( 0 , 0 ): Training stats: loss:  10.0407 , reg:  0.00110233 , violators_lhs:  75.526 , violators_rhs:  74.974 , count:  500\n",
      "2023-10-26 07:17:12,022   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Trained 500 edges in 0.06 s ( 0.0086 M/sec ); Eval 2*0 edges in 0.00 s ( 0 M/sec ); io: 0.31 s for 650,720 bytes ( 2.10 MB/sec )\n",
      "2023-10-26 07:17:12,023   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
      "2023-10-26 07:17:12,025   [Trainer-0] Saving partitioned embeddings to checkpoint\n",
      "2023-10-26 07:17:13,290   [Trainer-0] Finished epoch 1 / 1, edge path 7 / 7, edge chunk 1 / 2\n",
      "2023-10-26 07:17:13,291   [Trainer-0] Writing the metadata\n",
      "2023-10-26 07:17:13,309   [Trainer-0] Writing the training stats\n",
      "2023-10-26 07:17:13,312   [Trainer-0] Writing the checkpoint\n",
      "2023-10-26 07:17:13,313   [Trainer-0] Switching to the new checkpoint version\n",
      "2023-10-26 07:17:13,322   [Trainer-0] Starting epoch 1 / 1, edge path 7 / 7, edge chunk 2 / 2\n",
      "2023-10-26 07:17:13,324   [Trainer-0] Edge path: /home/jovyan/gcr/alo//.train_artifacts/output/graph/partitions/edges_partitioned_rel_2\n",
      "2023-10-26 07:17:13,326   [Trainer-0] still in queue: 0\n",
      "2023-10-26 07:17:13,327   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
      "2023-10-26 07:17:13,327   [Trainer-0] Loading partitioned embeddings from checkpoint\n",
      "2023-10-26 07:17:13,412   [Trainer-0] ( 0 , 0 ): Training stats: loss:  10.065 , reg:  0.00110328 , violators_lhs:  75.062 , violators_rhs:  77.712 , count:  500\n",
      "2023-10-26 07:17:13,413   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Trained 500 edges in 0.06 s ( 0.0088 M/sec ); Eval 2*0 edges in 0.00 s ( 0 M/sec ); io: 0.03 s for 650,720 bytes ( 21.79 MB/sec )\n",
      "2023-10-26 07:17:13,414   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
      "2023-10-26 07:17:13,415   [Trainer-0] Saving partitioned embeddings to checkpoint\n",
      "2023-10-26 07:17:14,877   [Trainer-0] Finished epoch 1 / 1, edge path 7 / 7, edge chunk 2 / 2\n",
      "2023-10-26 07:17:14,879   [Trainer-0] Writing the metadata\n",
      "2023-10-26 07:17:14,904   [Trainer-0] Writing the training stats\n",
      "2023-10-26 07:17:14,906   [Trainer-0] Writing the checkpoint\n",
      "2023-10-26 07:17:14,907   [Trainer-0] Switching to the new checkpoint version\n",
      "2023-10-26 07:17:15,430   [Trainer-0] Exiting\n",
      "[Embedding Complete]\n",
      "[Embeddig result saved at /home/jovyan/gcr/alo//.train_artifacts/output/graph/RESULT]\n",
      "\u001b[92m>> Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr/alo//.train_artifacts/models/graph/\u001b[0m\n",
      "In __del__: pbg deleted\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>is_married</th>\n",
       "      <th>EMB_00</th>\n",
       "      <th>EMB_01</th>\n",
       "      <th>EMB_02</th>\n",
       "      <th>EMB_03</th>\n",
       "      <th>EMB_04</th>\n",
       "      <th>EMB_05</th>\n",
       "      <th>EMB_06</th>\n",
       "      <th>EMB_07</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_54</th>\n",
       "      <th>EMB_55</th>\n",
       "      <th>EMB_56</th>\n",
       "      <th>EMB_57</th>\n",
       "      <th>EMB_58</th>\n",
       "      <th>EMB_59</th>\n",
       "      <th>EMB_60</th>\n",
       "      <th>EMB_61</th>\n",
       "      <th>EMB_62</th>\n",
       "      <th>EMB_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jasmine_Young</td>\n",
       "      <td>False</td>\n",
       "      <td>0.016538</td>\n",
       "      <td>-0.008067</td>\n",
       "      <td>-0.007596</td>\n",
       "      <td>-0.000633</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.016212</td>\n",
       "      <td>-0.010445</td>\n",
       "      <td>-0.013139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004342</td>\n",
       "      <td>-0.000188</td>\n",
       "      <td>0.007250</td>\n",
       "      <td>-0.002752</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>-0.004680</td>\n",
       "      <td>-0.004442</td>\n",
       "      <td>0.008832</td>\n",
       "      <td>-0.021160</td>\n",
       "      <td>-0.003933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jeffery_Robinson</td>\n",
       "      <td>True</td>\n",
       "      <td>0.017017</td>\n",
       "      <td>-0.022626</td>\n",
       "      <td>0.001806</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>0.016354</td>\n",
       "      <td>0.010338</td>\n",
       "      <td>0.003199</td>\n",
       "      <td>0.007888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001560</td>\n",
       "      <td>0.007903</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>-0.003179</td>\n",
       "      <td>-0.002239</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>-0.013558</td>\n",
       "      <td>0.012339</td>\n",
       "      <td>0.011655</td>\n",
       "      <td>0.012075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Steven_Sullivan</td>\n",
       "      <td>False</td>\n",
       "      <td>0.004450</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>-0.007673</td>\n",
       "      <td>-0.012033</td>\n",
       "      <td>0.008483</td>\n",
       "      <td>-0.015631</td>\n",
       "      <td>-0.005434</td>\n",
       "      <td>0.002231</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008088</td>\n",
       "      <td>-0.006004</td>\n",
       "      <td>0.008902</td>\n",
       "      <td>0.003953</td>\n",
       "      <td>0.006039</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>-0.001416</td>\n",
       "      <td>-0.003754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jay_Williams</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>-0.011676</td>\n",
       "      <td>-0.005207</td>\n",
       "      <td>0.008740</td>\n",
       "      <td>-0.003923</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.004290</td>\n",
       "      <td>0.016565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>-0.001760</td>\n",
       "      <td>-0.002549</td>\n",
       "      <td>0.007215</td>\n",
       "      <td>-0.020126</td>\n",
       "      <td>0.001544</td>\n",
       "      <td>-0.009346</td>\n",
       "      <td>-0.005447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Benjamin_Beck</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.025749</td>\n",
       "      <td>-0.008956</td>\n",
       "      <td>0.011284</td>\n",
       "      <td>0.021791</td>\n",
       "      <td>-0.008500</td>\n",
       "      <td>0.007442</td>\n",
       "      <td>-0.025526</td>\n",
       "      <td>-0.010672</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001501</td>\n",
       "      <td>-0.005327</td>\n",
       "      <td>0.005365</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>-0.002955</td>\n",
       "      <td>-0.008059</td>\n",
       "      <td>-0.017144</td>\n",
       "      <td>-0.021904</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>0.009757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gregory_Gomez</td>\n",
       "      <td>False</td>\n",
       "      <td>0.006974</td>\n",
       "      <td>-0.003312</td>\n",
       "      <td>-0.005165</td>\n",
       "      <td>-0.011515</td>\n",
       "      <td>0.005372</td>\n",
       "      <td>0.015446</td>\n",
       "      <td>0.012202</td>\n",
       "      <td>0.002737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>0.023558</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>0.022578</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.008357</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>-0.003001</td>\n",
       "      <td>0.018339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mary_Harris</td>\n",
       "      <td>True</td>\n",
       "      <td>0.006794</td>\n",
       "      <td>0.006037</td>\n",
       "      <td>-0.024521</td>\n",
       "      <td>-0.009489</td>\n",
       "      <td>-0.010266</td>\n",
       "      <td>0.009738</td>\n",
       "      <td>-0.007976</td>\n",
       "      <td>0.002430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009119</td>\n",
       "      <td>0.008931</td>\n",
       "      <td>0.007925</td>\n",
       "      <td>-0.022432</td>\n",
       "      <td>0.009433</td>\n",
       "      <td>0.008259</td>\n",
       "      <td>-0.005597</td>\n",
       "      <td>-0.006701</td>\n",
       "      <td>-0.012634</td>\n",
       "      <td>0.015962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jimmy_Smith</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.007756</td>\n",
       "      <td>-0.013257</td>\n",
       "      <td>-0.002573</td>\n",
       "      <td>0.009958</td>\n",
       "      <td>-0.001874</td>\n",
       "      <td>-0.003488</td>\n",
       "      <td>0.005286</td>\n",
       "      <td>0.018059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>-0.000217</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.008409</td>\n",
       "      <td>-0.015855</td>\n",
       "      <td>-0.001234</td>\n",
       "      <td>-0.013331</td>\n",
       "      <td>-0.005641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kenneth_Rubio</td>\n",
       "      <td>True</td>\n",
       "      <td>0.007522</td>\n",
       "      <td>-0.018704</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>-0.008999</td>\n",
       "      <td>-0.009502</td>\n",
       "      <td>-0.006239</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.022828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014636</td>\n",
       "      <td>0.010824</td>\n",
       "      <td>-0.005042</td>\n",
       "      <td>0.003140</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.009326</td>\n",
       "      <td>-0.018076</td>\n",
       "      <td>-0.001034</td>\n",
       "      <td>0.004514</td>\n",
       "      <td>0.003176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jordan_Simmons</td>\n",
       "      <td>False</td>\n",
       "      <td>0.014809</td>\n",
       "      <td>-0.020393</td>\n",
       "      <td>-0.005992</td>\n",
       "      <td>-0.000668</td>\n",
       "      <td>0.012844</td>\n",
       "      <td>-0.003438</td>\n",
       "      <td>-0.012427</td>\n",
       "      <td>-0.011591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006650</td>\n",
       "      <td>-0.001765</td>\n",
       "      <td>0.008347</td>\n",
       "      <td>-0.004738</td>\n",
       "      <td>0.016266</td>\n",
       "      <td>-0.004600</td>\n",
       "      <td>-0.006485</td>\n",
       "      <td>0.009370</td>\n",
       "      <td>-0.010399</td>\n",
       "      <td>0.010310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               name is_married    EMB_00    EMB_01    EMB_02    EMB_03  \\\n",
       "0     Jasmine_Young      False  0.016538 -0.008067 -0.007596 -0.000633   \n",
       "1  Jeffery_Robinson       True  0.017017 -0.022626  0.001806  0.000938   \n",
       "2   Steven_Sullivan      False  0.004450  0.000489 -0.007673 -0.012033   \n",
       "3      Jay_Williams      False  0.001083 -0.011676 -0.005207  0.008740   \n",
       "4     Benjamin_Beck       True -0.025749 -0.008956  0.011284  0.021791   \n",
       "5     Gregory_Gomez      False  0.006974 -0.003312 -0.005165 -0.011515   \n",
       "6       Mary_Harris       True  0.006794  0.006037 -0.024521 -0.009489   \n",
       "7       Jimmy_Smith      False -0.007756 -0.013257 -0.002573  0.009958   \n",
       "8     Kenneth_Rubio       True  0.007522 -0.018704 -0.000111 -0.008999   \n",
       "9    Jordan_Simmons      False  0.014809 -0.020393 -0.005992 -0.000668   \n",
       "\n",
       "     EMB_04    EMB_05    EMB_06    EMB_07  ...    EMB_54    EMB_55    EMB_56  \\\n",
       "0  0.001607  0.016212 -0.010445 -0.013139  ...  0.004342 -0.000188  0.007250   \n",
       "1  0.016354  0.010338  0.003199  0.007888  ...  0.001560  0.007903  0.003097   \n",
       "2  0.008483 -0.015631 -0.005434  0.002231  ... -0.008088 -0.006004  0.008902   \n",
       "3 -0.003923  0.001176  0.004290  0.016565  ...  0.002825 -0.000051  0.001242   \n",
       "4 -0.008500  0.007442 -0.025526 -0.010672  ... -0.001501 -0.005327  0.005365   \n",
       "5  0.005372  0.015446  0.012202  0.002737  ...  0.001540  0.001889  0.023558   \n",
       "6 -0.010266  0.009738 -0.007976  0.002430  ... -0.009119  0.008931  0.007925   \n",
       "7 -0.001874 -0.003488  0.005286  0.018059  ...  0.001500  0.001745  0.002381   \n",
       "8 -0.009502 -0.006239  0.000093  0.022828  ...  0.014636  0.010824 -0.005042   \n",
       "9  0.012844 -0.003438 -0.012427 -0.011591  ...  0.006650 -0.001765  0.008347   \n",
       "\n",
       "     EMB_57    EMB_58    EMB_59    EMB_60    EMB_61    EMB_62    EMB_63  \n",
       "0 -0.002752  0.000798 -0.004680 -0.004442  0.008832 -0.021160 -0.003933  \n",
       "1 -0.003179 -0.002239  0.000408 -0.013558  0.012339  0.011655  0.012075  \n",
       "2  0.003953  0.006039  0.003814  0.008377  0.000476 -0.001416 -0.003754  \n",
       "3 -0.001760 -0.002549  0.007215 -0.020126  0.001544 -0.009346 -0.005447  \n",
       "4  0.001235 -0.002955 -0.008059 -0.017144 -0.021904  0.002713  0.009757  \n",
       "5  0.006070  0.022578  0.000108  0.008357  0.007082 -0.003001  0.018339  \n",
       "6 -0.022432  0.009433  0.008259 -0.005597 -0.006701 -0.012634  0.015962  \n",
       "7 -0.000217  0.000012  0.008409 -0.015855 -0.001234 -0.013331 -0.005641  \n",
       "8  0.003140  0.010190  0.009326 -0.018076 -0.001034  0.004514  0.003176  \n",
       "9 -0.004738  0.016266 -0.004600 -0.006485  0.009370 -0.010399  0.010310  \n",
       "\n",
       "[10 rows x 66 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_graph, config_graph = alo.process_asset_step(alo.asset_source[pipeline][step], step, pipeline, data_input, config_input.copy(), [graph_args])\n",
    "# data_graph: graph asset의 결과물입니다. 다음 asset 실행 시 필요합니다. \n",
    "# config_graph: graph asset의 결과 config입니다. 다음 asset실행 시 필요합니다. \n",
    "\n",
    "# graph asset의 결과 dataframe은 data_graph['dataframe']으로 확인할 수 있습니다. \n",
    "data_graph['dataframe'].head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7698dd0-9153-46c0-97d8-6971ab8f3b2c",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### 2. Preprocess asset \n",
    "##### Preprocess asset의 args수정 및 확인\n",
    "- 필요한경우 preprocess_args의 항목을 ***preprocess_args[argument명]=value입력*** 을 통해 변경할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91e75b62-dd0c-441a-a8db-eb6d15a4345b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'handling_missing': 'interpolation',\n",
       " 'handling_encoding_y_column': 'is_married',\n",
       " 'handling_encoding_y': 'label',\n",
       " 'limit_encoding_categories': 30,\n",
       " 'load_train_preprocess': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - graph(1) - preprocess(2) - sampling(3) - train(4))\n",
    "step = 2 \n",
    "preprocess_args = alo.user_parameters[pipeline][step]['args'][0].copy()\n",
    "\n",
    "# 아래 주석을 풀어 preprocess_args 수정합니다. \n",
    "# preprocess_args['handling_missing'] = 'interpolation'\n",
    "preprocess_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d78a8-0a32-4f54-aac7-bff907e2ae94",
   "metadata": {},
   "source": [
    "##### Preprocess asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a116e5ee-0eaa-4cad-9e8b-b7452f7068be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m>> Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr/alo//.train_artifacts/models/preprocess/\u001b[0m\n",
      "************************************************************\n",
      "************************************************************\n",
      "is_married column : label Encoder saved : /home/jovyan/gcr/alo//.train_artifacts/models/preprocess/\n",
      "['EMB_00_nan', 'EMB_01_nan', 'EMB_02_nan', 'EMB_03_nan', 'EMB_04_nan', 'EMB_05_nan', 'EMB_06_nan', 'EMB_07_nan', 'EMB_08_nan', 'EMB_09_nan', 'EMB_10_nan', 'EMB_11_nan', 'EMB_12_nan', 'EMB_13_nan', 'EMB_14_nan', 'EMB_15_nan', 'EMB_16_nan', 'EMB_17_nan', 'EMB_18_nan', 'EMB_19_nan', 'EMB_20_nan', 'EMB_21_nan', 'EMB_22_nan', 'EMB_23_nan', 'EMB_24_nan', 'EMB_25_nan', 'EMB_26_nan', 'EMB_27_nan', 'EMB_28_nan', 'EMB_29_nan', 'EMB_30_nan', 'EMB_31_nan', 'EMB_32_nan', 'EMB_33_nan', 'EMB_34_nan', 'EMB_35_nan', 'EMB_36_nan', 'EMB_37_nan', 'EMB_38_nan', 'EMB_39_nan', 'EMB_40_nan', 'EMB_41_nan', 'EMB_42_nan', 'EMB_43_nan', 'EMB_44_nan', 'EMB_45_nan', 'EMB_46_nan', 'EMB_47_nan', 'EMB_48_nan', 'EMB_49_nan', 'EMB_50_nan', 'EMB_51_nan', 'EMB_52_nan', 'EMB_53_nan', 'EMB_54_nan', 'EMB_55_nan', 'EMB_56_nan', 'EMB_57_nan', 'EMB_58_nan', 'EMB_59_nan', 'EMB_60_nan', 'EMB_61_nan', 'EMB_62_nan', 'EMB_63_nan'] is_married_encoded_nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: No names found, cannot describe anything.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_00</th>\n",
       "      <th>EMB_01</th>\n",
       "      <th>EMB_02</th>\n",
       "      <th>EMB_03</th>\n",
       "      <th>EMB_04</th>\n",
       "      <th>EMB_05</th>\n",
       "      <th>EMB_06</th>\n",
       "      <th>EMB_07</th>\n",
       "      <th>EMB_08</th>\n",
       "      <th>EMB_09</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_55_nan</th>\n",
       "      <th>EMB_56_nan</th>\n",
       "      <th>EMB_57_nan</th>\n",
       "      <th>EMB_58_nan</th>\n",
       "      <th>EMB_59_nan</th>\n",
       "      <th>EMB_60_nan</th>\n",
       "      <th>EMB_61_nan</th>\n",
       "      <th>EMB_62_nan</th>\n",
       "      <th>EMB_63_nan</th>\n",
       "      <th>is_married_encoded_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.016538</td>\n",
       "      <td>-0.008067</td>\n",
       "      <td>-0.007596</td>\n",
       "      <td>-0.000633</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.016212</td>\n",
       "      <td>-0.010445</td>\n",
       "      <td>-0.013139</td>\n",
       "      <td>0.002467</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000188</td>\n",
       "      <td>0.007250</td>\n",
       "      <td>-0.002752</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>-0.004680</td>\n",
       "      <td>-0.004442</td>\n",
       "      <td>0.008832</td>\n",
       "      <td>-0.021160</td>\n",
       "      <td>-0.003933</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.017017</td>\n",
       "      <td>-0.022626</td>\n",
       "      <td>0.001806</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>0.016354</td>\n",
       "      <td>0.010338</td>\n",
       "      <td>0.003199</td>\n",
       "      <td>0.007888</td>\n",
       "      <td>0.013356</td>\n",
       "      <td>-0.004954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007903</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>-0.003179</td>\n",
       "      <td>-0.002239</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>-0.013558</td>\n",
       "      <td>0.012339</td>\n",
       "      <td>0.011655</td>\n",
       "      <td>0.012075</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004450</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>-0.007673</td>\n",
       "      <td>-0.012033</td>\n",
       "      <td>0.008483</td>\n",
       "      <td>-0.015631</td>\n",
       "      <td>-0.005434</td>\n",
       "      <td>0.002231</td>\n",
       "      <td>0.005285</td>\n",
       "      <td>0.005549</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006004</td>\n",
       "      <td>0.008902</td>\n",
       "      <td>0.003953</td>\n",
       "      <td>0.006039</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>-0.001416</td>\n",
       "      <td>-0.003754</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001083</td>\n",
       "      <td>-0.011676</td>\n",
       "      <td>-0.005207</td>\n",
       "      <td>0.008740</td>\n",
       "      <td>-0.003923</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.004290</td>\n",
       "      <td>0.016565</td>\n",
       "      <td>-0.005768</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>-0.001760</td>\n",
       "      <td>-0.002549</td>\n",
       "      <td>0.007215</td>\n",
       "      <td>-0.020126</td>\n",
       "      <td>0.001544</td>\n",
       "      <td>-0.009346</td>\n",
       "      <td>-0.005447</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.025749</td>\n",
       "      <td>-0.008956</td>\n",
       "      <td>0.011284</td>\n",
       "      <td>0.021791</td>\n",
       "      <td>-0.008500</td>\n",
       "      <td>0.007442</td>\n",
       "      <td>-0.025526</td>\n",
       "      <td>-0.010672</td>\n",
       "      <td>-0.001578</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005327</td>\n",
       "      <td>0.005365</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>-0.002955</td>\n",
       "      <td>-0.008059</td>\n",
       "      <td>-0.017144</td>\n",
       "      <td>-0.021904</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>0.009757</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.006974</td>\n",
       "      <td>-0.003312</td>\n",
       "      <td>-0.005165</td>\n",
       "      <td>-0.011515</td>\n",
       "      <td>0.005372</td>\n",
       "      <td>0.015446</td>\n",
       "      <td>0.012202</td>\n",
       "      <td>0.002737</td>\n",
       "      <td>-0.010678</td>\n",
       "      <td>0.018531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>0.023558</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>0.022578</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.008357</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>-0.003001</td>\n",
       "      <td>0.018339</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.006794</td>\n",
       "      <td>0.006037</td>\n",
       "      <td>-0.024521</td>\n",
       "      <td>-0.009489</td>\n",
       "      <td>-0.010266</td>\n",
       "      <td>0.009738</td>\n",
       "      <td>-0.007976</td>\n",
       "      <td>0.002430</td>\n",
       "      <td>-0.010220</td>\n",
       "      <td>-0.014123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008931</td>\n",
       "      <td>0.007925</td>\n",
       "      <td>-0.022432</td>\n",
       "      <td>0.009433</td>\n",
       "      <td>0.008259</td>\n",
       "      <td>-0.005597</td>\n",
       "      <td>-0.006701</td>\n",
       "      <td>-0.012634</td>\n",
       "      <td>0.015962</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.007756</td>\n",
       "      <td>-0.013257</td>\n",
       "      <td>-0.002573</td>\n",
       "      <td>0.009958</td>\n",
       "      <td>-0.001874</td>\n",
       "      <td>-0.003488</td>\n",
       "      <td>0.005286</td>\n",
       "      <td>0.018059</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>0.003831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>-0.000217</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.008409</td>\n",
       "      <td>-0.015855</td>\n",
       "      <td>-0.001234</td>\n",
       "      <td>-0.013331</td>\n",
       "      <td>-0.005641</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.007522</td>\n",
       "      <td>-0.018704</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>-0.008999</td>\n",
       "      <td>-0.009502</td>\n",
       "      <td>-0.006239</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.022828</td>\n",
       "      <td>-0.003850</td>\n",
       "      <td>-0.003091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010824</td>\n",
       "      <td>-0.005042</td>\n",
       "      <td>0.003140</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.009326</td>\n",
       "      <td>-0.018076</td>\n",
       "      <td>-0.001034</td>\n",
       "      <td>0.004514</td>\n",
       "      <td>0.003176</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.014809</td>\n",
       "      <td>-0.020393</td>\n",
       "      <td>-0.005992</td>\n",
       "      <td>-0.000668</td>\n",
       "      <td>0.012844</td>\n",
       "      <td>-0.003438</td>\n",
       "      <td>-0.012427</td>\n",
       "      <td>-0.011591</td>\n",
       "      <td>0.007965</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001765</td>\n",
       "      <td>0.008347</td>\n",
       "      <td>-0.004738</td>\n",
       "      <td>0.016266</td>\n",
       "      <td>-0.004600</td>\n",
       "      <td>-0.006485</td>\n",
       "      <td>0.009370</td>\n",
       "      <td>-0.010399</td>\n",
       "      <td>0.010310</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     EMB_00    EMB_01    EMB_02    EMB_03    EMB_04    EMB_05    EMB_06  \\\n",
       "0  0.016538 -0.008067 -0.007596 -0.000633  0.001607  0.016212 -0.010445   \n",
       "1  0.017017 -0.022626  0.001806  0.000938  0.016354  0.010338  0.003199   \n",
       "2  0.004450  0.000489 -0.007673 -0.012033  0.008483 -0.015631 -0.005434   \n",
       "3  0.001083 -0.011676 -0.005207  0.008740 -0.003923  0.001176  0.004290   \n",
       "4 -0.025749 -0.008956  0.011284  0.021791 -0.008500  0.007442 -0.025526   \n",
       "5  0.006974 -0.003312 -0.005165 -0.011515  0.005372  0.015446  0.012202   \n",
       "6  0.006794  0.006037 -0.024521 -0.009489 -0.010266  0.009738 -0.007976   \n",
       "7 -0.007756 -0.013257 -0.002573  0.009958 -0.001874 -0.003488  0.005286   \n",
       "8  0.007522 -0.018704 -0.000111 -0.008999 -0.009502 -0.006239  0.000093   \n",
       "9  0.014809 -0.020393 -0.005992 -0.000668  0.012844 -0.003438 -0.012427   \n",
       "\n",
       "     EMB_07    EMB_08    EMB_09  ...  EMB_55_nan  EMB_56_nan  EMB_57_nan  \\\n",
       "0 -0.013139  0.002467  0.002639  ...   -0.000188    0.007250   -0.002752   \n",
       "1  0.007888  0.013356 -0.004954  ...    0.007903    0.003097   -0.003179   \n",
       "2  0.002231  0.005285  0.005549  ...   -0.006004    0.008902    0.003953   \n",
       "3  0.016565 -0.005768  0.000586  ...   -0.000051    0.001242   -0.001760   \n",
       "4 -0.010672 -0.001578  0.000359  ...   -0.005327    0.005365    0.001235   \n",
       "5  0.002737 -0.010678  0.018531  ...    0.001889    0.023558    0.006070   \n",
       "6  0.002430 -0.010220 -0.014123  ...    0.008931    0.007925   -0.022432   \n",
       "7  0.018059  0.001619  0.003831  ...    0.001745    0.002381   -0.000217   \n",
       "8  0.022828 -0.003850 -0.003091  ...    0.010824   -0.005042    0.003140   \n",
       "9 -0.011591  0.007965  0.006985  ...   -0.001765    0.008347   -0.004738   \n",
       "\n",
       "   EMB_58_nan  EMB_59_nan  EMB_60_nan  EMB_61_nan  EMB_62_nan  EMB_63_nan  \\\n",
       "0    0.000798   -0.004680   -0.004442    0.008832   -0.021160   -0.003933   \n",
       "1   -0.002239    0.000408   -0.013558    0.012339    0.011655    0.012075   \n",
       "2    0.006039    0.003814    0.008377    0.000476   -0.001416   -0.003754   \n",
       "3   -0.002549    0.007215   -0.020126    0.001544   -0.009346   -0.005447   \n",
       "4   -0.002955   -0.008059   -0.017144   -0.021904    0.002713    0.009757   \n",
       "5    0.022578    0.000108    0.008357    0.007082   -0.003001    0.018339   \n",
       "6    0.009433    0.008259   -0.005597   -0.006701   -0.012634    0.015962   \n",
       "7    0.000012    0.008409   -0.015855   -0.001234   -0.013331   -0.005641   \n",
       "8    0.010190    0.009326   -0.018076   -0.001034    0.004514    0.003176   \n",
       "9    0.016266   -0.004600   -0.006485    0.009370   -0.010399    0.010310   \n",
       "\n",
       "   is_married_encoded_nan  \n",
       "0                       0  \n",
       "1                       1  \n",
       "2                       0  \n",
       "3                       0  \n",
       "4                       1  \n",
       "5                       0  \n",
       "6                       1  \n",
       "7                       0  \n",
       "8                       1  \n",
       "9                       0  \n",
       "\n",
       "[10 rows x 131 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocess, config_preprocess = alo.process_asset_step(alo.asset_source[pipeline][step], step, pipeline, data_graph, config_graph.copy(), [preprocess_args])\n",
    "# data_preprocess: preprocess asset의 결과물입니다. 다음 asset 실행 시 필요합니다. \n",
    "# config_preprocess: preprocess asset의 결과 config입니다. 다음 asset실행 시 필요합니다. \n",
    "\n",
    "# preprocess asset의 결과 dataframe은 data_preprocess['dataframe']으로 확인할 수 있습니다. \n",
    "data_preprocess['dataframe'].head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd463df-8ac2-4eeb-a020-49f89a05c5a1",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### 3. Sampling asset \n",
    "##### Sampling asset의 args수정 및 확인\n",
    "- 필요한경우 Sampling_args의 항목을 ***sampling_args[argument명]=value입력*** 을 통해 변경할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a20b2f60-8fb3-4115-9006-8bee0b3c5ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sampling_type': 'none',\n",
       " 'sampling_method': 'negative',\n",
       " 'label_sampling': True,\n",
       " 'ignore_label_class': 1,\n",
       " 'negative_target_class': None,\n",
       " 'label_sampling_num_type': 'compare',\n",
       " 'label_sampling_num': {1: 1, 0: 25},\n",
       " 'sampling_groupkey_columns': None,\n",
       " 'sampling_num_type': None,\n",
       " 'sampling_num': None}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - graph(1) - preprocess(2) - sampling(3) - train(4))\n",
    "step = 3 \n",
    "sampling_args = alo.user_parameters[pipeline][step]['args'][0].copy()\n",
    "\n",
    "# 아래 주석을 풀어 preprocess_args 수정합니다. \n",
    "# sampling_args['sampling_type'] = 'under'\n",
    "sampling_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4230191c-ec22-4e85-9a18-82817a90e2fb",
   "metadata": {},
   "source": [
    "##### Sampling asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "025f4f8c-f430-4c28-b8e0-e27e7fe1a0db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda/envs/gcr/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "************************************************************\n",
      "\u001b[92m>> Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr/alo//.train_artifacts/models/sampling/\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_00</th>\n",
       "      <th>EMB_01</th>\n",
       "      <th>EMB_02</th>\n",
       "      <th>EMB_03</th>\n",
       "      <th>EMB_04</th>\n",
       "      <th>EMB_05</th>\n",
       "      <th>EMB_06</th>\n",
       "      <th>EMB_07</th>\n",
       "      <th>EMB_08</th>\n",
       "      <th>EMB_09</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_55_nan</th>\n",
       "      <th>EMB_56_nan</th>\n",
       "      <th>EMB_57_nan</th>\n",
       "      <th>EMB_58_nan</th>\n",
       "      <th>EMB_59_nan</th>\n",
       "      <th>EMB_60_nan</th>\n",
       "      <th>EMB_61_nan</th>\n",
       "      <th>EMB_62_nan</th>\n",
       "      <th>EMB_63_nan</th>\n",
       "      <th>is_married_encoded_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.016538</td>\n",
       "      <td>-0.008067</td>\n",
       "      <td>-0.007596</td>\n",
       "      <td>-0.000633</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.016212</td>\n",
       "      <td>-0.010445</td>\n",
       "      <td>-0.013139</td>\n",
       "      <td>0.002467</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000188</td>\n",
       "      <td>0.007250</td>\n",
       "      <td>-0.002752</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>-0.004680</td>\n",
       "      <td>-0.004442</td>\n",
       "      <td>0.008832</td>\n",
       "      <td>-0.021160</td>\n",
       "      <td>-0.003933</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.017017</td>\n",
       "      <td>-0.022626</td>\n",
       "      <td>0.001806</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>0.016354</td>\n",
       "      <td>0.010338</td>\n",
       "      <td>0.003199</td>\n",
       "      <td>0.007888</td>\n",
       "      <td>0.013356</td>\n",
       "      <td>-0.004954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007903</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>-0.003179</td>\n",
       "      <td>-0.002239</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>-0.013558</td>\n",
       "      <td>0.012339</td>\n",
       "      <td>0.011655</td>\n",
       "      <td>0.012075</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004450</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>-0.007673</td>\n",
       "      <td>-0.012033</td>\n",
       "      <td>0.008483</td>\n",
       "      <td>-0.015631</td>\n",
       "      <td>-0.005434</td>\n",
       "      <td>0.002231</td>\n",
       "      <td>0.005285</td>\n",
       "      <td>0.005549</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006004</td>\n",
       "      <td>0.008902</td>\n",
       "      <td>0.003953</td>\n",
       "      <td>0.006039</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>-0.001416</td>\n",
       "      <td>-0.003754</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001083</td>\n",
       "      <td>-0.011676</td>\n",
       "      <td>-0.005207</td>\n",
       "      <td>0.008740</td>\n",
       "      <td>-0.003923</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.004290</td>\n",
       "      <td>0.016565</td>\n",
       "      <td>-0.005768</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>-0.001760</td>\n",
       "      <td>-0.002549</td>\n",
       "      <td>0.007215</td>\n",
       "      <td>-0.020126</td>\n",
       "      <td>0.001544</td>\n",
       "      <td>-0.009346</td>\n",
       "      <td>-0.005447</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.025749</td>\n",
       "      <td>-0.008956</td>\n",
       "      <td>0.011284</td>\n",
       "      <td>0.021791</td>\n",
       "      <td>-0.008500</td>\n",
       "      <td>0.007442</td>\n",
       "      <td>-0.025526</td>\n",
       "      <td>-0.010672</td>\n",
       "      <td>-0.001578</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005327</td>\n",
       "      <td>0.005365</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>-0.002955</td>\n",
       "      <td>-0.008059</td>\n",
       "      <td>-0.017144</td>\n",
       "      <td>-0.021904</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>0.009757</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.006974</td>\n",
       "      <td>-0.003312</td>\n",
       "      <td>-0.005165</td>\n",
       "      <td>-0.011515</td>\n",
       "      <td>0.005372</td>\n",
       "      <td>0.015446</td>\n",
       "      <td>0.012202</td>\n",
       "      <td>0.002737</td>\n",
       "      <td>-0.010678</td>\n",
       "      <td>0.018531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>0.023558</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>0.022578</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.008357</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>-0.003001</td>\n",
       "      <td>0.018339</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.006794</td>\n",
       "      <td>0.006037</td>\n",
       "      <td>-0.024521</td>\n",
       "      <td>-0.009489</td>\n",
       "      <td>-0.010266</td>\n",
       "      <td>0.009738</td>\n",
       "      <td>-0.007976</td>\n",
       "      <td>0.002430</td>\n",
       "      <td>-0.010220</td>\n",
       "      <td>-0.014123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008931</td>\n",
       "      <td>0.007925</td>\n",
       "      <td>-0.022432</td>\n",
       "      <td>0.009433</td>\n",
       "      <td>0.008259</td>\n",
       "      <td>-0.005597</td>\n",
       "      <td>-0.006701</td>\n",
       "      <td>-0.012634</td>\n",
       "      <td>0.015962</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.007756</td>\n",
       "      <td>-0.013257</td>\n",
       "      <td>-0.002573</td>\n",
       "      <td>0.009958</td>\n",
       "      <td>-0.001874</td>\n",
       "      <td>-0.003488</td>\n",
       "      <td>0.005286</td>\n",
       "      <td>0.018059</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>0.003831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>-0.000217</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.008409</td>\n",
       "      <td>-0.015855</td>\n",
       "      <td>-0.001234</td>\n",
       "      <td>-0.013331</td>\n",
       "      <td>-0.005641</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.007522</td>\n",
       "      <td>-0.018704</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>-0.008999</td>\n",
       "      <td>-0.009502</td>\n",
       "      <td>-0.006239</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.022828</td>\n",
       "      <td>-0.003850</td>\n",
       "      <td>-0.003091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010824</td>\n",
       "      <td>-0.005042</td>\n",
       "      <td>0.003140</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.009326</td>\n",
       "      <td>-0.018076</td>\n",
       "      <td>-0.001034</td>\n",
       "      <td>0.004514</td>\n",
       "      <td>0.003176</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.014809</td>\n",
       "      <td>-0.020393</td>\n",
       "      <td>-0.005992</td>\n",
       "      <td>-0.000668</td>\n",
       "      <td>0.012844</td>\n",
       "      <td>-0.003438</td>\n",
       "      <td>-0.012427</td>\n",
       "      <td>-0.011591</td>\n",
       "      <td>0.007965</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001765</td>\n",
       "      <td>0.008347</td>\n",
       "      <td>-0.004738</td>\n",
       "      <td>0.016266</td>\n",
       "      <td>-0.004600</td>\n",
       "      <td>-0.006485</td>\n",
       "      <td>0.009370</td>\n",
       "      <td>-0.010399</td>\n",
       "      <td>0.010310</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     EMB_00    EMB_01    EMB_02    EMB_03    EMB_04    EMB_05    EMB_06  \\\n",
       "0  0.016538 -0.008067 -0.007596 -0.000633  0.001607  0.016212 -0.010445   \n",
       "1  0.017017 -0.022626  0.001806  0.000938  0.016354  0.010338  0.003199   \n",
       "2  0.004450  0.000489 -0.007673 -0.012033  0.008483 -0.015631 -0.005434   \n",
       "3  0.001083 -0.011676 -0.005207  0.008740 -0.003923  0.001176  0.004290   \n",
       "4 -0.025749 -0.008956  0.011284  0.021791 -0.008500  0.007442 -0.025526   \n",
       "5  0.006974 -0.003312 -0.005165 -0.011515  0.005372  0.015446  0.012202   \n",
       "6  0.006794  0.006037 -0.024521 -0.009489 -0.010266  0.009738 -0.007976   \n",
       "7 -0.007756 -0.013257 -0.002573  0.009958 -0.001874 -0.003488  0.005286   \n",
       "8  0.007522 -0.018704 -0.000111 -0.008999 -0.009502 -0.006239  0.000093   \n",
       "9  0.014809 -0.020393 -0.005992 -0.000668  0.012844 -0.003438 -0.012427   \n",
       "\n",
       "     EMB_07    EMB_08    EMB_09  ...  EMB_55_nan  EMB_56_nan  EMB_57_nan  \\\n",
       "0 -0.013139  0.002467  0.002639  ...   -0.000188    0.007250   -0.002752   \n",
       "1  0.007888  0.013356 -0.004954  ...    0.007903    0.003097   -0.003179   \n",
       "2  0.002231  0.005285  0.005549  ...   -0.006004    0.008902    0.003953   \n",
       "3  0.016565 -0.005768  0.000586  ...   -0.000051    0.001242   -0.001760   \n",
       "4 -0.010672 -0.001578  0.000359  ...   -0.005327    0.005365    0.001235   \n",
       "5  0.002737 -0.010678  0.018531  ...    0.001889    0.023558    0.006070   \n",
       "6  0.002430 -0.010220 -0.014123  ...    0.008931    0.007925   -0.022432   \n",
       "7  0.018059  0.001619  0.003831  ...    0.001745    0.002381   -0.000217   \n",
       "8  0.022828 -0.003850 -0.003091  ...    0.010824   -0.005042    0.003140   \n",
       "9 -0.011591  0.007965  0.006985  ...   -0.001765    0.008347   -0.004738   \n",
       "\n",
       "   EMB_58_nan  EMB_59_nan  EMB_60_nan  EMB_61_nan  EMB_62_nan  EMB_63_nan  \\\n",
       "0    0.000798   -0.004680   -0.004442    0.008832   -0.021160   -0.003933   \n",
       "1   -0.002239    0.000408   -0.013558    0.012339    0.011655    0.012075   \n",
       "2    0.006039    0.003814    0.008377    0.000476   -0.001416   -0.003754   \n",
       "3   -0.002549    0.007215   -0.020126    0.001544   -0.009346   -0.005447   \n",
       "4   -0.002955   -0.008059   -0.017144   -0.021904    0.002713    0.009757   \n",
       "5    0.022578    0.000108    0.008357    0.007082   -0.003001    0.018339   \n",
       "6    0.009433    0.008259   -0.005597   -0.006701   -0.012634    0.015962   \n",
       "7    0.000012    0.008409   -0.015855   -0.001234   -0.013331   -0.005641   \n",
       "8    0.010190    0.009326   -0.018076   -0.001034    0.004514    0.003176   \n",
       "9    0.016266   -0.004600   -0.006485    0.009370   -0.010399    0.010310   \n",
       "\n",
       "   is_married_encoded_nan  \n",
       "0                       0  \n",
       "1                       1  \n",
       "2                       0  \n",
       "3                       0  \n",
       "4                       1  \n",
       "5                       0  \n",
       "6                       1  \n",
       "7                       0  \n",
       "8                       1  \n",
       "9                       0  \n",
       "\n",
       "[10 rows x 131 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sampling, config_sampling = alo.process_asset_step(alo.asset_source[pipeline][step], step, pipeline, data_preprocess, config_preprocess.copy(), [sampling_args])\n",
    "# data_sampling: sampling asset의 결과물입니다. 다음 asset 실행 시 필요합니다. \n",
    "# config_sampling: sampling asset의 결과 config입니다. 다음 asset실행 시 필요합니다. \n",
    "\n",
    "# sampling asset의 결과 dataframe은 data_sampling['dataframe']으로 확인할 수 있습니다. \n",
    "data_sampling['dataframe'].head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6a3a7-2204-42b5-8cc8-a5b5f740f457",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### 4. Train asset \n",
    "##### Train asset의 args수정 및 확인\n",
    "- 필요한경우 train_args의 항목을 ***train_args[argument명]=value입력*** 을 통해 변경할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39f0149f-bfb4-4223-835a-769fd57594bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_type': 'classification',\n",
       " 'data_split_method': 'cross_validate',\n",
       " 'evaluation_metric': 'accuracy',\n",
       " 'model_list': ['lgb', 'rf', 'gbm', 'cb'],\n",
       " 'num_hpo': 3,\n",
       " 'param_range': {'rf': {'max_depth': 6, 'n_estimators': [300, 500]},\n",
       "  'gbm': {'max_depth': [5, 7], 'n_estimators': [300, 500]},\n",
       "  'ngb': {'col_sample': [0.6, 0.8], 'n_estimators': [100, 300]},\n",
       "  'lgb': {'max_depth': [5, 9], 'n_estimators': [300, 500]},\n",
       "  'cb': {'max_depth': [5, 9], 'n_estimators': [100, 500]}},\n",
       " 'shap_ratio': 1.0,\n",
       " 'evaluation_report': False}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR asset 순서에 따라 step 순서를 입력합니다. (input(0) - graph(1) - preprocess(2) - sampling(3) - train(4))\n",
    "step = 4 \n",
    "tcr_args = alo.user_parameters[pipeline][step]['args'][0].copy()\n",
    "\n",
    "# 아래 주석을 풀어 tcr_args를 수정합니다. \n",
    "# tcr_args['model_list'] = ['lgb']\n",
    "tcr_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c0b28-561d-4815-b4fc-f2078e1a0f59",
   "metadata": {},
   "source": [
    "##### Train asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c2a5dcd-b0b8-455d-8b68-3483238a5aae",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "************************************************************\n",
      "\u001b[92m>> Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr/alo//.train_artifacts/models/train/\u001b[0m\n",
      "\u001b[92m>> Successfully got << output path >> for saving your data into csv or jpg file: \n",
      " /home/jovyan/gcr/alo//.train_artifacts/output/train/ \n",
      " L [NOTE] The names of output file must be fixed as << output.csv, output.jpg >> \u001b[0m\n",
      "\u001b[92m>> Successfully got << report path >> for saving your << report.html >> file: \n",
      " /home/jovyan/gcr/alo//.train_artifacts/report/\u001b[0m\n",
      "해당 column 은 Training 과정에 사용되지 않습니다. (column_name: ['EMB_38', 'EMB_20', 'EMB_18', 'EMB_16', 'EMB_58', 'EMB_36', 'EMB_60', 'EMB_42', 'EMB_56', 'EMB_04', 'EMB_09', 'EMB_54', 'is_married', 'EMB_55', 'EMB_08', 'EMB_52', 'EMB_22', 'EMB_37', 'EMB_43', 'EMB_59', 'EMB_39', 'EMB_26', 'EMB_62', 'EMB_07', 'EMB_45', 'EMB_25', 'EMB_11', 'EMB_12', 'EMB_06', 'EMB_23', 'EMB_29', 'EMB_47', 'EMB_15', 'EMB_61', 'EMB_19', 'EMB_49', 'EMB_17', 'EMB_53', 'EMB_30', 'EMB_32', 'EMB_27', 'EMB_40', 'EMB_31', 'EMB_48', 'EMB_02', 'EMB_57', 'EMB_14', 'EMB_46', 'is_married_encoded_nan', 'EMB_63', 'EMB_01', 'EMB_13', 'is_married_encoded', 'EMB_33', 'EMB_21', 'EMB_03', 'EMB_51', 'EMB_05', 'EMB_41', 'EMB_00', 'EMB_44', 'EMB_34', 'EMB_28', 'EMB_10', 'EMB_35', 'EMB_50', 'EMB_24'])\n",
      " Y 칼럼 ( ['y0'] )의 데이터 타입은 dict_values([dtype('int64')]) 입니다. 분석을 위해, Y 칼럼의 타입이 'int' 혹은 'float' 이어야 합니다.                 데이터 중 Y 칼럼의 모든 값이 숫자로 구성되어 있는지 확인해주세요. 만약, Y 칼럼이 카테고리 칼럼이라면, Preprocess 단계에서 해당 칼럼을 숫자로 변환하는 과정이 필요합니다.\n",
      "[INFO] 모델 학습을 시작합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 0th-fold RandomForestClassifier_set0 모델을 학습합니다.(1/48)\n",
      "[INFO] 1th-fold RandomForestClassifier_set0 모델을 학습합니다.(2/48)\n",
      "[INFO] 2th-fold RandomForestClassifier_set0 모델을 학습합니다.(3/48)\n",
      "[INFO] 3th-fold RandomForestClassifier_set0 모델을 학습합니다.(4/48)\n",
      "[INFO] 0th-fold RandomForestClassifier_set1 모델을 학습합니다.(5/48)\n",
      "[INFO] 1th-fold RandomForestClassifier_set1 모델을 학습합니다.(6/48)\n",
      "[INFO] 2th-fold RandomForestClassifier_set1 모델을 학습합니다.(7/48)\n",
      "[INFO] 3th-fold RandomForestClassifier_set1 모델을 학습합니다.(8/48)\n",
      "[INFO] 0th-fold RandomForestClassifier_set2 모델을 학습합니다.(9/48)\n",
      "[INFO] 1th-fold RandomForestClassifier_set2 모델을 학습합니다.(10/48)\n",
      "[INFO] 2th-fold RandomForestClassifier_set2 모델을 학습합니다.(11/48)\n",
      "[INFO] 3th-fold RandomForestClassifier_set2 모델을 학습합니다.(12/48)\n",
      "[INFO] 0th-fold GradientBoostingClassifier_set0 모델을 학습합니다.(13/48)\n",
      "[INFO] 1th-fold GradientBoostingClassifier_set0 모델을 학습합니다.(14/48)\n",
      "[INFO] 2th-fold GradientBoostingClassifier_set0 모델을 학습합니다.(15/48)\n",
      "[INFO] 3th-fold GradientBoostingClassifier_set0 모델을 학습합니다.(16/48)\n",
      "[INFO] 0th-fold GradientBoostingClassifier_set1 모델을 학습합니다.(17/48)\n",
      "[INFO] 1th-fold GradientBoostingClassifier_set1 모델을 학습합니다.(18/48)\n",
      "[INFO] 2th-fold GradientBoostingClassifier_set1 모델을 학습합니다.(19/48)\n",
      "[INFO] 3th-fold GradientBoostingClassifier_set1 모델을 학습합니다.(20/48)\n",
      "[INFO] 0th-fold GradientBoostingClassifier_set2 모델을 학습합니다.(21/48)\n",
      "[INFO] 1th-fold GradientBoostingClassifier_set2 모델을 학습합니다.(22/48)\n",
      "[INFO] 2th-fold GradientBoostingClassifier_set2 모델을 학습합니다.(23/48)\n",
      "[INFO] 3th-fold GradientBoostingClassifier_set2 모델을 학습합니다.(24/48)\n",
      "[INFO] 0th-fold LGBMClassifier_set0 모델을 학습합니다.(25/48)\n",
      "[INFO] 1th-fold LGBMClassifier_set0 모델을 학습합니다.(26/48)\n",
      "[INFO] 2th-fold LGBMClassifier_set0 모델을 학습합니다.(27/48)\n",
      "[INFO] 3th-fold LGBMClassifier_set0 모델을 학습합니다.(28/48)\n",
      "[INFO] 0th-fold LGBMClassifier_set1 모델을 학습합니다.(29/48)\n",
      "[INFO] 1th-fold LGBMClassifier_set1 모델을 학습합니다.(30/48)\n",
      "[INFO] 2th-fold LGBMClassifier_set1 모델을 학습합니다.(31/48)\n",
      "[INFO] 3th-fold LGBMClassifier_set1 모델을 학습합니다.(32/48)\n",
      "[INFO] 0th-fold LGBMClassifier_set2 모델을 학습합니다.(33/48)\n",
      "[INFO] 1th-fold LGBMClassifier_set2 모델을 학습합니다.(34/48)\n",
      "[INFO] 2th-fold LGBMClassifier_set2 모델을 학습합니다.(35/48)\n",
      "[INFO] 3th-fold LGBMClassifier_set2 모델을 학습합니다.(36/48)\n",
      "[INFO] 0th-fold CatBoostClassifier_set0 모델을 학습합니다.(37/48)\n",
      "[INFO] 1th-fold CatBoostClassifier_set0 모델을 학습합니다.(38/48)\n",
      "[INFO] 2th-fold CatBoostClassifier_set0 모델을 학습합니다.(39/48)\n",
      "[INFO] 3th-fold CatBoostClassifier_set0 모델을 학습합니다.(40/48)\n",
      "[INFO] 0th-fold CatBoostClassifier_set1 모델을 학습합니다.(41/48)\n",
      "[INFO] 1th-fold CatBoostClassifier_set1 모델을 학습합니다.(42/48)\n",
      "[INFO] 2th-fold CatBoostClassifier_set1 모델을 학습합니다.(43/48)\n",
      "[INFO] 3th-fold CatBoostClassifier_set1 모델을 학습합니다.(44/48)\n",
      "[INFO] 0th-fold CatBoostClassifier_set2 모델을 학습합니다.(45/48)\n",
      "[INFO] 1th-fold CatBoostClassifier_set2 모델을 학습합니다.(46/48)\n",
      "[INFO] 2th-fold CatBoostClassifier_set2 모델을 학습합니다.(47/48)\n",
      "[INFO] 3th-fold CatBoostClassifier_set2 모델을 학습합니다.(48/48)\n",
      "@scoring_classification func. - label list: @scoring_classification func. - label list:  @scoring_classification func. - label list: @scoring_classification func. - label list: @scoring_classification func. - label list: @scoring_classification func. - label list:  @scoring_classification func. - label list: @scoring_classification func. - label list: @scoring_classification func. - label list: @scoring_classification func. - label list:   @scoring_classification func. - label list:  {0, 1}{0, 1} {0, 1} {0, 1} \n",
      "{0, 1} {0, 1}\n",
      "{0, 1} \n",
      "\n",
      "{0, 1}\n",
      " {0, 1}\n",
      "\n",
      "@scoring_classification func. - label list: {0, 1}\n",
      " \n",
      "{0, 1}{0, 1}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 평가 지표는 ( accuracy ) 를 사용합니다. \n",
      "모델 정보 로그를 저장합니다. (저장위치: /home/jovyan/gcr/alo//.train_artifacts/models/train/model_selection.json)\n",
      "모델 파일을 저장합니다. (저장위치: /home/jovyan/gcr/alo//.train_artifacts/models/train/best_model_top0.pkl)\n",
      "모델 파일을 저장합니다. (저장위치: /home/jovyan/gcr/alo//.train_artifacts/models/train/best_model_top1.pkl)\n",
      "모델 파일을 저장합니다. (저장위치: /home/jovyan/gcr/alo//.train_artifacts/models/train/best_model_top2.pkl)\n",
      "[INFO] Summary_plot for Train data 를 저장했습니다.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_00</th>\n",
       "      <th>EMB_01</th>\n",
       "      <th>EMB_02</th>\n",
       "      <th>EMB_03</th>\n",
       "      <th>EMB_04</th>\n",
       "      <th>EMB_05</th>\n",
       "      <th>EMB_06</th>\n",
       "      <th>EMB_07</th>\n",
       "      <th>EMB_08</th>\n",
       "      <th>EMB_09</th>\n",
       "      <th>...</th>\n",
       "      <th>x61_shapley</th>\n",
       "      <th>x62_shapley</th>\n",
       "      <th>x63_shapley</th>\n",
       "      <th>y0</th>\n",
       "      <th>y0_pred</th>\n",
       "      <th>y0_pred_best0</th>\n",
       "      <th>y0_pred_best1</th>\n",
       "      <th>y0_pred_best2</th>\n",
       "      <th>y0_prob</th>\n",
       "      <th>y1_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003840</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>-0.007967</td>\n",
       "      <td>-0.002356</td>\n",
       "      <td>-0.005350</td>\n",
       "      <td>0.013863</td>\n",
       "      <td>0.006603</td>\n",
       "      <td>0.006003</td>\n",
       "      <td>-0.013113</td>\n",
       "      <td>0.019675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>-0.006309</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.686593</td>\n",
       "      <td>0.313407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.009052</td>\n",
       "      <td>0.013082</td>\n",
       "      <td>0.021508</td>\n",
       "      <td>0.015092</td>\n",
       "      <td>-0.004068</td>\n",
       "      <td>0.019373</td>\n",
       "      <td>0.013463</td>\n",
       "      <td>-0.003814</td>\n",
       "      <td>-0.032021</td>\n",
       "      <td>0.014427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002812</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>-0.000661</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.590732</td>\n",
       "      <td>0.409268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004987</td>\n",
       "      <td>0.008661</td>\n",
       "      <td>-0.008990</td>\n",
       "      <td>0.018038</td>\n",
       "      <td>0.014452</td>\n",
       "      <td>-0.014352</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.051285</td>\n",
       "      <td>-0.013601</td>\n",
       "      <td>0.028915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001597</td>\n",
       "      <td>0.002254</td>\n",
       "      <td>-0.003488</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.674853</td>\n",
       "      <td>0.325147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001998</td>\n",
       "      <td>-0.016198</td>\n",
       "      <td>-0.005150</td>\n",
       "      <td>-0.003461</td>\n",
       "      <td>-0.019599</td>\n",
       "      <td>-0.006972</td>\n",
       "      <td>-0.004956</td>\n",
       "      <td>-0.007177</td>\n",
       "      <td>-0.003454</td>\n",
       "      <td>-0.023739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>0.002890</td>\n",
       "      <td>-0.003445</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.519094</td>\n",
       "      <td>0.480906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.020020</td>\n",
       "      <td>0.011530</td>\n",
       "      <td>-0.007236</td>\n",
       "      <td>-0.015067</td>\n",
       "      <td>-0.009421</td>\n",
       "      <td>-0.005364</td>\n",
       "      <td>-0.023002</td>\n",
       "      <td>-0.000690</td>\n",
       "      <td>0.002372</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>0.001335</td>\n",
       "      <td>0.005789</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.814324</td>\n",
       "      <td>0.185676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.011804</td>\n",
       "      <td>0.015293</td>\n",
       "      <td>-0.000414</td>\n",
       "      <td>0.014433</td>\n",
       "      <td>-0.003025</td>\n",
       "      <td>-0.006060</td>\n",
       "      <td>0.012435</td>\n",
       "      <td>-0.008691</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>-0.017985</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.771844</td>\n",
       "      <td>0.228156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.015265</td>\n",
       "      <td>-0.004064</td>\n",
       "      <td>-0.008829</td>\n",
       "      <td>-0.002126</td>\n",
       "      <td>-0.009262</td>\n",
       "      <td>0.016647</td>\n",
       "      <td>0.002565</td>\n",
       "      <td>-0.003330</td>\n",
       "      <td>0.007537</td>\n",
       "      <td>-0.007018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.007688</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.766870</td>\n",
       "      <td>0.233130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.015778</td>\n",
       "      <td>-0.011173</td>\n",
       "      <td>0.014553</td>\n",
       "      <td>-0.001650</td>\n",
       "      <td>-0.011929</td>\n",
       "      <td>0.005844</td>\n",
       "      <td>-0.003917</td>\n",
       "      <td>0.007704</td>\n",
       "      <td>-0.002859</td>\n",
       "      <td>-0.006877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.023057</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.594031</td>\n",
       "      <td>0.405969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.008968</td>\n",
       "      <td>0.006837</td>\n",
       "      <td>-0.002328</td>\n",
       "      <td>0.014541</td>\n",
       "      <td>0.006340</td>\n",
       "      <td>0.003762</td>\n",
       "      <td>-0.009290</td>\n",
       "      <td>0.002642</td>\n",
       "      <td>-0.002153</td>\n",
       "      <td>0.007673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>-0.006740</td>\n",
       "      <td>-0.001670</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.624811</td>\n",
       "      <td>0.375189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.005291</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>0.004244</td>\n",
       "      <td>0.013976</td>\n",
       "      <td>-0.007684</td>\n",
       "      <td>-0.023415</td>\n",
       "      <td>0.007039</td>\n",
       "      <td>-0.012200</td>\n",
       "      <td>-0.012685</td>\n",
       "      <td>-0.014295</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>0.005918</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.700137</td>\n",
       "      <td>0.299863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 203 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     EMB_00    EMB_01    EMB_02    EMB_03    EMB_04    EMB_05    EMB_06  \\\n",
       "0 -0.003840  0.002288 -0.007967 -0.002356 -0.005350  0.013863  0.006603   \n",
       "1 -0.009052  0.013082  0.021508  0.015092 -0.004068  0.019373  0.013463   \n",
       "2  0.004987  0.008661 -0.008990  0.018038  0.014452 -0.014352  0.019380   \n",
       "3  0.001998 -0.016198 -0.005150 -0.003461 -0.019599 -0.006972 -0.004956   \n",
       "4 -0.020020  0.011530 -0.007236 -0.015067 -0.009421 -0.005364 -0.023002   \n",
       "5 -0.011804  0.015293 -0.000414  0.014433 -0.003025 -0.006060  0.012435   \n",
       "6  0.015265 -0.004064 -0.008829 -0.002126 -0.009262  0.016647  0.002565   \n",
       "7 -0.015778 -0.011173  0.014553 -0.001650 -0.011929  0.005844 -0.003917   \n",
       "8  0.008968  0.006837 -0.002328  0.014541  0.006340  0.003762 -0.009290   \n",
       "9  0.005291 -0.000264  0.004244  0.013976 -0.007684 -0.023415  0.007039   \n",
       "\n",
       "     EMB_07    EMB_08    EMB_09  ...  x61_shapley  x62_shapley  x63_shapley  \\\n",
       "0  0.006003 -0.013113  0.019675  ...     0.001273     0.001753    -0.006309   \n",
       "1 -0.003814 -0.032021  0.014427  ...     0.002812     0.002801    -0.000661   \n",
       "2  0.051285 -0.013601  0.028915  ...    -0.001597     0.002254    -0.003488   \n",
       "3 -0.007177 -0.003454 -0.023739  ...    -0.000064     0.002890    -0.003445   \n",
       "4 -0.000690  0.002372  0.001262  ...     0.002253     0.001335     0.005789   \n",
       "5 -0.008691  0.002073 -0.017985  ...    -0.000014     0.000366     0.000861   \n",
       "6 -0.003330  0.007537 -0.007018  ...     0.000022    -0.000100    -0.007688   \n",
       "7  0.007704 -0.002859 -0.006877  ...     0.000617    -0.023057     0.000237   \n",
       "8  0.002642 -0.002153  0.007673  ...     0.001483    -0.006740    -0.001670   \n",
       "9 -0.012200 -0.012685 -0.014295  ...    -0.000028     0.001810     0.005918   \n",
       "\n",
       "   y0  y0_pred  y0_pred_best0  y0_pred_best1  y0_pred_best2   y0_prob  \\\n",
       "0   0        0              0              0              0  0.686593   \n",
       "1   0        0              0              0              0  0.590732   \n",
       "2   0        0              0              0              0  0.674853   \n",
       "3   0        0              0              0              0  0.519094   \n",
       "4   0        0              0              0              0  0.814324   \n",
       "5   1        0              0              0              0  0.771844   \n",
       "6   1        0              0              0              0  0.766870   \n",
       "7   1        0              0              0              0  0.594031   \n",
       "8   0        0              0              0              0  0.624811   \n",
       "9   0        0              0              0              0  0.700137   \n",
       "\n",
       "    y1_prob  \n",
       "0  0.313407  \n",
       "1  0.409268  \n",
       "2  0.325147  \n",
       "3  0.480906  \n",
       "4  0.185676  \n",
       "5  0.228156  \n",
       "6  0.233130  \n",
       "7  0.405969  \n",
       "8  0.375189  \n",
       "9  0.299863  \n",
       "\n",
       "[10 rows x 203 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_tcr, config_tcr = alo.process_asset_step(alo.asset_source[pipeline][step], step, pipeline, data_sampling, config_sampling.copy(), [tcr_args])\n",
    "# data_tcr: TCR asset의 결과물입니다. \n",
    "# config_tcr: TCR asset의 결과 config입니다. \n",
    "\n",
    "# tcr asset의 결과 dataframe은 data_tcr['dataframe']으로 확인할 수 있습니다. \n",
    "data_tcr['dataframe'].head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f82f0-1e6c-4af5-b842-78d428c1e6f3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Inference workflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e052e1b-3478-47f8-8c6d-62aa4474b591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m[INFO] You did not write any << s3_private_key_file >> in the config yaml file. When you wanna get data from s3 storage, \n",
      " you have to write the s3_private_key_file path or set << ACCESS_KEY, SECRET_KEY >> in your os environment. \n",
      "\u001b[0m\n",
      "\u001b[94m>> Start setting-up << input >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[93m[NOTICE] << input >> asset had already been created at 2023-10-26 07:00:57.015663\u001b[0m\n",
      "\u001b[94m>> Start setting-up << preprocess >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[92m@ local asset_source_code mode: <preprocess> asset exists.\u001b[0m\n",
      "\u001b[94m>> Start setting-up << inference >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m>> Start renewing asset : /home/jovyan/gcr/alo/assets/inference\u001b[0m\n",
      "\u001b[92m/home/jovyan/gcr/alo/assets/inference successfully pulled.\u001b[0m\n",
      "\u001b[94m>> Start setting-up << result >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m>> Start renewing asset : /home/jovyan/gcr/alo/assets/result\u001b[0m\n",
      "\u001b[92m/home/jovyan/gcr/alo/assets/result successfully pulled.\u001b[0m\n",
      "\u001b[93m>> Ignored installing << pandas==1.5.3 >>. Another version will be installed in the previous step.\u001b[0m\n",
      "\u001b[93m>> Ignored installing << pandas==1.5.3 >>. Another version will be installed in the previous step.\u001b[0m\n",
      "\u001b[93m>> Ignored installing << pandas==1.5.3 >>. Another version will be installed in the previous step.\u001b[0m\n",
      "\u001b[94m======================================== Start dependency installation : << input >> \u001b[0m\n",
      "\u001b[93m>> Start checking existence & installing package - pandas==1.5.3 | Progress: ( 1 / 1 total packages )\u001b[0m\n",
      "\u001b[92m- << pandas==1.5.3 >> already exists\u001b[0m\n",
      "\u001b[94m======================================== Start dependency installation : << force-reinstall >> \u001b[0m\n",
      "\u001b[94m======================================== Finish dependency installation \n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 아래는 Inference 시 필요한 라이브러리를 설치하는 코드입니다. library 설치 에러가 발생하면 아래 셀을 재실행 해주세요\n",
    "external_load_data(pipelines[1], alo.external_path, alo.external_path_permission, alo.control['get_external_data'])\n",
    "pipeline = pipelines[1]\n",
    "alo.install_steps(pipeline, alo.control[\"get_asset_source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f681c8a8-ffde-4f59-a694-1b3849b90dd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 0. Input asset \n",
    "##### Input asset의 arguments 수정 및 확인\n",
    "- 필요한경우 input_args의 항목을 ***input_args[argument명]=value입력*** 을 통해 변경할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2f43e8e-5a9a-4cbc-b2b5-f548d92b3feb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_path': 'inference',\n",
       " 'x_columns': None,\n",
       " 'use_all_x': True,\n",
       " 'y_column': None,\n",
       " 'groupkey_columns': None,\n",
       " 'drop_columns': None,\n",
       " 'time_column': None}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR inference asset 순서에 따라 step 순서를 입력합니다. (input(0) - preprocess(1) - inference(2) - result(3))\n",
    "step = 0 \n",
    "input_args = alo.user_parameters[pipeline][step]['args'][0].copy()\n",
    "\n",
    "# 아래 주석을 풀어 input_args를 원하는 값으로 수정합니다. \n",
    "# input_args['x_columns'] = ['']\n",
    "input_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705ac75-11a8-4093-9531-3b09de8fc38e",
   "metadata": {},
   "source": [
    "##### Input asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "026f2355-99c7-437f-8274-dbed7aa968d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m==================== current time : 2023-10-26 07:22:53.172617+00:00 (UTC) ====================\u001b[0m\n",
      "************************************************************\n",
      "************************************************************\n",
      "\u001b[93m>> Load path : ['/home/jovyan/gcr/alo//input/inference/']\u001b[0m\n",
      "\u001b[92m>> The file for batch data has been loaded. (File name: /home/jovyan/gcr/alo//input/inference/inference.csv)\u001b[0m\n",
      "\u001b[93mYou set the << use_all_x >> as << True >> in the yaml file. So skip checking dataframe columns existence.\u001b[0m\n",
      "\u001b[92m==================== Success loading dataframe ====================\u001b[0m\n",
      "\u001b[94m>> Start processing ignore columns & drop columns: ['/home/jovyan/gcr/alo//input/inference/inference.csv']\u001b[0m\n",
      "\u001b[93m>> You set the << use_all_x >> parameter as << True >> in your config yaml. (So, these x_columns are used: ['EMB_38', 'EMB_20', 'EMB_18', 'EMB_16', 'EMB_58', 'EMB_36', 'EMB_60', 'EMB_42', 'EMB_56', 'EMB_04', 'EMB_09', 'EMB_54', 'EMB_55', 'EMB_08', 'EMB_52', 'EMB_22', 'EMB_37', 'EMB_43', 'EMB_59', 'EMB_39', 'EMB_26', 'EMB_62', 'EMB_07', 'EMB_45', 'EMB_25', 'EMB_11', 'EMB_12', 'EMB_06', 'EMB_23', 'EMB_29', 'EMB_47', 'EMB_61', 'EMB_15', 'EMB_19', 'EMB_49', 'EMB_17', 'EMB_53', 'EMB_30', 'EMB_32', 'EMB_27', 'EMB_40', 'EMB_31', 'EMB_48', 'EMB_02', 'EMB_14', 'EMB_57', 'EMB_46', 'EMB_63', 'EMB_01', 'EMB_13', 'EMB_33', 'EMB_21', 'EMB_03', 'EMB_51', 'EMB_05', 'EMB_41', 'EMB_00', 'EMB_44', 'EMB_34', 'EMB_28', 'EMB_10', 'EMB_35', 'EMB_50', 'EMB_24'] )\u001b[0m\n",
      "['EMB_38', 'EMB_20', 'EMB_18', 'EMB_16', 'EMB_58', 'EMB_36', 'EMB_60', 'EMB_42', 'EMB_56', 'EMB_04', 'EMB_09', 'EMB_54', 'EMB_55', 'EMB_08', 'EMB_52', 'EMB_22', 'EMB_37', 'EMB_43', 'EMB_59', 'EMB_39', 'EMB_26', 'EMB_62', 'EMB_07', 'EMB_45', 'EMB_25', 'EMB_11', 'EMB_12', 'EMB_06', 'EMB_23', 'EMB_29', 'EMB_47', 'EMB_61', 'EMB_15', 'EMB_19', 'EMB_49', 'EMB_17', 'EMB_53', 'EMB_30', 'EMB_32', 'EMB_27', 'EMB_40', 'EMB_31', 'EMB_48', 'EMB_02', 'EMB_14', 'EMB_57', 'EMB_46', 'EMB_63', 'EMB_01', 'EMB_13', 'EMB_33', 'EMB_21', 'EMB_03', 'EMB_51', 'EMB_05', 'EMB_41', 'EMB_00', 'EMB_44', 'EMB_34', 'EMB_28', 'EMB_10', 'EMB_35', 'EMB_50', 'EMB_24']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_00</th>\n",
       "      <th>EMB_01</th>\n",
       "      <th>EMB_02</th>\n",
       "      <th>EMB_03</th>\n",
       "      <th>EMB_04</th>\n",
       "      <th>EMB_05</th>\n",
       "      <th>EMB_06</th>\n",
       "      <th>EMB_07</th>\n",
       "      <th>EMB_08</th>\n",
       "      <th>EMB_09</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_54</th>\n",
       "      <th>EMB_55</th>\n",
       "      <th>EMB_56</th>\n",
       "      <th>EMB_57</th>\n",
       "      <th>EMB_58</th>\n",
       "      <th>EMB_59</th>\n",
       "      <th>EMB_60</th>\n",
       "      <th>EMB_61</th>\n",
       "      <th>EMB_62</th>\n",
       "      <th>EMB_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.011227</td>\n",
       "      <td>0.015182</td>\n",
       "      <td>0.002597</td>\n",
       "      <td>-0.003921</td>\n",
       "      <td>-0.009704</td>\n",
       "      <td>-0.000640</td>\n",
       "      <td>0.011198</td>\n",
       "      <td>0.040924</td>\n",
       "      <td>0.007627</td>\n",
       "      <td>0.023322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.007250</td>\n",
       "      <td>-0.014137</td>\n",
       "      <td>0.009595</td>\n",
       "      <td>-0.013396</td>\n",
       "      <td>0.009082</td>\n",
       "      <td>-0.005512</td>\n",
       "      <td>-0.002132</td>\n",
       "      <td>-0.004012</td>\n",
       "      <td>-0.000962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.027682</td>\n",
       "      <td>0.010461</td>\n",
       "      <td>-0.019723</td>\n",
       "      <td>-0.010939</td>\n",
       "      <td>0.003469</td>\n",
       "      <td>0.016413</td>\n",
       "      <td>0.018254</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>-0.020169</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002303</td>\n",
       "      <td>-0.003304</td>\n",
       "      <td>-0.017208</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>0.016926</td>\n",
       "      <td>0.022633</td>\n",
       "      <td>0.016857</td>\n",
       "      <td>-0.029941</td>\n",
       "      <td>-0.004121</td>\n",
       "      <td>-0.001152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.021544</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>0.008405</td>\n",
       "      <td>-0.029040</td>\n",
       "      <td>-0.025740</td>\n",
       "      <td>-0.002588</td>\n",
       "      <td>0.009490</td>\n",
       "      <td>0.004514</td>\n",
       "      <td>0.018385</td>\n",
       "      <td>0.013275</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004590</td>\n",
       "      <td>-0.015337</td>\n",
       "      <td>0.007174</td>\n",
       "      <td>0.013639</td>\n",
       "      <td>0.004892</td>\n",
       "      <td>0.012634</td>\n",
       "      <td>-0.007335</td>\n",
       "      <td>-0.013596</td>\n",
       "      <td>-0.005158</td>\n",
       "      <td>0.003181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.011922</td>\n",
       "      <td>0.017357</td>\n",
       "      <td>-0.002228</td>\n",
       "      <td>-0.001493</td>\n",
       "      <td>0.008546</td>\n",
       "      <td>-0.010302</td>\n",
       "      <td>0.012404</td>\n",
       "      <td>0.002372</td>\n",
       "      <td>-0.013243</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005986</td>\n",
       "      <td>-0.019676</td>\n",
       "      <td>0.002127</td>\n",
       "      <td>0.001586</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.044418</td>\n",
       "      <td>0.003059</td>\n",
       "      <td>-0.008317</td>\n",
       "      <td>-0.016886</td>\n",
       "      <td>0.014883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000818</td>\n",
       "      <td>0.026582</td>\n",
       "      <td>-0.013543</td>\n",
       "      <td>-0.033014</td>\n",
       "      <td>0.006545</td>\n",
       "      <td>0.009701</td>\n",
       "      <td>0.002904</td>\n",
       "      <td>0.009885</td>\n",
       "      <td>-0.012798</td>\n",
       "      <td>-0.007768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025573</td>\n",
       "      <td>-0.005811</td>\n",
       "      <td>-0.001401</td>\n",
       "      <td>0.006665</td>\n",
       "      <td>-0.015317</td>\n",
       "      <td>0.006962</td>\n",
       "      <td>-0.002084</td>\n",
       "      <td>-0.019869</td>\n",
       "      <td>-0.000995</td>\n",
       "      <td>0.013414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.020523</td>\n",
       "      <td>0.022243</td>\n",
       "      <td>-0.006265</td>\n",
       "      <td>-0.013159</td>\n",
       "      <td>-0.037123</td>\n",
       "      <td>-0.008584</td>\n",
       "      <td>-0.006340</td>\n",
       "      <td>0.004897</td>\n",
       "      <td>0.017507</td>\n",
       "      <td>0.006336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004591</td>\n",
       "      <td>-0.012558</td>\n",
       "      <td>-0.008265</td>\n",
       "      <td>-0.002984</td>\n",
       "      <td>0.030888</td>\n",
       "      <td>-0.029008</td>\n",
       "      <td>-0.033700</td>\n",
       "      <td>-0.001579</td>\n",
       "      <td>0.003421</td>\n",
       "      <td>0.010323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.003530</td>\n",
       "      <td>-0.015738</td>\n",
       "      <td>-0.006244</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.003296</td>\n",
       "      <td>0.009335</td>\n",
       "      <td>-0.001652</td>\n",
       "      <td>0.010254</td>\n",
       "      <td>-0.003611</td>\n",
       "      <td>0.011273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>-0.002409</td>\n",
       "      <td>0.009906</td>\n",
       "      <td>-0.000665</td>\n",
       "      <td>-0.014496</td>\n",
       "      <td>-0.007891</td>\n",
       "      <td>0.011762</td>\n",
       "      <td>-0.013416</td>\n",
       "      <td>-0.010920</td>\n",
       "      <td>-0.006670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.017136</td>\n",
       "      <td>0.009767</td>\n",
       "      <td>0.014779</td>\n",
       "      <td>0.008059</td>\n",
       "      <td>-0.000736</td>\n",
       "      <td>0.024253</td>\n",
       "      <td>0.016696</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>-0.035895</td>\n",
       "      <td>0.030446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028112</td>\n",
       "      <td>-0.015401</td>\n",
       "      <td>0.005603</td>\n",
       "      <td>-0.002173</td>\n",
       "      <td>0.017057</td>\n",
       "      <td>0.009590</td>\n",
       "      <td>-0.009864</td>\n",
       "      <td>0.003798</td>\n",
       "      <td>-0.024084</td>\n",
       "      <td>0.031504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.033809</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.006406</td>\n",
       "      <td>-0.019290</td>\n",
       "      <td>-0.017561</td>\n",
       "      <td>0.002544</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>0.007823</td>\n",
       "      <td>-0.002898</td>\n",
       "      <td>0.014413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004245</td>\n",
       "      <td>0.006260</td>\n",
       "      <td>-0.004780</td>\n",
       "      <td>0.001977</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>-0.003927</td>\n",
       "      <td>0.007886</td>\n",
       "      <td>-0.016147</td>\n",
       "      <td>0.012642</td>\n",
       "      <td>-0.014983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.001256</td>\n",
       "      <td>0.011899</td>\n",
       "      <td>-0.009817</td>\n",
       "      <td>0.007280</td>\n",
       "      <td>-0.043979</td>\n",
       "      <td>-0.023253</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>-0.003941</td>\n",
       "      <td>-0.039277</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008892</td>\n",
       "      <td>-0.014480</td>\n",
       "      <td>-0.005024</td>\n",
       "      <td>-0.028099</td>\n",
       "      <td>-0.006470</td>\n",
       "      <td>0.025809</td>\n",
       "      <td>-0.024412</td>\n",
       "      <td>-0.001494</td>\n",
       "      <td>-0.019587</td>\n",
       "      <td>-0.002925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     EMB_00    EMB_01    EMB_02    EMB_03    EMB_04    EMB_05    EMB_06  \\\n",
       "0 -0.011227  0.015182  0.002597 -0.003921 -0.009704 -0.000640  0.011198   \n",
       "1 -0.027682  0.010461 -0.019723 -0.010939  0.003469  0.016413  0.018254   \n",
       "2  0.021544  0.002321  0.008405 -0.029040 -0.025740 -0.002588  0.009490   \n",
       "3 -0.011922  0.017357 -0.002228 -0.001493  0.008546 -0.010302  0.012404   \n",
       "4 -0.000818  0.026582 -0.013543 -0.033014  0.006545  0.009701  0.002904   \n",
       "5  0.020523  0.022243 -0.006265 -0.013159 -0.037123 -0.008584 -0.006340   \n",
       "6 -0.003530 -0.015738 -0.006244  0.002629  0.003296  0.009335 -0.001652   \n",
       "7  0.017136  0.009767  0.014779  0.008059 -0.000736  0.024253  0.016696   \n",
       "8  0.033809  0.001664  0.006406 -0.019290 -0.017561  0.002544 -0.000144   \n",
       "9 -0.001256  0.011899 -0.009817  0.007280 -0.043979 -0.023253  0.002523   \n",
       "\n",
       "     EMB_07    EMB_08    EMB_09  ...    EMB_54    EMB_55    EMB_56    EMB_57  \\\n",
       "0  0.040924  0.007627  0.023322  ...  0.000512  0.007250 -0.014137  0.009595   \n",
       "1  0.000670 -0.020169  0.003201  ... -0.002303 -0.003304 -0.017208  0.001646   \n",
       "2  0.004514  0.018385  0.013275  ... -0.004590 -0.015337  0.007174  0.013639   \n",
       "3  0.002372 -0.013243  0.000979  ...  0.005986 -0.019676  0.002127  0.001586   \n",
       "4  0.009885 -0.012798 -0.007768  ... -0.025573 -0.005811 -0.001401  0.006665   \n",
       "5  0.004897  0.017507  0.006336  ...  0.004591 -0.012558 -0.008265 -0.002984   \n",
       "6  0.010254 -0.003611  0.011273  ...  0.002548 -0.002409  0.009906 -0.000665   \n",
       "7  0.001120 -0.035895  0.030446  ...  0.028112 -0.015401  0.005603 -0.002173   \n",
       "8  0.007823 -0.002898  0.014413  ...  0.004245  0.006260 -0.004780  0.001977   \n",
       "9  0.000154 -0.003941 -0.039277  ... -0.008892 -0.014480 -0.005024 -0.028099   \n",
       "\n",
       "     EMB_58    EMB_59    EMB_60    EMB_61    EMB_62    EMB_63  \n",
       "0 -0.013396  0.009082 -0.005512 -0.002132 -0.004012 -0.000962  \n",
       "1  0.016926  0.022633  0.016857 -0.029941 -0.004121 -0.001152  \n",
       "2  0.004892  0.012634 -0.007335 -0.013596 -0.005158  0.003181  \n",
       "3  0.006836  0.044418  0.003059 -0.008317 -0.016886  0.014883  \n",
       "4 -0.015317  0.006962 -0.002084 -0.019869 -0.000995  0.013414  \n",
       "5  0.030888 -0.029008 -0.033700 -0.001579  0.003421  0.010323  \n",
       "6 -0.014496 -0.007891  0.011762 -0.013416 -0.010920 -0.006670  \n",
       "7  0.017057  0.009590 -0.009864  0.003798 -0.024084  0.031504  \n",
       "8  0.002419 -0.003927  0.007886 -0.016147  0.012642 -0.014983  \n",
       "9 -0.006470  0.025809 -0.024412 -0.001494 -0.019587 -0.002925  \n",
       "\n",
       "[10 rows x 64 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pipe_val = {} # 초기 input asset process 세팅\n",
    "\n",
    "data_input, config_input = alo.process_asset_step(alo.asset_source[pipeline][step], step, pipeline, data, pipe_val, [input_args])\n",
    "# data_input: input asset의 결과물입니다. 다음 asset 실행 시 필요합니다. \n",
    "# config_input: input asset의 결과 config입니다. 다음 asset실행 시 필요합니다.\n",
    "\n",
    "# input asset의 결과 dataframe은 data_input['dataframe']으로 확인할 수 있습니다. \n",
    "data_input['dataframe'].head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ddf39d-af5c-48bd-9a96-a3543e030b2c",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### 1. Preprocess asset \n",
    "##### Preprocess asset의 args수정 및 확인\n",
    "- 필요한경우 preprocess_args의 항목을 ***preprocess_args[argument명]=value입력*** 을 통해 변경할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b486fd42-87de-4c15-aa77-324cb77415b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'handling_missing': 'interpolation',\n",
       " 'handling_encoding_y_column': None,\n",
       " 'limit_encoding_categories': 30,\n",
       " 'load_train_preprocess': True}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR inference asset 순서에 따라 step 순서를 입력합니다. (input(0) - preprocess(1) - inference(2) - result(3))\n",
    "step = 1 \n",
    "preprocess_args = alo.user_parameters[pipeline][step]['args'][0].copy()\n",
    "\n",
    "# 아래 주석을 풀어 preprocess_args 수정합니다. \n",
    "# preprocess_args['handling_missing'] = 'interpolation'\n",
    "preprocess_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6a36f8-55c3-4e3e-8879-d8aeb7f9ecac",
   "metadata": {},
   "source": [
    "##### Preprocess asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f690c108-afc0-48e3-9b04-85a4f498c8da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m>> Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr/alo//.train_artifacts/models/preprocess/\u001b[0m\n",
      "************************************************************\n",
      "************************************************************\n",
      "['EMB_38_nan', 'EMB_20_nan', 'EMB_18_nan', 'EMB_16_nan', 'EMB_58_nan', 'EMB_36_nan', 'EMB_60_nan', 'EMB_42_nan', 'EMB_56_nan', 'EMB_04_nan', 'EMB_09_nan', 'EMB_54_nan', 'EMB_55_nan', 'EMB_08_nan', 'EMB_52_nan', 'EMB_22_nan', 'EMB_37_nan', 'EMB_43_nan', 'EMB_59_nan', 'EMB_39_nan', 'EMB_26_nan', 'EMB_62_nan', 'EMB_07_nan', 'EMB_45_nan', 'EMB_25_nan', 'EMB_11_nan', 'EMB_12_nan', 'EMB_06_nan', 'EMB_23_nan', 'EMB_29_nan', 'EMB_47_nan', 'EMB_61_nan', 'EMB_15_nan', 'EMB_19_nan', 'EMB_49_nan', 'EMB_17_nan', 'EMB_53_nan', 'EMB_30_nan', 'EMB_32_nan', 'EMB_27_nan', 'EMB_40_nan', 'EMB_31_nan', 'EMB_48_nan', 'EMB_02_nan', 'EMB_14_nan', 'EMB_57_nan', 'EMB_46_nan', 'EMB_63_nan', 'EMB_01_nan', 'EMB_13_nan', 'EMB_33_nan', 'EMB_21_nan', 'EMB_03_nan', 'EMB_51_nan', 'EMB_05_nan', 'EMB_41_nan', 'EMB_00_nan', 'EMB_44_nan', 'EMB_34_nan', 'EMB_28_nan', 'EMB_10_nan', 'EMB_35_nan', 'EMB_50_nan', 'EMB_24_nan'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: No names found, cannot describe anything.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_38</th>\n",
       "      <th>EMB_20</th>\n",
       "      <th>EMB_18</th>\n",
       "      <th>EMB_16</th>\n",
       "      <th>EMB_58</th>\n",
       "      <th>EMB_36</th>\n",
       "      <th>EMB_60</th>\n",
       "      <th>EMB_42</th>\n",
       "      <th>EMB_56</th>\n",
       "      <th>EMB_04</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_05_nan</th>\n",
       "      <th>EMB_41_nan</th>\n",
       "      <th>EMB_00_nan</th>\n",
       "      <th>EMB_44_nan</th>\n",
       "      <th>EMB_34_nan</th>\n",
       "      <th>EMB_28_nan</th>\n",
       "      <th>EMB_10_nan</th>\n",
       "      <th>EMB_35_nan</th>\n",
       "      <th>EMB_50_nan</th>\n",
       "      <th>EMB_24_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.031011</td>\n",
       "      <td>0.014823</td>\n",
       "      <td>-0.024925</td>\n",
       "      <td>-0.003508</td>\n",
       "      <td>-0.013396</td>\n",
       "      <td>-0.018555</td>\n",
       "      <td>-0.005512</td>\n",
       "      <td>0.029787</td>\n",
       "      <td>-0.014137</td>\n",
       "      <td>-0.009704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000640</td>\n",
       "      <td>-0.019736</td>\n",
       "      <td>-0.011227</td>\n",
       "      <td>0.005069</td>\n",
       "      <td>0.016305</td>\n",
       "      <td>0.019516</td>\n",
       "      <td>0.015590</td>\n",
       "      <td>-0.022317</td>\n",
       "      <td>-0.000181</td>\n",
       "      <td>-0.024142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.015606</td>\n",
       "      <td>0.007733</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.011617</td>\n",
       "      <td>0.016926</td>\n",
       "      <td>0.033393</td>\n",
       "      <td>0.016857</td>\n",
       "      <td>0.005872</td>\n",
       "      <td>-0.017208</td>\n",
       "      <td>0.003469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016413</td>\n",
       "      <td>-0.004436</td>\n",
       "      <td>-0.027682</td>\n",
       "      <td>-0.022899</td>\n",
       "      <td>-0.003409</td>\n",
       "      <td>0.018568</td>\n",
       "      <td>0.018375</td>\n",
       "      <td>0.007061</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-0.008296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.027343</td>\n",
       "      <td>0.009834</td>\n",
       "      <td>-0.012177</td>\n",
       "      <td>-0.010790</td>\n",
       "      <td>0.004892</td>\n",
       "      <td>0.008763</td>\n",
       "      <td>-0.007335</td>\n",
       "      <td>-0.008777</td>\n",
       "      <td>0.007174</td>\n",
       "      <td>-0.025740</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002588</td>\n",
       "      <td>-0.000235</td>\n",
       "      <td>0.021544</td>\n",
       "      <td>0.005860</td>\n",
       "      <td>0.010677</td>\n",
       "      <td>-0.002425</td>\n",
       "      <td>0.006936</td>\n",
       "      <td>0.014337</td>\n",
       "      <td>0.006457</td>\n",
       "      <td>0.001680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.001033</td>\n",
       "      <td>0.035589</td>\n",
       "      <td>-0.025108</td>\n",
       "      <td>0.011112</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>-0.005146</td>\n",
       "      <td>0.003059</td>\n",
       "      <td>-0.000790</td>\n",
       "      <td>0.002127</td>\n",
       "      <td>0.008546</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010302</td>\n",
       "      <td>0.006835</td>\n",
       "      <td>-0.011922</td>\n",
       "      <td>0.015190</td>\n",
       "      <td>-0.004980</td>\n",
       "      <td>0.022811</td>\n",
       "      <td>0.022793</td>\n",
       "      <td>-0.030626</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>-0.010933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000823</td>\n",
       "      <td>0.008647</td>\n",
       "      <td>0.004431</td>\n",
       "      <td>-0.000183</td>\n",
       "      <td>-0.015317</td>\n",
       "      <td>0.011087</td>\n",
       "      <td>-0.002084</td>\n",
       "      <td>0.007036</td>\n",
       "      <td>-0.001401</td>\n",
       "      <td>0.006545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009701</td>\n",
       "      <td>-0.024108</td>\n",
       "      <td>-0.000818</td>\n",
       "      <td>-0.007217</td>\n",
       "      <td>0.018639</td>\n",
       "      <td>-0.004010</td>\n",
       "      <td>-0.016046</td>\n",
       "      <td>0.029978</td>\n",
       "      <td>0.010655</td>\n",
       "      <td>-0.006701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.002510</td>\n",
       "      <td>-0.007344</td>\n",
       "      <td>-0.004544</td>\n",
       "      <td>0.036327</td>\n",
       "      <td>0.030888</td>\n",
       "      <td>0.004079</td>\n",
       "      <td>-0.033700</td>\n",
       "      <td>-0.005674</td>\n",
       "      <td>-0.008265</td>\n",
       "      <td>-0.037123</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008584</td>\n",
       "      <td>0.036134</td>\n",
       "      <td>0.020523</td>\n",
       "      <td>0.013542</td>\n",
       "      <td>-0.012563</td>\n",
       "      <td>0.012050</td>\n",
       "      <td>-0.015265</td>\n",
       "      <td>-0.008885</td>\n",
       "      <td>-0.005132</td>\n",
       "      <td>0.012273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.007830</td>\n",
       "      <td>-0.016144</td>\n",
       "      <td>0.006649</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>-0.014496</td>\n",
       "      <td>-0.010096</td>\n",
       "      <td>0.011762</td>\n",
       "      <td>0.008549</td>\n",
       "      <td>0.009906</td>\n",
       "      <td>0.003296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009335</td>\n",
       "      <td>0.013090</td>\n",
       "      <td>-0.003530</td>\n",
       "      <td>-0.008928</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>-0.004509</td>\n",
       "      <td>-0.014907</td>\n",
       "      <td>-0.002927</td>\n",
       "      <td>0.020503</td>\n",
       "      <td>0.013235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.012708</td>\n",
       "      <td>-0.009585</td>\n",
       "      <td>-0.028099</td>\n",
       "      <td>0.014349</td>\n",
       "      <td>0.017057</td>\n",
       "      <td>-0.015230</td>\n",
       "      <td>-0.009864</td>\n",
       "      <td>0.012561</td>\n",
       "      <td>0.005603</td>\n",
       "      <td>-0.000736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024253</td>\n",
       "      <td>0.014879</td>\n",
       "      <td>0.017136</td>\n",
       "      <td>-0.022676</td>\n",
       "      <td>0.019442</td>\n",
       "      <td>0.013341</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>-0.006273</td>\n",
       "      <td>0.003189</td>\n",
       "      <td>0.001766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.008812</td>\n",
       "      <td>-0.006370</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>-0.008975</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>-0.012096</td>\n",
       "      <td>0.007886</td>\n",
       "      <td>-0.002530</td>\n",
       "      <td>-0.004780</td>\n",
       "      <td>-0.017561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002544</td>\n",
       "      <td>0.011761</td>\n",
       "      <td>0.033809</td>\n",
       "      <td>-0.006096</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.014591</td>\n",
       "      <td>-0.001417</td>\n",
       "      <td>0.006506</td>\n",
       "      <td>-0.001833</td>\n",
       "      <td>0.007646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.016117</td>\n",
       "      <td>0.023265</td>\n",
       "      <td>-0.003688</td>\n",
       "      <td>-0.008485</td>\n",
       "      <td>-0.006470</td>\n",
       "      <td>0.010499</td>\n",
       "      <td>-0.024412</td>\n",
       "      <td>0.003755</td>\n",
       "      <td>-0.005024</td>\n",
       "      <td>-0.043979</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023253</td>\n",
       "      <td>0.028730</td>\n",
       "      <td>-0.001256</td>\n",
       "      <td>0.005802</td>\n",
       "      <td>-0.005851</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.011507</td>\n",
       "      <td>0.007737</td>\n",
       "      <td>0.012933</td>\n",
       "      <td>0.025491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     EMB_38    EMB_20    EMB_18    EMB_16    EMB_58    EMB_36    EMB_60  \\\n",
       "0 -0.031011  0.014823 -0.024925 -0.003508 -0.013396 -0.018555 -0.005512   \n",
       "1 -0.015606  0.007733  0.006381  0.011617  0.016926  0.033393  0.016857   \n",
       "2 -0.027343  0.009834 -0.012177 -0.010790  0.004892  0.008763 -0.007335   \n",
       "3 -0.001033  0.035589 -0.025108  0.011112  0.006836 -0.005146  0.003059   \n",
       "4 -0.000823  0.008647  0.004431 -0.000183 -0.015317  0.011087 -0.002084   \n",
       "5  0.002510 -0.007344 -0.004544  0.036327  0.030888  0.004079 -0.033700   \n",
       "6  0.007830 -0.016144  0.006649  0.001771 -0.014496 -0.010096  0.011762   \n",
       "7 -0.012708 -0.009585 -0.028099  0.014349  0.017057 -0.015230 -0.009864   \n",
       "8 -0.008812 -0.006370  0.000837 -0.008975  0.002419 -0.012096  0.007886   \n",
       "9  0.016117  0.023265 -0.003688 -0.008485 -0.006470  0.010499 -0.024412   \n",
       "\n",
       "     EMB_42    EMB_56    EMB_04  ...  EMB_05_nan  EMB_41_nan  EMB_00_nan  \\\n",
       "0  0.029787 -0.014137 -0.009704  ...   -0.000640   -0.019736   -0.011227   \n",
       "1  0.005872 -0.017208  0.003469  ...    0.016413   -0.004436   -0.027682   \n",
       "2 -0.008777  0.007174 -0.025740  ...   -0.002588   -0.000235    0.021544   \n",
       "3 -0.000790  0.002127  0.008546  ...   -0.010302    0.006835   -0.011922   \n",
       "4  0.007036 -0.001401  0.006545  ...    0.009701   -0.024108   -0.000818   \n",
       "5 -0.005674 -0.008265 -0.037123  ...   -0.008584    0.036134    0.020523   \n",
       "6  0.008549  0.009906  0.003296  ...    0.009335    0.013090   -0.003530   \n",
       "7  0.012561  0.005603 -0.000736  ...    0.024253    0.014879    0.017136   \n",
       "8 -0.002530 -0.004780 -0.017561  ...    0.002544    0.011761    0.033809   \n",
       "9  0.003755 -0.005024 -0.043979  ...   -0.023253    0.028730   -0.001256   \n",
       "\n",
       "   EMB_44_nan  EMB_34_nan  EMB_28_nan  EMB_10_nan  EMB_35_nan  EMB_50_nan  \\\n",
       "0    0.005069    0.016305    0.019516    0.015590   -0.022317   -0.000181   \n",
       "1   -0.022899   -0.003409    0.018568    0.018375    0.007061    0.000035   \n",
       "2    0.005860    0.010677   -0.002425    0.006936    0.014337    0.006457   \n",
       "3    0.015190   -0.004980    0.022811    0.022793   -0.030626    0.000888   \n",
       "4   -0.007217    0.018639   -0.004010   -0.016046    0.029978    0.010655   \n",
       "5    0.013542   -0.012563    0.012050   -0.015265   -0.008885   -0.005132   \n",
       "6   -0.008928    0.000528   -0.004509   -0.014907   -0.002927    0.020503   \n",
       "7   -0.022676    0.019442    0.013341    0.001322   -0.006273    0.003189   \n",
       "8   -0.006096    0.004464    0.014591   -0.001417    0.006506   -0.001833   \n",
       "9    0.005802   -0.005851    0.000064    0.011507    0.007737    0.012933   \n",
       "\n",
       "   EMB_24_nan  \n",
       "0   -0.024142  \n",
       "1   -0.008296  \n",
       "2    0.001680  \n",
       "3   -0.010933  \n",
       "4   -0.006701  \n",
       "5    0.012273  \n",
       "6    0.013235  \n",
       "7    0.001766  \n",
       "8    0.007646  \n",
       "9    0.025491  \n",
       "\n",
       "[10 rows x 128 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocess, config_preprocess = alo.process_asset_step(alo.asset_source[pipeline][step], step, pipeline, data_input, config_input.copy(), [preprocess_args])\n",
    "# data_preprocess: preprocess asset의 결과물입니다. 다음 asset 실행 시 필요합니다. \n",
    "# config_preprocess: preprocess asset의 결과 config입니다. 다음 asset실행 시 필요합니다. \n",
    "\n",
    "# preprocess asset의 결과 dataframe은 data_preprocess['dataframe']으로 확인할 수 있습니다. \n",
    "data_preprocess['dataframe'].head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a66a6da-7efd-4c9f-92b4-b91366365db7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2. Inference asset \n",
    "##### Inference asset의 args수정 및 확인\n",
    "- 필요한경우 inference_args의 항목을 ***inference_args[argument명]=value입력*** 을 통해 변경할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13b21092-9b65-4de1-874e-46e8cbd96075",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_type': 'classification'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR inference asset 순서에 따라 step 순서를 입력합니다. (input(0) - preprocess(1) - inference(2) - result(3))\n",
    "step = 2\n",
    "tcr_args = alo.user_parameters[pipeline][step]['args'][0].copy()\n",
    "\n",
    "# 아래 주석을 풀어 tcr_args를 수정합니다. \n",
    "# tcr_args['model_type'] = \n",
    "tcr_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5544d75-1e2f-4009-9e24-7cb91bc18ced",
   "metadata": {},
   "source": [
    "##### inference asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf9857de-2615-41ec-829f-cddb0808aab2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ################################### inference_init (sec):  5.9604644775390625e-05 ################################### \n",
      "\n",
      "************************************************************\n",
      "************************************************************\n",
      "\u001b[92m>> Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr/alo//.train_artifacts/models/train/\u001b[0m\n",
      "\u001b[92m>> Successfully got << output path >> for saving your data into csv or jpg file: \n",
      " /home/jovyan/gcr/alo//.inference_artifacts/output/inference/ \n",
      " L [NOTE] The names of output file must be fixed as << output.csv, output.jpg >> \u001b[0m\n",
      "ignore_columns는 Training 과정에 사용되지 않습니다.\n",
      "[INFO] XAI 분석 시, 활용할 모델을 로드합니다.\n",
      "모델을 Load 완료 하였습니다. (모델 위치: /home/jovyan/gcr/alo//.train_artifacts/models/train/best_model_top0.pkl)\n",
      "################################joblib load model time:  0.09248661994934082 s###########################################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_00</th>\n",
       "      <th>EMB_01</th>\n",
       "      <th>EMB_02</th>\n",
       "      <th>EMB_03</th>\n",
       "      <th>EMB_04</th>\n",
       "      <th>EMB_05</th>\n",
       "      <th>EMB_06</th>\n",
       "      <th>EMB_07</th>\n",
       "      <th>EMB_08</th>\n",
       "      <th>EMB_09</th>\n",
       "      <th>...</th>\n",
       "      <th>x56</th>\n",
       "      <th>x57</th>\n",
       "      <th>x58</th>\n",
       "      <th>x59</th>\n",
       "      <th>x60</th>\n",
       "      <th>x61</th>\n",
       "      <th>x62</th>\n",
       "      <th>x63</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prediction_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.011227</td>\n",
       "      <td>0.015182</td>\n",
       "      <td>0.002597</td>\n",
       "      <td>-0.003921</td>\n",
       "      <td>-0.009704</td>\n",
       "      <td>-0.000640</td>\n",
       "      <td>0.011198</td>\n",
       "      <td>0.040924</td>\n",
       "      <td>0.007627</td>\n",
       "      <td>0.023322</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014137</td>\n",
       "      <td>0.009595</td>\n",
       "      <td>-0.013396</td>\n",
       "      <td>0.009082</td>\n",
       "      <td>-0.005512</td>\n",
       "      <td>-0.002132</td>\n",
       "      <td>-0.004012</td>\n",
       "      <td>-0.000962</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6351386734434903, 0.36486132655650916]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.027682</td>\n",
       "      <td>0.010461</td>\n",
       "      <td>-0.019723</td>\n",
       "      <td>-0.010939</td>\n",
       "      <td>0.003469</td>\n",
       "      <td>0.016413</td>\n",
       "      <td>0.018254</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>-0.020169</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017208</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>0.016926</td>\n",
       "      <td>0.022633</td>\n",
       "      <td>0.016857</td>\n",
       "      <td>-0.029941</td>\n",
       "      <td>-0.004121</td>\n",
       "      <td>-0.001152</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6943764275721842, 0.30562357242781585]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.021544</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>0.008405</td>\n",
       "      <td>-0.029040</td>\n",
       "      <td>-0.025740</td>\n",
       "      <td>-0.002588</td>\n",
       "      <td>0.009490</td>\n",
       "      <td>0.004514</td>\n",
       "      <td>0.018385</td>\n",
       "      <td>0.013275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007174</td>\n",
       "      <td>0.013639</td>\n",
       "      <td>0.004892</td>\n",
       "      <td>0.012634</td>\n",
       "      <td>-0.007335</td>\n",
       "      <td>-0.013596</td>\n",
       "      <td>-0.005158</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6624854911063665, 0.3375145088936336]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.011922</td>\n",
       "      <td>0.017357</td>\n",
       "      <td>-0.002228</td>\n",
       "      <td>-0.001493</td>\n",
       "      <td>0.008546</td>\n",
       "      <td>-0.010302</td>\n",
       "      <td>0.012404</td>\n",
       "      <td>0.002372</td>\n",
       "      <td>-0.013243</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002127</td>\n",
       "      <td>0.001586</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.044418</td>\n",
       "      <td>0.003059</td>\n",
       "      <td>-0.008317</td>\n",
       "      <td>-0.016886</td>\n",
       "      <td>0.014883</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7143277863393447, 0.2856722136606552]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000818</td>\n",
       "      <td>0.026582</td>\n",
       "      <td>-0.013543</td>\n",
       "      <td>-0.033014</td>\n",
       "      <td>0.006545</td>\n",
       "      <td>0.009701</td>\n",
       "      <td>0.002904</td>\n",
       "      <td>0.009885</td>\n",
       "      <td>-0.012798</td>\n",
       "      <td>-0.007768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001401</td>\n",
       "      <td>0.006665</td>\n",
       "      <td>-0.015317</td>\n",
       "      <td>0.006962</td>\n",
       "      <td>-0.002084</td>\n",
       "      <td>-0.019869</td>\n",
       "      <td>-0.000995</td>\n",
       "      <td>0.013414</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6451080860347516, 0.35489191396524883]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.020523</td>\n",
       "      <td>0.022243</td>\n",
       "      <td>-0.006265</td>\n",
       "      <td>-0.013159</td>\n",
       "      <td>-0.037123</td>\n",
       "      <td>-0.008584</td>\n",
       "      <td>-0.006340</td>\n",
       "      <td>0.004897</td>\n",
       "      <td>0.017507</td>\n",
       "      <td>0.006336</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008265</td>\n",
       "      <td>-0.002984</td>\n",
       "      <td>0.030888</td>\n",
       "      <td>-0.029008</td>\n",
       "      <td>-0.033700</td>\n",
       "      <td>-0.001579</td>\n",
       "      <td>0.003421</td>\n",
       "      <td>0.010323</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6467639579366166, 0.35323604206338366]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.003530</td>\n",
       "      <td>-0.015738</td>\n",
       "      <td>-0.006244</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.003296</td>\n",
       "      <td>0.009335</td>\n",
       "      <td>-0.001652</td>\n",
       "      <td>0.010254</td>\n",
       "      <td>-0.003611</td>\n",
       "      <td>0.011273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009906</td>\n",
       "      <td>-0.000665</td>\n",
       "      <td>-0.014496</td>\n",
       "      <td>-0.007891</td>\n",
       "      <td>0.011762</td>\n",
       "      <td>-0.013416</td>\n",
       "      <td>-0.010920</td>\n",
       "      <td>-0.006670</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7875577871744598, 0.21244221282553982]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.017136</td>\n",
       "      <td>0.009767</td>\n",
       "      <td>0.014779</td>\n",
       "      <td>0.008059</td>\n",
       "      <td>-0.000736</td>\n",
       "      <td>0.024253</td>\n",
       "      <td>0.016696</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>-0.035895</td>\n",
       "      <td>0.030446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005603</td>\n",
       "      <td>-0.002173</td>\n",
       "      <td>0.017057</td>\n",
       "      <td>0.009590</td>\n",
       "      <td>-0.009864</td>\n",
       "      <td>0.003798</td>\n",
       "      <td>-0.024084</td>\n",
       "      <td>0.031504</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.633962306591046, 0.3660376934089537]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.033809</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.006406</td>\n",
       "      <td>-0.019290</td>\n",
       "      <td>-0.017561</td>\n",
       "      <td>0.002544</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>0.007823</td>\n",
       "      <td>-0.002898</td>\n",
       "      <td>0.014413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004780</td>\n",
       "      <td>0.001977</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>-0.003927</td>\n",
       "      <td>0.007886</td>\n",
       "      <td>-0.016147</td>\n",
       "      <td>0.012642</td>\n",
       "      <td>-0.014983</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7882125364073506, 0.21178746359264902]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.001256</td>\n",
       "      <td>0.011899</td>\n",
       "      <td>-0.009817</td>\n",
       "      <td>0.007280</td>\n",
       "      <td>-0.043979</td>\n",
       "      <td>-0.023253</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>-0.003941</td>\n",
       "      <td>-0.039277</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005024</td>\n",
       "      <td>-0.028099</td>\n",
       "      <td>-0.006470</td>\n",
       "      <td>0.025809</td>\n",
       "      <td>-0.024412</td>\n",
       "      <td>-0.001494</td>\n",
       "      <td>-0.019587</td>\n",
       "      <td>-0.002925</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6189925375541533, 0.3810074624458469]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     EMB_00    EMB_01    EMB_02    EMB_03    EMB_04    EMB_05    EMB_06  \\\n",
       "0 -0.011227  0.015182  0.002597 -0.003921 -0.009704 -0.000640  0.011198   \n",
       "1 -0.027682  0.010461 -0.019723 -0.010939  0.003469  0.016413  0.018254   \n",
       "2  0.021544  0.002321  0.008405 -0.029040 -0.025740 -0.002588  0.009490   \n",
       "3 -0.011922  0.017357 -0.002228 -0.001493  0.008546 -0.010302  0.012404   \n",
       "4 -0.000818  0.026582 -0.013543 -0.033014  0.006545  0.009701  0.002904   \n",
       "5  0.020523  0.022243 -0.006265 -0.013159 -0.037123 -0.008584 -0.006340   \n",
       "6 -0.003530 -0.015738 -0.006244  0.002629  0.003296  0.009335 -0.001652   \n",
       "7  0.017136  0.009767  0.014779  0.008059 -0.000736  0.024253  0.016696   \n",
       "8  0.033809  0.001664  0.006406 -0.019290 -0.017561  0.002544 -0.000144   \n",
       "9 -0.001256  0.011899 -0.009817  0.007280 -0.043979 -0.023253  0.002523   \n",
       "\n",
       "     EMB_07    EMB_08    EMB_09  ...       x56       x57       x58       x59  \\\n",
       "0  0.040924  0.007627  0.023322  ... -0.014137  0.009595 -0.013396  0.009082   \n",
       "1  0.000670 -0.020169  0.003201  ... -0.017208  0.001646  0.016926  0.022633   \n",
       "2  0.004514  0.018385  0.013275  ...  0.007174  0.013639  0.004892  0.012634   \n",
       "3  0.002372 -0.013243  0.000979  ...  0.002127  0.001586  0.006836  0.044418   \n",
       "4  0.009885 -0.012798 -0.007768  ... -0.001401  0.006665 -0.015317  0.006962   \n",
       "5  0.004897  0.017507  0.006336  ... -0.008265 -0.002984  0.030888 -0.029008   \n",
       "6  0.010254 -0.003611  0.011273  ...  0.009906 -0.000665 -0.014496 -0.007891   \n",
       "7  0.001120 -0.035895  0.030446  ...  0.005603 -0.002173  0.017057  0.009590   \n",
       "8  0.007823 -0.002898  0.014413  ... -0.004780  0.001977  0.002419 -0.003927   \n",
       "9  0.000154 -0.003941 -0.039277  ... -0.005024 -0.028099 -0.006470  0.025809   \n",
       "\n",
       "        x60       x61       x62       x63  prediction  \\\n",
       "0 -0.005512 -0.002132 -0.004012 -0.000962           0   \n",
       "1  0.016857 -0.029941 -0.004121 -0.001152           0   \n",
       "2 -0.007335 -0.013596 -0.005158  0.003181           0   \n",
       "3  0.003059 -0.008317 -0.016886  0.014883           0   \n",
       "4 -0.002084 -0.019869 -0.000995  0.013414           0   \n",
       "5 -0.033700 -0.001579  0.003421  0.010323           0   \n",
       "6  0.011762 -0.013416 -0.010920 -0.006670           0   \n",
       "7 -0.009864  0.003798 -0.024084  0.031504           0   \n",
       "8  0.007886 -0.016147  0.012642 -0.014983           0   \n",
       "9 -0.024412 -0.001494 -0.019587 -0.002925           0   \n",
       "\n",
       "                            prediction_score  \n",
       "0  [0.6351386734434903, 0.36486132655650916]  \n",
       "1  [0.6943764275721842, 0.30562357242781585]  \n",
       "2   [0.6624854911063665, 0.3375145088936336]  \n",
       "3   [0.7143277863393447, 0.2856722136606552]  \n",
       "4  [0.6451080860347516, 0.35489191396524883]  \n",
       "5  [0.6467639579366166, 0.35323604206338366]  \n",
       "6  [0.7875577871744598, 0.21244221282553982]  \n",
       "7    [0.633962306591046, 0.3660376934089537]  \n",
       "8  [0.7882125364073506, 0.21178746359264902]  \n",
       "9   [0.6189925375541533, 0.3810074624458469]  \n",
       "\n",
       "[10 rows x 130 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tcr, config_tcr = alo.process_asset_step(alo.asset_source[pipeline][step], step, pipeline, data_preprocess, config_preprocess.copy(), [tcr_args])\n",
    "# data_tcr: TCR asset의 결과물입니다. \n",
    "# config_tcr: TCR asset의 결과 config입니다. \n",
    "\n",
    "# tcr asset의 결과 dataframe은 data_tcr['dataframe']으로 확인할 수 있습니다. \n",
    "data_tcr['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e749a64d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3. Result asset \n",
    "##### Result asset의 args수정 및 확인\n",
    "- 필요한경우 Result_args의 항목을 ***result_args[argument명]=value입력*** 을 통해 변경할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17905521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result_save_name': None}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCR inference asset 순서에 따라 step 순서를 입력합니다. (input(0) - preprocess(1) - inference(2) - result(3))\n",
    "step = 3\n",
    "result_args = alo.user_parameters[pipeline][step]['args'][0].copy()\n",
    "\n",
    "# 아래 주석을 풀어 result_args를 수정합니다. \n",
    "# tcr_args['model_type'] = \n",
    "result_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c2f5c",
   "metadata": {},
   "source": [
    "##### result asset 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c899d49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "************************************************************\n",
      "Loading Embeddings\n",
      "\u001b[92m>> Successfully got model path for saving or loading your AI model: \n",
      " /home/jovyan/gcr/alo//.train_artifacts/models/result/\u001b[0m\n",
      "Merging data\n",
      "\u001b[92m>> Successfully got << output path >> for saving your data into csv or jpg file: \n",
      " /home/jovyan/gcr/alo//.inference_artifacts/output/result/ \n",
      " L [NOTE] The names of output file must be fixed as << output.csv, output.jpg >> \u001b[0m\n",
      "\u001b[92m>> Successfully got << output path >> for saving your data into csv or jpg file: \n",
      " /home/jovyan/gcr/alo//.inference_artifacts/output/result/ \n",
      " L [NOTE] The names of output file must be fixed as << output.csv, output.jpg >> \u001b[0m\n",
      "Check Result at /home/jovyan/gcr/alo//.inference_artifacts/output/result/inference_result.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>is_married</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gregory_Hull</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allison_Peterson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Daniel_Davies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alison_Fox</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Daniel_Moore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Barbara_Smith</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Paul_Terry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Christina_Salas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jose_Boyd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Zachary_Fowler</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name  is_married  prediction\n",
       "0      Gregory_Hull         NaN           0\n",
       "1  Allison_Peterson         NaN           0\n",
       "2     Daniel_Davies         NaN           0\n",
       "3        Alison_Fox         NaN           0\n",
       "4      Daniel_Moore         NaN           0\n",
       "5     Barbara_Smith         NaN           0\n",
       "6        Paul_Terry         NaN           0\n",
       "7   Christina_Salas         NaN           0\n",
       "8         Jose_Boyd         NaN           0\n",
       "9    Zachary_Fowler         NaN           0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_result, config_result = alo.process_asset_step(alo.asset_source[pipeline][step], step, pipeline, data_tcr, config_tcr.copy(), [result_args])\n",
    "# data_result: result asset의 결과물입니다. \n",
    "# config_result: result asset의 결과 config입니다. \n",
    "\n",
    "# result asset의 결과 dataframe은 data_result['dataframe']으로 확인할 수 있습니다. \n",
    "data_result['dataframe'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41befae8-040f-40f7-b22f-9c05f1ebe10e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcr",
   "language": "python",
   "name": "gcr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
