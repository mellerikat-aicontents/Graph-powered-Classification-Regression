## load data from external_path
external_path:
    - load_train_data_path: ./solution/sample_data/train
    - load_inference_data_path: ./solution/sample_data/test
    - save_train_artifacts_path:
    - save_inference_artifacts_path:
    - load_model_path: 

external_path_permission:
    - s3_private_key_file:
 
## Parameter setting for experiment 
## 
user_parameters:
    - train_pipeline:
        - step: input
          args:
            - input_path: train           # (str)
              x_columns:                  # (list)
              use_all_x: True             # (bool), True | False
              y_column: target            # (str) 
              groupkey_columns:
              drop_columns:
              time_column:
              concat_dataframes:
              encoding:                   # (str), utf-8(default) | cp949
          ui_args:
            - x_columns
            - y_column
          
        - step: graph
          args:       
            - center_node_column: ID        # (str), automatic selection of columns with the most unique values, ex CUSTOMER_ID(default)
              dimension: 128                # (int), 256(default), dimension > 1 & even
              num_epochs: 1                 # (int), 10(default)
              num_partitions:               # (int), 1(default)
              use_gpu:                      # (bool), False(default) | True
          ui_args:
            - center_node_column
            - dimension
            - num_epochs
        
        - step: train
          args:
            - task:                         # (str), classification(default) | regression
              epochs:                       # (int), 10(default)
          ui_args:
            - model_type
            - evaluation_metric
            - shap_ratio

        - step: output
          args:
      
                                                      
    - inference_pipeline:
        - step: input
          args:
            - input_path: test         # (str)
              x_columns:               # (list)
              use_all_x: true          # (bool), True | False 
              y_column: 
              groupkey_columns:
              drop_columns:
              time_column:
              concat_dataframes:
              encoding:  
          ui_args:
            - x_columns

        - step: inference
          args:

        - step: output
          args:
   
## asset information       
asset_source:
    - train_pipeline:
        - step: input
          source:  ## git | local 
            code: http://mod.lge.com/hub/smartdata/ml-framework/alov2-module/input.git
            # code: local
            branch: tabular_2.0
            requirements:
              - pandas==1.5.3
        
        - step: graph
          source:
            code: http://mod.lge.com/hub/dxadvtech/assets/gfe.git
            # code: local
            branch: feature/dnn
            requirements:
              - torch==2.0.0
              - requirements.txt

        - step: train
          source:
            code: http://mod.lge.com/hub/dxadvtech/assets/gdnn.git
            # code: local
            branch: main
            requirements:
              - pandas==1.5.3
              - torch==2.0.0
              - requirements.txt
        
        - step: output
          source:
            code: http://mod.lge.com/hub/dxadvtech/assets/output.git
            # code: local
            branch: main
            requirements:
              - requirements.txt

   
    - inference_pipeline:
        - step: input
          source:  ## git | local
           # code: http://mod.lge.com/hub/smartdata/ml-framework/alov2-module/input.git
            code: local
            branch: tabular_2.0
            requirements:
              - pandas==1.5.3

        - step: inference
          source:
            code: http://mod.lge.com/hub/dxadvtech/assets/gdnn.git
            branch: main
            requirements:
              - pandas==1.5.3
              - torch==2.0.0
              - requirements.txt
                
        - step: output
          source:
            #code: http://mod.lge.com/hub/dxadvtech/assets/output.git
            code: local
            branch: main
            requirements:
              - requirements.txt
     
control:
    ## 1. 패키지 설치 및 asset 존재 여부를 실험 시마다 체크할지, 한번만 할지 결정
    ## 1-2 requirements.txt 및 종속 패키지들 한번만 설치할 지 매번 설치할지도 결정 
    - get_asset_source: once ## once, every
    # TODO 아래 get_external_data 제작하기
    - get_external_data: every ## once, every
    ## 2. 생성된 artifacts 를 backup 할지를 결정 True/False
    - backup_artifacts: True
    ## 3. pipeline 로그를 backup 할지를 결정 True/False
    - backup_log: True
    ## 4. 저장 공간 사이즈를 결정 (단위 MB)
    - backup_size: 1000
 
    ## 5. Asset 사이 데이터 전달 방법으로 memory, file 를 지원
    - interface_mode: memory
