## 해당 파일에서 기술하기에 힘든 내용의 자세한 설명은 아래 user guide 참조 하시길 바랍니다.
## http://collab.lge.com/main/pages/viewpage.action?pageId=2184972859

## 외부에서 데이터 가져오기 / 결과 저장하는 경우 해당 위치에 지정
external_path:
    - load_train_data_path: /nas001/users/jw0220.kim/gcr_test_data/adult_income/
    - load_inference_data_path: /nas001/users/jw0220.kim/gcr_test_data/inference/
    - save_train_artifacts_path:
    - save_inference_artifacts_path:

external_path_permission:
    - s3_private_key_file:
 
## 실험에 필요한 파라미터를 설정함 
## - 해당 위치에서 삭제되면, code 의 default 로 실행
user_parameters:
    - train_pipeline:
        - step: input  ## 필수
          args:
            - input_path: adult_income    # external_path의 load_train_data_path의 최하위 디렉토리 명 입력
              x_columns:            # 그래프 구성에 사용할 x컬럼 지정
              use_all_x: True       # 데이터의 모든 x컬럼을 사용할 경우 True로 입력 / True, False
              y_column: target  # Label에 해당하는 컬럼 지정
              groupkey_columns:
              drop_columns:
              time_column:
              concat_dataframes:
              encoding:             # cp949

          
        - step: train1 ## graph train
          args:       
            - center_node_column: ID        # 이름, ID등 나머지 정보들을 대표할 수 있는 컬럼을 중심 노드로 방사형 구조 그래프 설계 / default: 고유값이 가장 많은 컬럼 자동 선택(ex Customer_ID)          
            # User Settings
              embedding_column: ID         # 임베딩 대상 컬럼 / default: 중심 노드 컬럼 (center_node_column)
              dimension: 128                   # 임베딩 차원 (dimension > 1 & 짝수)/ default: 256
              num_epochs: 1                  # 그래프 학습 epoch 수 / default: 10
              num_partitions:                   # node partition 수, 데이터가 큰 경우 partition을 늘려 메모리 사용량 감소 / default: 1 
            # Advanced Settings
              use_gpu:                          # Graph Embedding 학습시 GPU 사용여부(True/False), default: False

        - step: preprocess
          args:
            - handling_missing: interpolation         # 숫자형 결측치 처리 방법 설정 (interpolation, fill_number, dropna)
              handling_encoding_y_column: target  # 인코딩을 적용할 y컬럼 설정 (input asset의 y_column과 동일하게 설정)
              handling_encoding_y: label              # none, label, ordinal, binary, onehot, hashing
              handling_scaling_x: none                # onehot이나 hashing 인코딩 등을 진행 시 컬럼이 너무 많아지는 것에 대한 한계치 설정 
              load_train_preprocess: False            # (inference workflow 전용) True 이면, train preprocess 를 참조하여 진행
        
        - step: train2 ## 필수
          args:
            - model_type: classification               # classification / regression (task의 목적에 맞게 입력)
              data_split_method: cross_validate        # hyper parameter optimization을 위한 data 분할 방식 / (cross_validate, train_test_split)
              evaluation_metric: accuracy              # classification: accuracy, precision, recall, f1-score / regression: mse, r2, mae, rmse
              model_list: [lgb]                        # 알고리즘 선택(param_range를 사용할 경우, classification의 경우 rf, gbm, lgb, cb만 가능)
              num_hpo: 3                               # HPO 개수 설정, 0 인 경우 하기 설정을 무시하고 위에서 선택한 알고리즘에 대해서 default로 실행됨
              param_range: {
                rf: {max_depth: 6, n_estimators: [300, 500]},
                gbm: {max_depth: [5, 7], n_estimators: [300, 500]},
                ngb: {col_sample: [0.6, 0.8], n_estimators:[100, 300]},               
                lgb: {max_depth:[5, 9], n_estimators:[300, 500]},
                cb: {max_depth:[5, 9], n_estimators:[100, 500]},
              } # 탐색하고 싶은 parameter [min, max] 설정, 숫자만 입력할 경우 해당 parameter는 고정됨
              ## num_hpo=3, max_depth=[1, 3]인 경우 hpo 과정에서 max_depth가 1, 2, 3인 경우에 대해 실행됨.(소수점인 경우 integer로 실행)
              shap_ratio: 0.001
              # train시 shap value 뽑을 데이터를 sampling하는 비율
              # 1로 입력하면, train 데이터 전체에 대해 shap value가 출력 됩니다. 단, 학습 시간이 크게 소요됩니다.
   
    - inference_pipeline:
        - step: input  ## 필수
          args:
            - input_path: inference
              x_columns: 
              use_all_x: true
              y_column: 
              groupkey_columns:
              drop_columns:
              time_column:
              concat_dataframes:
              encoding: # cp949 

        - step: inference1 # graph inference
          args:
              
        - step: preprocess
          args:
            - handling_missing: interpolation
              handling_encoding_y_column:  
              limit_encoding_categories: 30 # onehot이나 hashing 인코딩 등을 진행 시 컬럼이 너무 많아지는 것에 대한 한계치 설정
              load_train_preprocess: True   ## (inference workflow 전용) True 이면, train preprocess 를 참조하여 진행

        - step: inference2 ## 필수
          args:
            - model_type: classification
              run_shapley: False

        - step: result
          args:
            - result_save_name:               # result save file name, default: inference_result.csv
   
## asset 의 설치 정보를 기록       
asset_source:
    - train_pipeline:
        - step: input
          source:  ## git / local 지원
            code: http://mod.lge.com/hub/smartdata/ml-framework/alov2-module/input.git
            # code: local
            branch: tabular_2.0
            requirements:
              - pandas==1.5.3
        
        - step: train1
          source:
            code: http://mod.lge.com/hub/dxadvtech/assets/gfe.git
            # code: local
            branch: develop
            requirements:
              - torch==2.0.0
              - requirements.txt

        - step: preprocess
          source:  ## git / local 지원
            code: http://mod.lge.com/hub/smartdata/ml-framework/alov2-module/preprocess.git
            # code: local
            branch: release-1.2
            requirements:
              - pandas==1.5.3
              - category_encoders

        - step: train2
          source:
            code: http://mod.lge.com/hub/dxadvtech/assets/tcr.git
            # code: local
            branch: tcr_v1.1.2
            requirements:
              - requirements.txt 
              - numpy==1.25.2 --force-reinstall 
   
    - inference_pipeline:
        - step: input
          source:  ## git / local 지원
           # code: http://mod.lge.com/hub/smartdata/ml-framework/alov2-module/input.git
            code: local
            branch: tabular_2.0
            requirements:
              - pandas==1.5.3

        - step: inference1
          source:
            code: http://mod.lge.com/hub/dxadvtech/assets/gfe.git
            branch: develop
            requirements:
              - pandas==1.5.3
              - torch==2.0.0
              - requirements.txt

        - step: preprocess
          source:  ## git / local 지원
           # code: http://mod.lge.com/hub/smartdata/ml-framework/alov2-module/preprocess.git
            code: local
            branch: release-1.2
            requirements:
              - pandas==1.5.3

        - step: inference2
          source:
            code: http://mod.lge.com/hub/dxadvtech/assets/tcr.git
            # code: local
            branch: tcr_v1.1.2
            requirements:
              - pandas==1.5.3
              
        - step: result
          source:
            code: http://mod.lge.com/hub/dxadvtech/assets/gfe.git
            branch: develop
            requirements:
              - pandas==1.5.3
     
control:
    ## 1. 패키지 설치 및 asset 존재 여부를 실험 시마다 체크할지, 한번만 할지 결정
    ## 1-2 requirements.txt 및 종속 패키지들 한번만 설치할 지 매번 설치할지도 결정 
    - get_asset_source: once ## once, every
    # TODO 아래 get_external_data 제작하기
    - get_external_data: once ## once, every
    ## 2. 생성된 artifacts 를 backup 할지를 결정 True/False
    - backup_artifacts: True
    ## 3. pipeline 로그를 backup 할지를 결정 True/False
    - backup_log: True
    ## 4. 저장 공간 사이즈를 결정 (단위 MB)
    - backup_size: 1000
 
    ## 5. Asset 사이 데이터 전달 방법으로 memory, file 를 지원
    - interface_mode: memory
